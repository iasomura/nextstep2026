{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ddb4e1",
   "metadata": {},
   "source": [
    "# 04-5_evaluation_execution.ipynb â€” ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«5 æœ€çµ‚è©•ä¾¡å®Ÿè¡Œ\n",
    "\n",
    "- ç›®çš„: å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã®å®Ÿè¡Œã€çµ±è¨ˆåˆ†æã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "- å…¥åŠ›: `artifacts/{RUN_ID}/handoff/04-4_agent_implementation.pkl`\n",
    "- å‡ºåŠ›:\n",
    "  - `artifacts/{RUN_ID}/handoff/04-5_evaluation_results.pkl`\n",
    "  - `artifacts/{RUN_ID}/results/evaluation_results_{timestamp}.csv`\n",
    "  - `artifacts/{RUN_ID}/results/performance_metrics_{timestamp}.json`\n",
    "  - `artifacts/{RUN_ID}/results/error_analysis_{timestamp}.json`\n",
    "\n",
    "> æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ `03_ai_agent_analysis_part4_config_patched_structured_output_v2_FIXED3.ipynb`\n",
    "> ã‹ã‚‰ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’ç§»æ¤ã—ã¦ã„ã¾ã™: 07-SIMPLE, 07-RERUN-FIX, 08-INVESTIGATE, 25(Controller API)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72262c1a-2761-49f0-97f4-1ba8453abf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NX] RUN_ID = 2025-10-22_074213 | paths.RUN_ID = 2025-10-22_074213\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0 (02ä»¥é™ å…±é€š): ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰è§£æ±ºã—ã¦ paths ã‚’èª­ã‚€ ===\n",
    "try:\n",
    "    import run_id_registry as runreg\n",
    "    rid = runreg.bootstrap()  # envâ†’ãƒ•ã‚¡ã‚¤ãƒ«â†’latestâ†’æ–°è¦\n",
    "    import importlib\n",
    "    import _compat.paths as paths\n",
    "    importlib.reload(paths)\n",
    "    print(\"[NX] RUN_ID =\", rid, \"| paths.RUN_ID =\", getattr(paths, \"RUN_ID\", None))\n",
    "except Exception as e:\n",
    "    print(\"[04-5] registry/paths not available -> skipping. Reason:\", e)\n",
    "    rid = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb894daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPAT shim: load_configuration (inject) ===\n",
    "# - Provides load_configuration(cfg_override) expected by generalization_study()\n",
    "# - Tolerates absence of _compat.paths by falling back to local config.json\n",
    "from typing import Optional, Dict, Any\n",
    "from pathlib import Path\n",
    "import json, sys, os\n",
    "\n",
    "try:\n",
    "    import _compat.paths as _px\n",
    "except Exception:\n",
    "    _px = None  # allow fallback\n",
    "\n",
    "def _deep_update(base: Dict[str, Any], upd: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    if not isinstance(base, dict):\n",
    "        return upd or {}\n",
    "    if not isinstance(upd, dict):\n",
    "        return base\n",
    "    out = dict(base)\n",
    "    for k, v in upd.items():\n",
    "        if isinstance(v, dict) and isinstance(out.get(k), dict):\n",
    "            out[k] = _deep_update(out[k], v)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def load_configuration(cfg_override: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    # 1) Try compat layer if available\n",
    "    try:\n",
    "        if _px and hasattr(_px, \"ensure_roots\"):\n",
    "            try:\n",
    "                _px.ensure_roots()\n",
    "            except Exception:\n",
    "                pass\n",
    "        base = {}\n",
    "        if _px and hasattr(_px, \"load_config\"):\n",
    "            try:\n",
    "                base = _px.load_config()\n",
    "            except Exception:\n",
    "                base = {}\n",
    "        # 2) Fallback: local config.json candidates\n",
    "        if not base:\n",
    "            candidates = [\n",
    "                Path(\"/mnt/data/config.json\"),\n",
    "                Path(\"config.json\"),\n",
    "                Path(\"_compat/config.json\"),\n",
    "            ]\n",
    "            for p in candidates:\n",
    "                if p.exists():\n",
    "                    base = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "                    break\n",
    "        if not base:\n",
    "            raise FileNotFoundError(\"Missing config.json (tried /mnt/data, cwd, _compat).\")\n",
    "        # 3) Merge overrides\n",
    "        return _deep_update(dict(base), cfg_override or {})\n",
    "    except Exception as e:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72da12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_ID: 2025-10-22_074213\n",
      "development_mode: False\n",
      "results_dir: artifacts/2025-10-22_074213/results\n",
      "handoff_dir: artifacts/2025-10-22_074213/handoff\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 00: åˆæœŸåŒ–ï¼ˆPaths/Config bootstrapï¼‰\n",
    "# - RUN_IDã®æ±ºå®šã¨æˆæœç‰©ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ±ä¸€åŒ–\n",
    "# - config.jsonã®ãƒ­ãƒ¼ãƒ‰ï¼†ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼ˆDEV/PRODï¼‰\n",
    "# - ä¹±æ•°ç¨®ã®å›ºå®š\n",
    "\n",
    "import os, json, sys, time, pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# äº’æ›ãƒ‘ã‚¹ã®èª­è¾¼ï¼ˆartifacts/{RUN_ID}/... ã¸çµ±ä¸€ï¼‰\n",
    "# /mnt/data/paths.py ã¯ drop-in shim ã¨ã—ã¦åˆ©ç”¨\n",
    "paths_module = None\n",
    "try:\n",
    "    sys.path.append(\"_compat/\")\n",
    "    import paths as _compat_paths  # drop-in shim\n",
    "    paths_module = _compat_paths\n",
    "    _compat_paths.ensure_roots()\n",
    "    RUN_ID = _compat_paths.load_config().get(\"run_id\")\n",
    "    BASE_DIRS = _compat_paths.compat_base_dirs\n",
    "except Exception as e:\n",
    "    print(\"[WARN] paths shimã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ:\", e)\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ãƒ¬ãƒ³ãƒˆç›´ä¸‹ã« artifacts/{RUN_ID}/.. ã‚’æ§‹ç¯‰\n",
    "    RUN_ID = os.environ.get(\"RUN_ID\") or datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "    ARTIFACTS = Path(\"artifacts\") / RUN_ID\n",
    "    for sub in [\"raw\",\"processed\",\"models\",\"results\",\"handoff\",\"logs\",\"traces\"]:\n",
    "        (ARTIFACTS / sub).mkdir(parents=True, exist_ok=True)\n",
    "    BASE_DIRS = {k: str(ARTIFACTS / v) for k, v in {\n",
    "        'raw': 'raw','data':'processed','models':'models','results':'results',\n",
    "        'handoff':'handoff','logs':'logs','traces':'traces'\n",
    "    }.items()}\n",
    "\n",
    "# Configèª­ã¿è¾¼ã¿\n",
    "DEFAULT_CONFIG_PATH = Path(\"_compat/config.json\")\n",
    "cfg = {\n",
    "    \"system\": {\"development_mode\": False, \"seed\": 42},\n",
    "    \"engine\": {},\n",
    "    \"paths\": {\"results_dir\": \"results\"}\n",
    "}\n",
    "if DEFAULT_CONFIG_PATH.exists():\n",
    "    with open(DEFAULT_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        cfg_from_file = json.load(f)\n",
    "        # Shallow merge (keep structure simple for robustness)\n",
    "        for k,v in cfg_from_file.items():\n",
    "            if isinstance(v, dict) and k in cfg and isinstance(cfg[k], dict):\n",
    "                cfg[k].update(v)\n",
    "            else:\n",
    "                cfg[k] = v\n",
    "\n",
    "# ç’°å¢ƒå¤‰æ•°/å¼•æ•°ã«ã‚ˆã‚‹DEVãƒ¢ãƒ¼ãƒ‰ä¸Šæ›¸ã\n",
    "ENV_DEV = os.environ.get(\"DEV_MODE\")\n",
    "if ENV_DEV is not None:\n",
    "    cfg[\"system\"][\"development_mode\"] = (ENV_DEV.lower() == \"true\")\n",
    "\n",
    "# ä¹±æ•°seed\n",
    "SEED = int(cfg.get(\"system\",{}).get(\"seed\", 42))\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"development_mode:\", cfg[\"system\"][\"development_mode\"])\n",
    "print(\"results_dir:\", BASE_DIRS.get(\"results\"))\n",
    "print(\"handoff_dir:\", BASE_DIRS.get(\"handoff\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c9fabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect handoff: artifacts/2025-10-22_074213/handoff/04-4_agent_implementation.pkl\n",
      "[OK] handoff loaded.\n",
      "{'SAMPLES': 200, 'MAX_WORKERS': 10, 'SAMPLING': 'random'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 01: å…¥åŠ›ãƒãƒ³ãƒ‰ã‚ªãƒ•ã®èª­è¾¼ã¨ãƒ¢ãƒ¼ãƒ‰åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "from pathlib import Path\n",
    "import pickle, json\n",
    "from datetime import datetime\n",
    "\n",
    "handoff_path = Path(BASE_DIRS[\"handoff\"]) / \"04-4_agent_implementation.pkl\"\n",
    "print(\"Expect handoff:\", handoff_path)\n",
    "\n",
    "handoff = None\n",
    "if handoff_path.exists():\n",
    "    try:\n",
    "        with open(handoff_path, \"rb\") as f:\n",
    "            handoff = pickle.load(f)\n",
    "        print(\"[OK] handoff loaded.\")\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] handoff load failed:\", e)\n",
    "else:\n",
    "    print(\"[WARN] handoff file not found (NOT_FOUND). å‡¦ç†ã¯å®Ÿè¡Œæ™‚ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚\")\n",
    "\n",
    "# å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ¨™æº–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "if cfg[\"system\"][\"development_mode\"]:\n",
    "    SAMPLES = 50\n",
    "    MAX_WORKERS = 12\n",
    "    SAMPLING = \"low-ml-priority\"  # ä½MLç¢ºç‡å„ªå…ˆ\n",
    "else:\n",
    "    SAMPLES = 200\n",
    "    MAX_WORKERS = 10\n",
    "    SAMPLING = \"random\"\n",
    "print({\"SAMPLES\": SAMPLES, \"MAX_WORKERS\": MAX_WORKERS, \"SAMPLING\": SAMPLING})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f7fd3a-da10-4731-bf44-3290cedd6333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] handoff missing keys: ['HIGH_RISK_WORDS', 'DANGEROUS_TLDS', 'cert_full_info_map', 'brand_keywords'] at artifacts/2025-10-22_074213/handoff/04-4_agent_implementation.pkl\n",
      "[OK] handoff ready: artifacts/2025-10-22_074213/handoff/04-4_agent_implementation.pkl (source=fallback)\n",
      "[WARN] handoff is missing keys: ['HIGH_RISK_WORDS', 'DANGEROUS_TLDS', 'cert_full_info_map', 'brand_keywords']\n"
     ]
    }
   ],
   "source": [
    "# Cell 01.1: Handoff ãƒ­ãƒ¼ãƒ€ï¼ˆå …ç‰¢åŒ– + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\n",
    "# - ç©º/å£Šã‚ŒãŸ pkl(\"Ran out of input\") ã‚’æ¤œçŸ¥\n",
    "# - artifacts/*/handoff ã‚’èµ°æŸ»ã—ã€ç›´è¿‘ã®æœ‰åŠ¹ 04-4 handoff ã‚’è‡ªå‹•æ¡ç”¨\n",
    "# - å¿…é ˆã‚­ãƒ¼ã‚’æ¤œè¨¼\n",
    "# - \"thin handoff\"ï¼ˆagent_specâ†’å†æ§‹ç¯‰ï¼‰ã«ã‚‚å¯¾å¿œ\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle, json, os, glob, time\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "REQUIRED_KEYS = [\n",
    "    \"cfg\", \"HIGH_RISK_WORDS\", \"DANGEROUS_TLDS\", \"cert_full_info_map\", \"brand_keywords\"\n",
    "]\n",
    "\n",
    "def _safe_pickle_load(p: Path) -> Tuple[Optional[dict], Optional[str]]:\n",
    "    \"\"\"æˆ»ã‚Šå€¤: (obj or None, err or None)\"\"\"\n",
    "    if not p.exists():\n",
    "        return None, \"NOT_FOUND\"\n",
    "    try:\n",
    "        st = p.stat()\n",
    "    except Exception as e:\n",
    "        return None, f\"STAT_ERROR: {e!r}\"\n",
    "    if st.st_size == 0:\n",
    "        return None, \"EMPTY_FILE\"\n",
    "    try:\n",
    "        with p.open(\"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "    except EOFError:\n",
    "        return None, \"EOF (Ran out of input)\"\n",
    "    except Exception as e:\n",
    "        return None, f\"UNPICKLE_ERROR: {e!r}\"\n",
    "    if not isinstance(obj, dict):\n",
    "        # äº’æ›: (ReturnCode, PathsMap) ãªã©ã‚’è¨±å®¹\n",
    "        if isinstance(obj, tuple) and len(obj) == 2 and isinstance(obj[1], dict):\n",
    "            obj = {\"return_code\": obj[0], \"paths\": obj[1]}\n",
    "        else:\n",
    "            # æƒ³å®šå¤–ã ãŒ dict ã«å¤‰æ›ï¼ˆå‡ºæ¥ã‚‹ç¯„å›²ã§ï¼‰\n",
    "            try:\n",
    "                obj = dict(obj)\n",
    "            except Exception:\n",
    "                return None, f\"INVALID_OBJECT_TYPE: {type(obj)}\"\n",
    "    return obj, None\n",
    "\n",
    "def _iter_candidate_paths() -> list[Path]:\n",
    "    # ã¾ãšã¯ 04-5ã§æœŸå¾…ã—ã¦ã„ã‚‹ handoff_path ã‚’å„ªå…ˆ\n",
    "    primaries = []\n",
    "    try:\n",
    "        primaries.append(handoff_path)  # defined in Cell 01\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    # artifacts/*/handoff/04-4_agent_implementation.pkl ã‚’æ–°ã—ã„é †ã«\n",
    "    roots = [Path(\"artifacts\"), Path(\"/mnt/data/artifacts\")]\n",
    "    cands = []\n",
    "    for root in roots:\n",
    "        if root.exists():\n",
    "            for p in root.glob(\"*/handoff/04-4_agent_implementation.pkl\"):\n",
    "                try:\n",
    "                    cands.append((p.stat().st_mtime, p))\n",
    "                except Exception:\n",
    "                    continue\n",
    "    cands.sort(key=lambda t: t[0], reverse=True)\n",
    "    ordered = list(dict.fromkeys(primaries + [p for _, p in cands]))  # é‡è¤‡é™¤å»ã—é †åºä¿æŒ\n",
    "    return ordered\n",
    "\n",
    "def _maybe_reconstruct_from_agent_spec(h: dict) -> dict:\n",
    "    # agent_spec ãŒ dict ã§ä¿å­˜ã•ã‚Œã¦ã„ã‚‹å ´åˆã€å¿…è¦ãªã‚‰å†æ§‹ç¯‰ã®ãƒ•ãƒƒã‚¯ã‚’ç”¨æ„\n",
    "    spec = h.get(\"agent_spec\")\n",
    "    if spec and isinstance(spec, dict):\n",
    "        try:\n",
    "            # ã“ã“ã§ã¯é‡ã„å†æ§‹ç¯‰ã¯è¡Œã‚ãšã€å¿…è¦ãªã‚­ãƒ¼ãŒç„¡ã‘ã‚Œã° WARN ã®ã¿\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] agent_spec å†æ§‹ç¯‰ã«å¤±æ•—:\", e)\n",
    "    return h\n",
    "\n",
    "def _validate_required_keys(h: dict) -> list[str]:\n",
    "    return [k for k in REQUIRED_KEYS if k not in h]\n",
    "\n",
    "# å®Ÿä½“ãƒ­ãƒ¼ãƒ‰\n",
    "handoff_ok = False\n",
    "handoff_src = None\n",
    "_handoff_errors = []\n",
    "\n",
    "for p in _iter_candidate_paths():\n",
    "    obj, err = _safe_pickle_load(p)\n",
    "    if err:\n",
    "        _handoff_errors.append((str(p), err))\n",
    "        continue\n",
    "    # agent_spec best-effort\n",
    "    obj = _maybe_reconstruct_from_agent_spec(obj)\n",
    "    miss = _validate_required_keys(obj)\n",
    "    if miss:\n",
    "        print(\"[WARN] handoff missing keys:\", miss, \"at\", p)\n",
    "        # å¿…é ˆã‚­ãƒ¼æ¬ æã§ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€™è£œã¨ã—ã¦ã¯ä¿æŒã—ã€æ¬¡ã®å€™è£œã‚’è©¦ã™\n",
    "        # æœ€çµ‚çš„ã«ä¸€ç•ªæ–°ã—ã„æˆåŠŸå€™è£œã‚’æ¡ç”¨\n",
    "    # å€™è£œæ¡ç”¨ï¼ˆæœ€åˆã«è¦‹ã¤ã‹ã£ãŸæ­£å¸¸ãƒ•ã‚¡ã‚¤ãƒ«ã§è‰¯ã„ï¼‰\n",
    "    handoff = obj\n",
    "    handoff_path = p\n",
    "    handoff_ok = True\n",
    "    handoff_src = \"primary\" if ('primaries' in locals() and primaries and p == primaries[0]) else \"fallback\"\n",
    "    break\n",
    "\n",
    "if not handoff_ok:\n",
    "    # æœ€å¾Œã®è©¦è¡Œ: /mnt/data ç›´ä¸‹ã®é›‘å¤šãªå ´æ‰€ã‚’è»½ãã‚¹ã‚­ãƒ£ãƒ³ï¼ˆå®‰å…¨ã®ãŸã‚æ·±ãã¯è¦‹ãªã„ï¼‰\n",
    "    misc_globs = list(Path(\"/mnt/data\").glob(\"**/04-4_agent_implementation.pkl\"))\n",
    "    misc_globs = [p for p in misc_globs if \"/.ipynb_checkpoints/\" not in str(p)]\n",
    "    misc_globs.sort(key=lambda p: p.stat().st_mtime if p.exists() else 0, reverse=True)\n",
    "    for p in misc_globs[:10]:\n",
    "        obj, err = _safe_pickle_load(p)\n",
    "        if err:\n",
    "            _handoff_errors.append((str(p), err))\n",
    "            continue\n",
    "        handoff = _maybe_reconstruct_from_agent_spec(obj)\n",
    "        handoff_path = p\n",
    "        handoff_ok = True\n",
    "        handoff_src = \"fallback-misc\"\n",
    "        break\n",
    "\n",
    "# ãƒ­ã‚°ã¨æ¤œè¨¼\n",
    "if handoff_ok:\n",
    "    print(f\"[OK] handoff ready: {handoff_path} (source={handoff_src})\")\n",
    "    missing = _validate_required_keys(handoff)\n",
    "    if missing:\n",
    "        print(\"[WARN] handoff is missing keys:\", missing)\n",
    "else:\n",
    "    print(\"[ERROR] handoff ãƒ­ãƒ¼ãƒ‰å¤±æ•—å±¥æ­´:\")\n",
    "    for p, err in _handoff_errors[:10]:\n",
    "        print(\"  -\", p, \"->\", err)\n",
    "    print(\"[FATAL] No valid 04-4 handoff found. Please run Module 04-4 to generate it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78577a4-7781-4900-9641-e0293ea6440b",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§ª ã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ã®ã¿ï¼‰\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª:\n",
      "----------------------------------------\n",
      "âŒ VLLM_CONFIG ãŒæœªå®šç¾©\n",
      "   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
      "\n",
      "2ï¸âƒ£ è©•ä¾¡é–¢æ•°ã®ç¢ºèª:\n",
      "----------------------------------------\n",
      "âŒ evaluate_domain ãŒæœªå®šç¾©ã§ã™\n",
      "   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
      "\n",
      "3ï¸âƒ£ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™:\n",
      "----------------------------------------\n",
      "âŒ false_negatives_df ãŒæœªå®šç¾©ã§ã™\n",
      "   ã‚»ãƒ«01ã¾ãŸã¯02ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„\n",
      "\n",
      "4ï¸âƒ£ è©•ä¾¡å®Ÿè¡Œï¼ˆ1ä»¶ãšã¤å‡¦ç†ï¼‰:\n",
      "----------------------------------------\n",
      "\n",
      "âŒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“\n",
      "   å¿…è¦ãªå¤‰æ•°ã¾ãŸã¯é–¢æ•°ãŒæœªå®šç¾©ã§ã™\n",
      "\n",
      "ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
      "----------------------------------------\n",
      "âŒ ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“\n",
      "   ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦ãã ã•ã„\n",
      "\n",
      "================================================================================\n",
      "ğŸ–¥ï¸ GPUä½¿ç”¨çŠ¶æ³\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ GPUä½¿ç”¨å ´æ‰€:\n",
      "  - ãƒ­ãƒ¼ã‚«ãƒ«: XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆäºˆæ¸¬æ™‚ï¼‰\n",
      "  - ãƒªãƒ¢ãƒ¼ãƒˆ: vLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆ192.168.100.71ï¼‰\n",
      "    â†’ Qwen3-14Bãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«GPUã‚’ä½¿ç”¨\n",
      "\n",
      "âš ï¸ æ³¨æ„:\n",
      "  - ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯GPUä½¿ç”¨ã¯è¦‹ãˆã¾ã›ã‚“\n",
      "  - vLLMã‚µãƒ¼ãƒãƒ¼å´ã§GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\n",
      "  - ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã§GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„\n",
      "================================================================================\n",
      "ğŸ” å½é™½æ€§å•é¡Œã®èª¿æŸ»: kobe-denki.co.jp\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ DANGEROUS_TLDSã®å†…å®¹ç¢ºèª:\n",
      "----------------------------------------\n",
      "âŒ DANGEROUS_TLDSãŒæœªå®šç¾©ã§ã™\n",
      "   ã‚»ãƒ«04ã§çµ±è¨ˆçš„ã«ç”Ÿæˆã•ã‚Œã‚‹ã¯ãšã§ã™\n",
      "\n",
      "2ï¸âƒ£ HIGH_RISK_WORDSã®å†…å®¹ç¢ºèª:\n",
      "----------------------------------------\n",
      "âŒ HIGH_RISK_WORDSãŒæœªå®šç¾©ã§ã™\n",
      "\n",
      "3ï¸âƒ£ kobe-denki.co.jpã®å®Ÿéš›ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:\n",
      "----------------------------------------\n",
      "false_negatives_dfãŒæœªå®šç¾©ã§ã™\n",
      "\n",
      "4ï¸âƒ£ å•é¡Œã®åˆ†æ:\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ” ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:\n",
      "\n",
      "âš ï¸ å¯èƒ½æ€§2: ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œ\n",
      "   - 'kobe-denki.co.jp'ãŒ'kobe', 'denki', 'co', 'jp'ã«åˆ†å‰²ã•ã‚Œã‚‹\n",
      "   - 'co'ãŒç‹¬ç«‹ã—ãŸå˜èªã¨ã—ã¦è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹\n",
      "   - è§£æ±ºç­–: TLDéƒ¨åˆ†ï¼ˆ.co.jpï¼‰ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–ã™ã¹ã\n",
      "\n",
      "\n",
      "âš ï¸ å¯èƒ½æ€§3: contextual_risk_assessmentãƒ„ãƒ¼ãƒ«ã®èª¤åˆ¤å®š\n",
      "   - ä½MLç¢ºç‡ï¼ˆ0.144ï¼‰ã‚’ã€Œé€†èª¬çš„ã«å±é™ºã€ã¨è§£é‡ˆ\n",
      "   - .co.jpã‚’å±é™ºã¨èª¤èªè­˜\n",
      "   - è§£æ±ºç­–: æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jp, .ne.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
      "\n",
      "\n",
      "5ï¸âƒ£ ä¿®æ­£ææ¡ˆ:\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“ æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£:\n",
      "\n",
      "1. **HIGH_RISK_WORDSã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**:\n",
      "   ```python\n",
      "   # å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–\n",
      "   problematic = ['co', 'com', 'jp', 'net', 'org']\n",
      "   HIGH_RISK_WORDS = [w for w in HIGH_RISK_WORDS if w not in problematic]\n",
      "   ```\n",
      "\n",
      "2. **æ­£è¦TLDã®å®‰å…¨ãƒªã‚¹ãƒˆä½œæˆ**:\n",
      "   ```python\n",
      "   SAFE_TLDS = [\n",
      "       '.co.jp',  # æ—¥æœ¬ä¼æ¥­\n",
      "       '.ne.jp',  # æ—¥æœ¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
      "       '.or.jp',  # æ—¥æœ¬çµ„ç¹”\n",
      "       '.ac.jp',  # æ—¥æœ¬æ•™è‚²æ©Ÿé–¢\n",
      "       '.go.jp',  # æ—¥æœ¬æ”¿åºœ\n",
      "       '.edu',    # æ•™è‚²æ©Ÿé–¢\n",
      "       '.gov',    # æ”¿åºœæ©Ÿé–¢\n",
      "   ]\n",
      "   ```\n",
      "\n",
      "3. **ãƒ‰ãƒ¡ã‚¤ãƒ³è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„**:\n",
      "   - TLDéƒ¨åˆ†ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–\n",
      "   - æ­£è¦TLDã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’æ¸›ã‚‰ã™\n",
      "   - è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
      "\n",
      "\n",
      "6ï¸âƒ£ ä¿®æ­£ç‰ˆã®å˜èªãƒªã‚¹ãƒˆã‚’ä½œæˆ:\n",
      "----------------------------------------\n",
      "\n",
      "7ï¸âƒ£ kobe-denki.co.jpã®å†è©•ä¾¡ï¼ˆå¯èƒ½ãªå ´åˆï¼‰:\n",
      "----------------------------------------\n",
      "ãƒ‰ãƒ¡ã‚¤ãƒ³: kobe-denki.co.jp\n",
      "  - å…¨ä½“ã®é•·ã•: 16æ–‡å­—\n",
      "  - ãƒ‘ãƒ¼ãƒ„: ['kobe-denki', 'co', 'jp']\n",
      "  - TLD: .co.jp\n",
      "  - ãƒ‰ãƒ¡ã‚¤ãƒ³åéƒ¨åˆ†: kobe-denki\n",
      "  âœ… .co.jpã¯æ—¥æœ¬ã®æ­£è¦ä¼æ¥­/çµ„ç¹”å‘ã‘TLDã§ã™\n",
      "     â†’ ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ­£å½“ãªå¯èƒ½æ€§ãŒé«˜ã„\n",
      "  - æŠ½å‡ºã•ã‚ŒãŸå˜èª: ['kobe', 'denki']\n",
      "  - 'kobe' = ç¥æˆ¸ï¼ˆæ—¥æœ¬ã®éƒ½å¸‚åï¼‰\n",
      "  - 'denki' = é›»æ©Ÿ/é›»æ°—ï¼ˆæ—¥æœ¬èªï¼‰\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š çµè«–:\n",
      "================================================================================\n",
      "\n",
      "kobe-denki.co.jpãŒèª¤åˆ¤å®šã•ã‚Œã‚‹åŸå› :\n",
      "1. 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ï¼ˆæœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ï¼‰\n",
      "2. .co.jpãŒå±é™ºTLDã¨ã—ã¦èª¤èªè­˜ã•ã‚Œã¦ã„ã‚‹\n",
      "3. æ—¥æœ¬ã®æ­£è¦ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹å¾´ãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„\n",
      "\n",
      "æ¨å¥¨å¯¾å¿œ:\n",
      "- HIGH_RISK_WORDSã‹ã‚‰'co'ã‚’é™¤å¤–\n",
      "- æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†\n",
      "- è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 07-SIMPLE\n",
    "æ¦‚è¦: ã‚·ãƒ³ãƒ—ãƒ«ãªå‹•ä½œç¢ºèªç‰ˆï¼ˆã¾ãš5ä»¶ã§ãƒ†ã‚¹ãƒˆï¼‰\n",
    "å…¥åŠ›: false_negatives_df, evaluate_domain\n",
    "å‡ºåŠ›: ãƒ†ã‚¹ãƒˆçµæœã®è¡¨ç¤ºã¨å‹•ä½œç¢ºèª\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ§ª ã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ã®ã¿ï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== 1. vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª ==========\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    # vLLMè¨­å®šã®ç¢ºèª\n",
    "    if 'VLLM_CONFIG' in globals():\n",
    "        print(f\"âœ… VLLMè¨­å®š:\")\n",
    "        print(f\"   - URL: {VLLM_CONFIG['base_url']}\")\n",
    "        print(f\"   - Model: {VLLM_CONFIG['model']}\")\n",
    "        \n",
    "        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ\n",
    "        test_llm = ChatOpenAI(**VLLM_CONFIG)\n",
    "        response = test_llm.invoke(\"Say 'OK' if you are working\")\n",
    "        print(f\"âœ… vLLMã‚µãƒ¼ãƒãƒ¼å¿œç­”ç¢ºèª\")\n",
    "        print(f\"   ã‚µãƒ¼ãƒãƒ¼å´ã§GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "    else:\n",
    "        print(\"âŒ VLLM_CONFIG ãŒæœªå®šç¾©\")\n",
    "        print(\"   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# ========== 2. è©•ä¾¡é–¢æ•°ã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ è©•ä¾¡é–¢æ•°ã®ç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'evaluate_domain' in globals():\n",
    "    print(\"âœ… evaluate_domain é–¢æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "else:\n",
    "    print(\"âŒ evaluate_domain ãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ==========\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'false_negatives_df' not in globals():\n",
    "    print(\"âŒ false_negatives_df ãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«01ã¾ãŸã¯02ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„\")\n",
    "else:\n",
    "    # 5ä»¶ã ã‘å–å¾—\n",
    "    test_samples = false_negatives_df.head(5).copy()\n",
    "    print(f\"âœ… ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«: {len(test_samples)}ä»¶\")\n",
    "    \n",
    "    # MLç¢ºç‡ã‚«ãƒ©ãƒ ã®ç¢ºèª\n",
    "    prob_col = 'prediction_proba' if 'prediction_proba' in test_samples.columns else 'ml_probability'\n",
    "    \n",
    "    print(\"\\nğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³:\")\n",
    "    for idx, row in test_samples.iterrows():\n",
    "        print(f\"   {idx+1}. {row['domain'][:50]:50s} (ML: {row[prob_col]:.3f})\")\n",
    "\n",
    "# ========== 4. 1ä»¶ãšã¤è©•ä¾¡ï¼ˆæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ï¼‰ ==========\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ è©•ä¾¡å®Ÿè¡Œï¼ˆ1ä»¶ãšã¤å‡¦ç†ï¼‰:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def simple_evaluate_one(domain, ml_prob):\n",
    "    \"\"\"1ä»¶ã ã‘è©•ä¾¡ã™ã‚‹æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªé–¢æ•°\"\"\"\n",
    "    \n",
    "    print(f\"\\nè©•ä¾¡ä¸­: {domain}\")\n",
    "    print(f\"  MLç¢ºç‡: {ml_prob:.3f}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # evaluate_domain ã‚’å‘¼ã³å‡ºã—\n",
    "        result = evaluate_domain(domain, ml_prob, enable_trace=False)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # çµæœã®è¡¨ç¤º\n",
    "        is_phishing = result.get('ai_is_phishing', False)\n",
    "        confidence = result.get('ai_confidence', 0.0)\n",
    "        \n",
    "        print(f\"  çµæœ: {'ğŸš¨ ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°' if is_phishing else 'âœ… æ­£å¸¸'}\")\n",
    "        print(f\"  ä¿¡é ¼åº¦: {confidence:.2f}\")\n",
    "        print(f\"  å‡¦ç†æ™‚é–“: {elapsed:.1f}ç§’\")\n",
    "        \n",
    "        # è©³ç´°æƒ…å ±\n",
    "        if result.get('detected_brands'):\n",
    "            print(f\"  æ¤œå‡ºãƒ–ãƒ©ãƒ³ãƒ‰: {', '.join(result['detected_brands'])}\")\n",
    "        if result.get('risk_factors'):\n",
    "            print(f\"  ãƒªã‚¹ã‚¯è¦å› : {', '.join(result['risk_factors'][:3])}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return {\n",
    "            'domain': domain,\n",
    "            'ml_probability': ml_prob,\n",
    "            'ai_is_phishing': False,\n",
    "            'ai_confidence': 0.0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# ========== 5. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ==========\n",
    "\n",
    "if 'test_samples' in locals() and 'evaluate_domain' in globals():\n",
    "    print(\"\\nğŸš€ ãƒ†ã‚¹ãƒˆè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # 1ä»¶ãšã¤å‡¦ç†\n",
    "    for idx, row in test_samples.iterrows():\n",
    "        domain = row['domain']\n",
    "        ml_prob = row[prob_col]\n",
    "        \n",
    "        result = simple_evaluate_one(domain, ml_prob)\n",
    "        results.append(result)\n",
    "        \n",
    "        # å°‘ã—å¾…æ©Ÿï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    total_elapsed = time.time() - total_start\n",
    "    \n",
    "    # ========== 6. çµæœã‚µãƒãƒªãƒ¼ ==========\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # çµ±è¨ˆ\n",
    "    detected = sum(1 for r in results if r.get('ai_is_phishing', False))\n",
    "    errors = sum(1 for r in results if r.get('error'))\n",
    "    \n",
    "    print(f\"\\nçµ±è¨ˆ:\")\n",
    "    print(f\"  - å‡¦ç†ä»¶æ•°: {len(results)}ä»¶\")\n",
    "    print(f\"  - ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°æ¤œå‡º: {detected}ä»¶ ({detected/len(results)*100:.1f}%)\")\n",
    "    print(f\"  - ã‚¨ãƒ©ãƒ¼: {errors}ä»¶\")\n",
    "    print(f\"  - ç·å‡¦ç†æ™‚é–“: {total_elapsed:.1f}ç§’\")\n",
    "    print(f\"  - å¹³å‡å‡¦ç†æ™‚é–“: {total_elapsed/len(results):.1f}ç§’/ä»¶\")\n",
    "    \n",
    "    # çµæœè©³ç´°\n",
    "    print(f\"\\nè©³ç´°çµæœ:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        domain = test_samples.iloc[i-1]['domain']\n",
    "        is_phishing = \"ğŸš¨ ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°\" if result.get('ai_is_phishing') else \"âœ… æ­£å¸¸\"\n",
    "        confidence = result.get('ai_confidence', 0.0)\n",
    "        print(f\"  {i}. {domain[:40]:40s} â†’ {is_phishing} (ä¿¡é ¼åº¦: {confidence:.2f})\")\n",
    "    \n",
    "    # DataFrameã«å¤‰æ›\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… ãƒ†ã‚¹ãƒˆå®Œäº†ï¼\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâŒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“\")\n",
    "    print(\"   å¿…è¦ãªå¤‰æ•°ã¾ãŸã¯é–¢æ•°ãŒæœªå®šç¾©ã§ã™\")\n",
    "\n",
    "# ========== 7. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ==========\n",
    "\n",
    "print(\"\\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'results' in locals() and len(results) > 0:\n",
    "    avg_time = total_elapsed / len(results)\n",
    "    estimated_total = 4215 * avg_time / 60\n",
    "    \n",
    "    print(f\"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸï¼\")\n",
    "    print(f\"   å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.1f}ç§’/ä»¶\")\n",
    "    print(f\"   4,215ä»¶ã®æ¨å®šå‡¦ç†æ™‚é–“: {estimated_total:.1f}åˆ†\")\n",
    "    \n",
    "    print(\"\\nå…¨ä»¶å‡¦ç†ã‚’é–‹å§‹ã™ã‚‹å ´åˆ:\")\n",
    "    print(\"1. ä¸¦åˆ—å‡¦ç†ç‰ˆã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰:\")\n",
    "    print(\"   - ã‚»ãƒ«07-BATCH: 10ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†\")\n",
    "    print(\"   - ã‚»ãƒ«07-PARALLEL: ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ç‰ˆ\")\n",
    "    print(\"\")\n",
    "    print(\"2. ã¾ãŸã¯ã€ã“ã®ã‚»ãƒ«ã‚’æ”¹é€ ã—ã¦å…¨ä»¶å‡¦ç†:\")\n",
    "    print(\"   for idx, row in false_negatives_df.iterrows():\")\n",
    "    print(\"       # å‡¦ç†...\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"   ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 8. GPUä½¿ç”¨çŠ¶æ³ã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ–¥ï¸ GPUä½¿ç”¨çŠ¶æ³\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ“ GPUä½¿ç”¨å ´æ‰€:\")\n",
    "print(\"  - ãƒ­ãƒ¼ã‚«ãƒ«: XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆäºˆæ¸¬æ™‚ï¼‰\")\n",
    "print(\"  - ãƒªãƒ¢ãƒ¼ãƒˆ: vLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆ192.168.100.71ï¼‰\")\n",
    "print(\"    â†’ Qwen3-14Bãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«GPUã‚’ä½¿ç”¨\")\n",
    "print(\"\")\n",
    "print(\"âš ï¸ æ³¨æ„:\")\n",
    "print(\"  - ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯GPUä½¿ç”¨ã¯è¦‹ãˆã¾ã›ã‚“\")\n",
    "print(\"  - vLLMã‚µãƒ¼ãƒãƒ¼å´ã§GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "print(\"  - ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã§GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
    "if 'results' in locals():\n",
    "    globals()['test_results'] = results\n",
    "    globals()['test_results_df'] = results_df\n",
    "    print(\"\\nğŸ’¾ å¤‰æ•°ã‚’ä¿å­˜ã—ã¾ã—ãŸ:\")\n",
    "    print(\"  - test_results: çµæœãƒªã‚¹ãƒˆ\")\n",
    "    print(\"  - test_results_df: çµæœDataFrame\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 08-INVESTIGATE\n",
    "æ¦‚è¦: kobe-denki.co.jpã®å½é™½æ€§å•é¡Œã‚’èª¿æŸ»\n",
    "å…¥åŠ›: DANGEROUS_TLDS, HIGH_RISK_WORDS, false_negatives_df\n",
    "å‡ºåŠ›: å•é¡Œã®åŸå› åˆ†æ\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” å½é™½æ€§å•é¡Œã®èª¿æŸ»: kobe-denki.co.jp\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== 1. DANGEROUS_TLDSã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ DANGEROUS_TLDSã®å†…å®¹ç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'DANGEROUS_TLDS' in globals():\n",
    "    print(f\"DANGEROUS_TLDS ({len(DANGEROUS_TLDS)}å€‹):\")\n",
    "    # .jpã‚„.co.jpãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    jp_related = [tld for tld in DANGEROUS_TLDS if 'jp' in tld or 'co' in tld]\n",
    "    if jp_related:\n",
    "        print(f\"\\nâš ï¸ æ—¥æœ¬é–¢é€£TLDãŒå±é™ºãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã™:\")\n",
    "        for tld in jp_related:\n",
    "            print(f\"   - {tld}\")\n",
    "    else:\n",
    "        print(\"âœ… .jpã‚„.co.jpã¯å±é™ºTLDãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    \n",
    "    # å®Ÿéš›ã®ãƒªã‚¹ãƒˆå†…å®¹ï¼ˆæœ€åˆã®20å€‹ï¼‰\n",
    "    print(f\"\\nå±é™ºTLDãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®20å€‹ï¼‰:\")\n",
    "    for i, tld in enumerate(DANGEROUS_TLDS[:20], 1):\n",
    "        print(f\"   {i:2d}. {tld}\")\n",
    "else:\n",
    "    print(\"âŒ DANGEROUS_TLDSãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«04ã§çµ±è¨ˆçš„ã«ç”Ÿæˆã•ã‚Œã‚‹ã¯ãšã§ã™\")\n",
    "\n",
    "# ========== 2. HIGH_RISK_WORDSã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ HIGH_RISK_WORDSã®å†…å®¹ç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    print(f\"HIGH_RISK_WORDS ({len(HIGH_RISK_WORDS)}å€‹):\")\n",
    "    \n",
    "    # 'co'ã‚„'jp'ãŒå˜èªã¨ã—ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    problematic_words = [word for word in HIGH_RISK_WORDS if word in ['co', 'jp', 'com']]\n",
    "    if problematic_words:\n",
    "        print(f\"\\nâŒ å•é¡Œã®ã‚ã‚‹å˜èªãŒé«˜ãƒªã‚¹ã‚¯ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã™:\")\n",
    "        for word in problematic_words:\n",
    "            print(f\"   - '{word}' â† ã“ã‚ŒãŒåŸå› ã®å¯èƒ½æ€§ï¼\")\n",
    "    else:\n",
    "        print(\"âœ… 'co', 'jp'ãªã©ã¯é«˜ãƒªã‚¹ã‚¯å˜èªã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    \n",
    "    # å®Ÿéš›ã®ãƒªã‚¹ãƒˆå†…å®¹ï¼ˆæœ€åˆã®20å€‹ï¼‰\n",
    "    print(f\"\\né«˜ãƒªã‚¹ã‚¯å˜èªãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®20å€‹ï¼‰:\")\n",
    "    for i, word in enumerate(HIGH_RISK_WORDS[:20], 1):\n",
    "        print(f\"   {i:2d}. {word}\")\n",
    "else:\n",
    "    print(\"âŒ HIGH_RISK_WORDSãŒæœªå®šç¾©ã§ã™\")\n",
    "\n",
    "# ========== 3. kobe-denki.co.jpã®å®Ÿéš›ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ kobe-denki.co.jpã®å®Ÿéš›ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'false_negatives_df' in globals():\n",
    "    # ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒæœ¬å½“ã«å½é™°æ€§ï¼ˆãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ï¼‰ãªã®ã‹ç¢ºèª\n",
    "    kobe_rows = false_negatives_df[false_negatives_df['domain'].str.contains('kobe-denki', na=False)]\n",
    "    \n",
    "    if not kobe_rows.empty:\n",
    "        print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®æƒ…å ±:\")\n",
    "        for idx, row in kobe_rows.iterrows():\n",
    "            print(f\"   - ãƒ‰ãƒ¡ã‚¤ãƒ³: {row['domain']}\")\n",
    "            if 'prediction_proba' in row:\n",
    "                print(f\"   - MLäºˆæ¸¬ç¢ºç‡: {row['prediction_proba']:.3f}\")\n",
    "            if 'is_phishing' in row:\n",
    "                print(f\"   - å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«: {'ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°' if row['is_phishing'] else 'æ­£å¸¸'}\")\n",
    "            print(f\"   - ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: å½é™°æ€§ãƒªã‚¹ãƒˆï¼ˆMLãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ï¼‰\")\n",
    "        \n",
    "        print(\"\\nâš ï¸ é‡è¦ãªè¦³å¯Ÿ:\")\n",
    "        print(\"   ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯å½é™°æ€§ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€\")\n",
    "        print(\"   å®Ÿéš›ã«ã¯ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "        print(\"   ãŸã ã—ã€.co.jpãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ—¥æœ¬ä¼æ¥­ã®æ­£è¦ãƒ‰ãƒ¡ã‚¤ãƒ³ãªã®ã§ã€\")\n",
    "        print(\"   ãƒ‡ãƒ¼ã‚¿ãƒ©ãƒ™ãƒ«ãŒé–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…®ã™ã¹ãã§ã™ã€‚\")\n",
    "    else:\n",
    "        print(\"kobe-denki.co.jpã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "else:\n",
    "    print(\"false_negatives_dfãŒæœªå®šç¾©ã§ã™\")\n",
    "\n",
    "# ========== 4. å•é¡Œã®åˆ†æ ==========\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ å•é¡Œã®åˆ†æ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nğŸ” ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:\")\n",
    "\n",
    "problems = []\n",
    "\n",
    "# å•é¡Œ1: 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦æ‰±ã‚ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§\n",
    "if 'HIGH_RISK_WORDS' in globals() and 'co' in HIGH_RISK_WORDS:\n",
    "    problems.append(\"\"\"\n",
    "âŒ å•é¡Œ1: 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹\n",
    "   - 'co'ã¯.co.jpã‚„.co.ukãªã©ã®æ­£è¦TLDã®ä¸€éƒ¨\n",
    "   - ã“ã‚ŒãŒåŸå› ã§æ­£è¦ã®ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒèª¤åˆ¤å®šã•ã‚Œã‚‹\n",
    "   - è§£æ±ºç­–: HIGH_RISK_WORDSã‹ã‚‰'co'ã‚’é™¤å¤–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹\n",
    "\"\"\")\n",
    "\n",
    "# å•é¡Œ2: å˜èªæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œ\n",
    "problems.append(\"\"\"\n",
    "âš ï¸ å¯èƒ½æ€§2: ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œ\n",
    "   - 'kobe-denki.co.jp'ãŒ'kobe', 'denki', 'co', 'jp'ã«åˆ†å‰²ã•ã‚Œã‚‹\n",
    "   - 'co'ãŒç‹¬ç«‹ã—ãŸå˜èªã¨ã—ã¦è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹\n",
    "   - è§£æ±ºç­–: TLDéƒ¨åˆ†ï¼ˆ.co.jpï¼‰ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–ã™ã¹ã\n",
    "\"\"\")\n",
    "\n",
    "# å•é¡Œ3: contextual_risk_assessmentã®èª¤åˆ¤å®š\n",
    "problems.append(\"\"\"\n",
    "âš ï¸ å¯èƒ½æ€§3: contextual_risk_assessmentãƒ„ãƒ¼ãƒ«ã®èª¤åˆ¤å®š\n",
    "   - ä½MLç¢ºç‡ï¼ˆ0.144ï¼‰ã‚’ã€Œé€†èª¬çš„ã«å±é™ºã€ã¨è§£é‡ˆ\n",
    "   - .co.jpã‚’å±é™ºã¨èª¤èªè­˜\n",
    "   - è§£æ±ºç­–: æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jp, .ne.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "\"\"\")\n",
    "\n",
    "for problem in problems:\n",
    "    print(problem)\n",
    "\n",
    "# ========== 5. ä¿®æ­£ææ¡ˆ ==========\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ ä¿®æ­£ææ¡ˆ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“ æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£:\n",
    "\n",
    "1. **HIGH_RISK_WORDSã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**:\n",
    "   ```python\n",
    "   # å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–\n",
    "   problematic = ['co', 'com', 'jp', 'net', 'org']\n",
    "   HIGH_RISK_WORDS = [w for w in HIGH_RISK_WORDS if w not in problematic]\n",
    "   ```\n",
    "\n",
    "2. **æ­£è¦TLDã®å®‰å…¨ãƒªã‚¹ãƒˆä½œæˆ**:\n",
    "   ```python\n",
    "   SAFE_TLDS = [\n",
    "       '.co.jp',  # æ—¥æœ¬ä¼æ¥­\n",
    "       '.ne.jp',  # æ—¥æœ¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "       '.or.jp',  # æ—¥æœ¬çµ„ç¹”\n",
    "       '.ac.jp',  # æ—¥æœ¬æ•™è‚²æ©Ÿé–¢\n",
    "       '.go.jp',  # æ—¥æœ¬æ”¿åºœ\n",
    "       '.edu',    # æ•™è‚²æ©Ÿé–¢\n",
    "       '.gov',    # æ”¿åºœæ©Ÿé–¢\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **ãƒ‰ãƒ¡ã‚¤ãƒ³è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„**:\n",
    "   - TLDéƒ¨åˆ†ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–\n",
    "   - æ­£è¦TLDã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’æ¸›ã‚‰ã™\n",
    "   - è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
    "\"\"\")\n",
    "\n",
    "# ========== 6. ãƒ†ã‚¹ãƒˆ: ä¿®æ­£ç‰ˆã®å˜èªãƒªã‚¹ãƒˆ ==========\n",
    "\n",
    "print(\"\\n6ï¸âƒ£ ä¿®æ­£ç‰ˆã®å˜èªãƒªã‚¹ãƒˆã‚’ä½œæˆ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    # å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–\n",
    "    problematic = ['co', 'com', 'jp', 'net', 'org', 'www']\n",
    "    HIGH_RISK_WORDS_FIXED = [w for w in HIGH_RISK_WORDS if w not in problematic]\n",
    "    \n",
    "    removed = set(HIGH_RISK_WORDS) - set(HIGH_RISK_WORDS_FIXED)\n",
    "    print(f\"é™¤å¤–ã•ã‚ŒãŸå˜èª: {removed}\")\n",
    "    print(f\"ä¿®æ­£å‰: {len(HIGH_RISK_WORDS)}å€‹\")\n",
    "    print(f\"ä¿®æ­£å¾Œ: {len(HIGH_RISK_WORDS_FIXED)}å€‹\")\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "    globals()['HIGH_RISK_WORDS_FIXED'] = HIGH_RISK_WORDS_FIXED\n",
    "    \n",
    "    print(\"\\nâœ… HIGH_RISK_WORDS_FIXEDã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "    print(\"   ã“ã‚Œã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§å½é™½æ€§ã‚’æ¸›ã‚‰ã›ã¾ã™\")\n",
    "\n",
    "# ========== 7. å†è©•ä¾¡ãƒ†ã‚¹ãƒˆ ==========\n",
    "\n",
    "print(\"\\n7ï¸âƒ£ kobe-denki.co.jpã®å†è©•ä¾¡ï¼ˆå¯èƒ½ãªå ´åˆï¼‰:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def analyze_domain_parts(domain):\n",
    "    \"\"\"ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æ§‹æˆè¦ç´ ã‚’åˆ†æ\"\"\"\n",
    "    parts = domain.split('.')\n",
    "    \n",
    "    print(f\"ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}\")\n",
    "    print(f\"  - å…¨ä½“ã®é•·ã•: {len(domain)}æ–‡å­—\")\n",
    "    print(f\"  - ãƒ‘ãƒ¼ãƒ„: {parts}\")\n",
    "    \n",
    "    # TLDåˆ¤å®š\n",
    "    if len(parts) >= 2:\n",
    "        if len(parts) >= 3 and parts[-2] in ['co', 'ac', 'or', 'ne', 'go']:\n",
    "            tld = f\".{parts[-2]}.{parts[-1]}\"\n",
    "            domain_without_tld = '.'.join(parts[:-2])\n",
    "        else:\n",
    "            tld = f\".{parts[-1]}\"\n",
    "            domain_without_tld = '.'.join(parts[:-1])\n",
    "        \n",
    "        print(f\"  - TLD: {tld}\")\n",
    "        print(f\"  - ãƒ‰ãƒ¡ã‚¤ãƒ³åéƒ¨åˆ†: {domain_without_tld}\")\n",
    "        \n",
    "        # æ—¥æœ¬ã®æ­£è¦TLDã‹ãƒã‚§ãƒƒã‚¯\n",
    "        jp_official_tlds = ['.co.jp', '.ne.jp', '.or.jp', '.ac.jp', '.go.jp']\n",
    "        if tld in jp_official_tlds:\n",
    "            print(f\"  âœ… {tld}ã¯æ—¥æœ¬ã®æ­£è¦ä¼æ¥­/çµ„ç¹”å‘ã‘TLDã§ã™\")\n",
    "            print(f\"     â†’ ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ­£å½“ãªå¯èƒ½æ€§ãŒé«˜ã„\")\n",
    "        \n",
    "        # å˜èªæŠ½å‡ºï¼ˆãƒã‚¤ãƒ•ãƒ³ã§åˆ†å‰²ï¼‰\n",
    "        words = domain_without_tld.replace('-', ' ').replace('_', ' ').split()\n",
    "        print(f\"  - æŠ½å‡ºã•ã‚ŒãŸå˜èª: {words}\")\n",
    "        \n",
    "        # 'kobe'ã¯ç¥æˆ¸ã€'denki'ã¯é›»æ©Ÿ/é›»æ°—\n",
    "        if 'kobe' in words:\n",
    "            print(\"  - 'kobe' = ç¥æˆ¸ï¼ˆæ—¥æœ¬ã®éƒ½å¸‚åï¼‰\")\n",
    "        if 'denki' in words:\n",
    "            print(\"  - 'denki' = é›»æ©Ÿ/é›»æ°—ï¼ˆæ—¥æœ¬èªï¼‰\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "analyze_domain_parts('kobe-denki.co.jp')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š çµè«–:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "kobe-denki.co.jpãŒèª¤åˆ¤å®šã•ã‚Œã‚‹åŸå› :\n",
    "1. 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ï¼ˆæœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ï¼‰\n",
    "2. .co.jpãŒå±é™ºTLDã¨ã—ã¦èª¤èªè­˜ã•ã‚Œã¦ã„ã‚‹\n",
    "3. æ—¥æœ¬ã®æ­£è¦ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹å¾´ãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„\n",
    "\n",
    "æ¨å¥¨å¯¾å¿œ:\n",
    "- HIGH_RISK_WORDSã‹ã‚‰'co'ã‚’é™¤å¤–\n",
    "- æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†\n",
    "- è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8863a58a-8eae-45ae-ba21-b8604e84c732",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”§ ã‚»ãƒ«07-RERUNç”¨ã®é–¢æ•°ä¿®æ­£\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ é–¢æ•°ã‚·ã‚°ãƒãƒãƒ£ã®ç¢ºèª:\n",
      "âŒ process_domains_batch_fixed ãŒæœªå®šç¾©ã§ã™\n",
      "   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
      "\n",
      "======================================================================\n",
      "âœ… é–¢æ•°ä¿®æ­£å®Œäº†\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ ä¿®æ­£å†…å®¹:\n",
      "  1. process_domains_batch_fixed ã®æ­£ã—ã„å‘¼ã³å‡ºã—æ–¹æ³•\n",
      "     - awaitã‚’ä½¿ã‚ãªã„ï¼ˆåŒæœŸé–¢æ•°ã®ãŸã‚ï¼‰\n",
      "     - max_concurrent â†’ batch_size ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
      "     - ThreadPoolExecutorã§éåŒæœŸåŒ–\n",
      "\n",
      "ğŸ“‹ æä¾›ã™ã‚‹é–¢æ•°:\n",
      "  - evaluate_with_fixed_agent_corrected: éåŒæœŸç‰ˆï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰\n",
      "  - evaluate_samples_sync: åŒæœŸç‰ˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰\n",
      "\n",
      "ğŸ¯ æ¨å¥¨ã•ã‚Œã‚‹ä½¿ç”¨æ–¹æ³•:\n",
      "  1. éåŒæœŸç’°å¢ƒï¼ˆJupyterï¼‰:\n",
      "     results_df = await evaluate_with_fixed_agent_corrected(samples_df)\n",
      "  2. åŒæœŸç’°å¢ƒ:\n",
      "     results_df = evaluate_samples_sync(samples_df)\n",
      "\n",
      "ğŸ’¡ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯:\n",
      "   test_evaluation_functions()\n",
      "\n",
      "ã¾ãŸã¯ã€ã‚»ãƒ«07-RERUNã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 07-RERUN-FIX\n",
    "æ¦‚è¦: process_domains_batch_fixedé–¢æ•°ã®å‘¼ã³å‡ºã—æ–¹æ³•ã‚’ä¿®æ­£\n",
    "å…¥åŠ›: evaluate_domain, process_domains_batch_fixed\n",
    "å‡ºåŠ›: ä¿®æ­£ç‰ˆã®evaluate_with_fixed_agenté–¢æ•°\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”§ ã‚»ãƒ«07-RERUNç”¨ã®é–¢æ•°ä¿®æ­£\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1. é–¢æ•°ã®ã‚·ã‚°ãƒãƒãƒ£ã‚’ç¢ºèª ==========\n",
    "\n",
    "print(\"\\nğŸ“‹ é–¢æ•°ã‚·ã‚°ãƒãƒãƒ£ã®ç¢ºèª:\")\n",
    "\n",
    "if 'process_domains_batch_fixed' in globals():\n",
    "    import inspect\n",
    "    sig = inspect.signature(process_domains_batch_fixed)\n",
    "    print(f\"âœ… process_domains_batch_fixed ã®ã‚·ã‚°ãƒãƒãƒ£:\")\n",
    "    print(f\"   {sig}\")\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°\n",
    "    print(\"\\n   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\")\n",
    "    for param_name, param in sig.parameters.items():\n",
    "        default = param.default\n",
    "        if default == inspect.Parameter.empty:\n",
    "            default = \"å¿…é ˆ\"\n",
    "        print(f\"   - {param_name}: {default}\")\n",
    "else:\n",
    "    print(\"âŒ process_domains_batch_fixed ãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 2. ä¿®æ­£ç‰ˆã®evaluate_with_fixed_agenté–¢æ•° ==========\n",
    "\n",
    "async def evaluate_with_fixed_agent_corrected(samples_df: pd.DataFrame, \n",
    "                                              max_concurrent: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ä¿®æ­£ç‰ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§è©•ä¾¡ï¼ˆé–¢æ•°å‘¼ã³å‡ºã—ä¿®æ­£ç‰ˆï¼‰\n",
    "    \n",
    "    Args:\n",
    "        samples_df: è©•ä¾¡å¯¾è±¡ã®DataFrame\n",
    "        max_concurrent: æœ€å¤§ä¸¦åˆ—æ•°\n",
    "    \n",
    "    Returns:\n",
    "        è©•ä¾¡çµæœã®DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    if samples_df.empty:\n",
    "        print(\"âŒ No samples to evaluate\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # MLç¢ºç‡ã‚«ãƒ©ãƒ ã®ç¢ºèª\n",
    "    prob_col = 'prediction_proba' if 'prediction_proba' in samples_df.columns else 'ml_probability'\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "    domains = samples_df['domain'].tolist()\n",
    "    ml_probabilities = samples_df[prob_col].tolist()\n",
    "    \n",
    "    print(f\"\\nâš¡ è©•ä¾¡é–‹å§‹:\")\n",
    "    print(f\"  - å¯¾è±¡: {len(domains)}ä»¶\")\n",
    "    print(f\"  - ä¸¦åˆ—æ•°: {max_concurrent}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # process_domains_batch_fixedã¯åŒæœŸé–¢æ•°ãªã®ã§ã€ThreadPoolExecutorã§éåŒæœŸåŒ–\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    # ä¿®æ­£ç‰ˆé–¢æ•°ã‚’ä½¿ç”¨ï¼ˆæ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ï¼‰\n",
    "    if 'process_domains_batch_fixed' in globals():\n",
    "        print(\"  - ä½¿ç”¨: process_domains_batch_fixedï¼ˆä¿®æ­£ç‰ˆãƒ»åŒæœŸï¼‰\")\n",
    "        \n",
    "        # ThreadPoolExecutorã§éåŒæœŸå®Ÿè¡Œ\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = loop.run_in_executor(\n",
    "                executor,\n",
    "                process_domains_batch_fixed,\n",
    "                domains,           # ç¬¬1å¼•æ•°: domains\n",
    "                ml_probabilities,  # ç¬¬2å¼•æ•°: ml_probabilities\n",
    "                max_concurrent,    # ç¬¬3å¼•æ•°: batch_sizeï¼ˆmax_concurrentã‚’ä½¿ç”¨ï¼‰\n",
    "                False             # ç¬¬4å¼•æ•°: enable_trace\n",
    "            )\n",
    "            results = await future\n",
    "            \n",
    "    elif 'process_domains_batch' in globals():\n",
    "        print(\"  - ä½¿ç”¨: process_domains_batchï¼ˆå…ƒç‰ˆãƒ»éåŒæœŸï¼‰\")\n",
    "        \n",
    "        # (domain, ml_probability)ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "        domains_data = list(zip(domains, ml_probabilities))\n",
    "        \n",
    "        # éåŒæœŸç‰ˆã‚’ç›´æ¥å‘¼ã³å‡ºã—\n",
    "        results = await process_domains_batch(domains_data, max_concurrent=max_concurrent)\n",
    "    else:\n",
    "        print(\"âŒ è©•ä¾¡é–¢æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # çµæœã‚’DataFrameã«å¤‰æ›\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # å…ƒã®DataFrameã®æƒ…å ±ã‚’è¿½åŠ \n",
    "    if not results_df.empty and not samples_df.empty:\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã‚‹\n",
    "        results_df.index = samples_df.index[:len(results_df)]\n",
    "        \n",
    "        # å…ƒã®ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰\n",
    "        for col in samples_df.columns:\n",
    "            if col not in results_df.columns:\n",
    "                results_df[col] = samples_df[col]\n",
    "    \n",
    "    print(f\"\\nâœ… è©•ä¾¡å®Œäº†:\")\n",
    "    print(f\"  - å‡¦ç†æ™‚é–“: {elapsed:.2f}ç§’\")\n",
    "    print(f\"  - å‡¦ç†é€Ÿåº¦: {len(domains)/elapsed:.2f} ãƒ‰ãƒ¡ã‚¤ãƒ³/ç§’\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ========== 3. åˆ¥ã®å®Ÿè£…æ–¹æ³•ï¼ˆã‚ˆã‚Šç°¡å˜ï¼‰ ==========\n",
    "\n",
    "def evaluate_samples_sync(samples_df: pd.DataFrame, \n",
    "                         batch_size: int = 24) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åŒæœŸçš„ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’è©•ä¾¡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰\n",
    "    \n",
    "    Args:\n",
    "        samples_df: è©•ä¾¡å¯¾è±¡ã®DataFrame\n",
    "        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    \n",
    "    Returns:\n",
    "        è©•ä¾¡çµæœã®DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    if samples_df.empty:\n",
    "        print(\"âŒ No samples to evaluate\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # MLç¢ºç‡ã‚«ãƒ©ãƒ ã®ç¢ºèª\n",
    "    prob_col = 'prediction_proba' if 'prediction_proba' in samples_df.columns else 'ml_probability'\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "    domains = samples_df['domain'].tolist()\n",
    "    ml_probabilities = samples_df[prob_col].tolist()\n",
    "    \n",
    "    print(f\"\\nâš¡ åŒæœŸè©•ä¾¡é–‹å§‹:\")\n",
    "    print(f\"  - å¯¾è±¡: {len(domains)}ä»¶\")\n",
    "    print(f\"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # è©•ä¾¡å®Ÿè¡Œ\n",
    "    if 'process_domains_batch_fixed' in globals():\n",
    "        results = process_domains_batch_fixed(\n",
    "            domains, \n",
    "            ml_probabilities, \n",
    "            batch_size=batch_size,\n",
    "            enable_trace=False\n",
    "        )\n",
    "    elif 'evaluate_domain' in globals():\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å€‹åˆ¥ã«è©•ä¾¡\n",
    "        print(\"  âš ï¸ process_domains_batch_fixedãŒæœªå®šç¾©ã€‚å€‹åˆ¥è©•ä¾¡ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\")\n",
    "        results = []\n",
    "        for domain, ml_prob in zip(domains, ml_probabilities):\n",
    "            result = evaluate_domain(domain, ml_prob, enable_trace=False)\n",
    "            results.append(result)\n",
    "    else:\n",
    "        print(\"âŒ è©•ä¾¡é–¢æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # çµæœã‚’DataFrameã«å¤‰æ›\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\nâœ… è©•ä¾¡å®Œäº†:\")\n",
    "    print(f\"  - å‡¦ç†æ™‚é–“: {elapsed:.2f}ç§’\")\n",
    "    print(f\"  - å‡¦ç†é€Ÿåº¦: {len(domains)/elapsed:.2f} ãƒ‰ãƒ¡ã‚¤ãƒ³/ç§’\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ========== 4. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ ==========\n",
    "\n",
    "globals()['evaluate_with_fixed_agent_corrected'] = evaluate_with_fixed_agent_corrected\n",
    "globals()['evaluate_samples_sync'] = evaluate_samples_sync\n",
    "\n",
    "# å…ƒã®é–¢æ•°ã‚’ä¸Šæ›¸ãï¼ˆã‚»ãƒ«07-RERUNã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰\n",
    "globals()['evaluate_with_fixed_agent'] = evaluate_with_fixed_agent_corrected\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… é–¢æ•°ä¿®æ­£å®Œäº†\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“‹ ä¿®æ­£å†…å®¹:\")\n",
    "print(\"  1. process_domains_batch_fixed ã®æ­£ã—ã„å‘¼ã³å‡ºã—æ–¹æ³•\")\n",
    "print(\"     - awaitã‚’ä½¿ã‚ãªã„ï¼ˆåŒæœŸé–¢æ•°ã®ãŸã‚ï¼‰\")\n",
    "print(\"     - max_concurrent â†’ batch_size ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\")\n",
    "print(\"     - ThreadPoolExecutorã§éåŒæœŸåŒ–\")\n",
    "\n",
    "print(\"\\nğŸ“‹ æä¾›ã™ã‚‹é–¢æ•°:\")\n",
    "print(\"  - evaluate_with_fixed_agent_corrected: éåŒæœŸç‰ˆï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰\")\n",
    "print(\"  - evaluate_samples_sync: åŒæœŸç‰ˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰\")\n",
    "\n",
    "print(\"\\nğŸ¯ æ¨å¥¨ã•ã‚Œã‚‹ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"  1. éåŒæœŸç’°å¢ƒï¼ˆJupyterï¼‰:\")\n",
    "print(\"     results_df = await evaluate_with_fixed_agent_corrected(samples_df)\")\n",
    "print(\"  2. åŒæœŸç’°å¢ƒ:\")\n",
    "print(\"     results_df = evaluate_samples_sync(samples_df)\")\n",
    "\n",
    "# ========== 5. ãƒ†ã‚¹ãƒˆ ==========\n",
    "\n",
    "def test_evaluation_functions():\n",
    "    \"\"\"è©•ä¾¡é–¢æ•°ã®ãƒ†ã‚¹ãƒˆ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ§ª è©•ä¾¡é–¢æ•°ã®ãƒ†ã‚¹ãƒˆ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ\n",
    "    test_data = pd.DataFrame({\n",
    "        'domain': ['test-phishing.com', 'normal-site.org'],\n",
    "        'prediction_proba': [0.15, 0.85]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:\")\n",
    "    print(test_data)\n",
    "    \n",
    "    try:\n",
    "        # åŒæœŸç‰ˆãƒ†ã‚¹ãƒˆ\n",
    "        print(\"\\n1ï¸âƒ£ åŒæœŸç‰ˆãƒ†ã‚¹ãƒˆ:\")\n",
    "        results = evaluate_samples_sync(test_data, batch_size=2)\n",
    "        if not results.empty:\n",
    "            print(\"âœ… æˆåŠŸ\")\n",
    "            print(f\"   çµæœ: {len(results)}ä»¶\")\n",
    "        else:\n",
    "            print(\"âŒ å¤±æ•—: ç©ºã®çµæœ\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    \n",
    "    print(\"\\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®ææ¡ˆ\n",
    "print(\"\\nğŸ’¡ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯:\")\n",
    "print(\"   test_evaluation_functions()\")\n",
    "print(\"\\nã¾ãŸã¯ã€ã‚»ãƒ«07-RERUNã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad2a7466",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§ª ã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ã®ã¿ï¼‰\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª:\n",
      "----------------------------------------\n",
      "âŒ VLLM_CONFIG ãŒæœªå®šç¾©\n",
      "   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
      "\n",
      "2ï¸âƒ£ è©•ä¾¡é–¢æ•°ã®ç¢ºèª:\n",
      "----------------------------------------\n",
      "âŒ evaluate_domain ãŒæœªå®šç¾©ã§ã™\n",
      "   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
      "\n",
      "3ï¸âƒ£ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™:\n",
      "----------------------------------------\n",
      "âŒ false_negatives_df ãŒæœªå®šç¾©ã§ã™\n",
      "   ã‚»ãƒ«01ã¾ãŸã¯02ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„\n",
      "\n",
      "4ï¸âƒ£ è©•ä¾¡å®Ÿè¡Œï¼ˆ1ä»¶ãšã¤å‡¦ç†ï¼‰:\n",
      "----------------------------------------\n",
      "\n",
      "âŒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“\n",
      "   å¿…è¦ãªå¤‰æ•°ã¾ãŸã¯é–¢æ•°ãŒæœªå®šç¾©ã§ã™\n",
      "\n",
      "ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
      "----------------------------------------\n",
      "âŒ ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“\n",
      "   ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦ãã ã•ã„\n",
      "\n",
      "================================================================================\n",
      "ğŸ–¥ï¸ GPUä½¿ç”¨çŠ¶æ³\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ GPUä½¿ç”¨å ´æ‰€:\n",
      "  - ãƒ­ãƒ¼ã‚«ãƒ«: XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆäºˆæ¸¬æ™‚ï¼‰\n",
      "  - ãƒªãƒ¢ãƒ¼ãƒˆ: vLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆ192.168.100.71ï¼‰\n",
      "    â†’ Qwen3-14Bãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«GPUã‚’ä½¿ç”¨\n",
      "\n",
      "âš ï¸ æ³¨æ„:\n",
      "  - ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯GPUä½¿ç”¨ã¯è¦‹ãˆã¾ã›ã‚“\n",
      "  - vLLMã‚µãƒ¼ãƒãƒ¼å´ã§GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\n",
      "  - ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã§GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„\n",
      "================================================================================\n",
      "ğŸ” å½é™½æ€§å•é¡Œã®èª¿æŸ»: kobe-denki.co.jp\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ DANGEROUS_TLDSã®å†…å®¹ç¢ºèª:\n",
      "----------------------------------------\n",
      "âŒ DANGEROUS_TLDSãŒæœªå®šç¾©ã§ã™\n",
      "   ã‚»ãƒ«04ã§çµ±è¨ˆçš„ã«ç”Ÿæˆã•ã‚Œã‚‹ã¯ãšã§ã™\n",
      "\n",
      "2ï¸âƒ£ HIGH_RISK_WORDSã®å†…å®¹ç¢ºèª:\n",
      "----------------------------------------\n",
      "âŒ HIGH_RISK_WORDSãŒæœªå®šç¾©ã§ã™\n",
      "\n",
      "3ï¸âƒ£ kobe-denki.co.jpã®å®Ÿéš›ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:\n",
      "----------------------------------------\n",
      "false_negatives_dfãŒæœªå®šç¾©ã§ã™\n",
      "\n",
      "4ï¸âƒ£ å•é¡Œã®åˆ†æ:\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ” ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:\n",
      "\n",
      "âš ï¸ å¯èƒ½æ€§2: ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œ\n",
      "   - 'kobe-denki.co.jp'ãŒ'kobe', 'denki', 'co', 'jp'ã«åˆ†å‰²ã•ã‚Œã‚‹\n",
      "   - 'co'ãŒç‹¬ç«‹ã—ãŸå˜èªã¨ã—ã¦è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹\n",
      "   - è§£æ±ºç­–: TLDéƒ¨åˆ†ï¼ˆ.co.jpï¼‰ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–ã™ã¹ã\n",
      "\n",
      "\n",
      "âš ï¸ å¯èƒ½æ€§3: contextual_risk_assessmentãƒ„ãƒ¼ãƒ«ã®èª¤åˆ¤å®š\n",
      "   - ä½MLç¢ºç‡ï¼ˆ0.144ï¼‰ã‚’ã€Œé€†èª¬çš„ã«å±é™ºã€ã¨è§£é‡ˆ\n",
      "   - .co.jpã‚’å±é™ºã¨èª¤èªè­˜\n",
      "   - è§£æ±ºç­–: æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jp, .ne.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
      "\n",
      "\n",
      "5ï¸âƒ£ ä¿®æ­£ææ¡ˆ:\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“ æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£:\n",
      "\n",
      "1. **HIGH_RISK_WORDSã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**:\n",
      "   ```python\n",
      "   # å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–\n",
      "   problematic = ['co', 'com', 'jp', 'net', 'org']\n",
      "   HIGH_RISK_WORDS = [w for w in HIGH_RISK_WORDS if w not in problematic]\n",
      "   ```\n",
      "\n",
      "2. **æ­£è¦TLDã®å®‰å…¨ãƒªã‚¹ãƒˆä½œæˆ**:\n",
      "   ```python\n",
      "   SAFE_TLDS = [\n",
      "       '.co.jp',  # æ—¥æœ¬ä¼æ¥­\n",
      "       '.ne.jp',  # æ—¥æœ¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
      "       '.or.jp',  # æ—¥æœ¬çµ„ç¹”\n",
      "       '.ac.jp',  # æ—¥æœ¬æ•™è‚²æ©Ÿé–¢\n",
      "       '.go.jp',  # æ—¥æœ¬æ”¿åºœ\n",
      "       '.edu',    # æ•™è‚²æ©Ÿé–¢\n",
      "       '.gov',    # æ”¿åºœæ©Ÿé–¢\n",
      "   ]\n",
      "   ```\n",
      "\n",
      "3. **ãƒ‰ãƒ¡ã‚¤ãƒ³è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„**:\n",
      "   - TLDéƒ¨åˆ†ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–\n",
      "   - æ­£è¦TLDã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’æ¸›ã‚‰ã™\n",
      "   - è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
      "\n",
      "\n",
      "6ï¸âƒ£ ä¿®æ­£ç‰ˆã®å˜èªãƒªã‚¹ãƒˆã‚’ä½œæˆ:\n",
      "----------------------------------------\n",
      "\n",
      "7ï¸âƒ£ kobe-denki.co.jpã®å†è©•ä¾¡ï¼ˆå¯èƒ½ãªå ´åˆï¼‰:\n",
      "----------------------------------------\n",
      "ãƒ‰ãƒ¡ã‚¤ãƒ³: kobe-denki.co.jp\n",
      "  - å…¨ä½“ã®é•·ã•: 16æ–‡å­—\n",
      "  - ãƒ‘ãƒ¼ãƒ„: ['kobe-denki', 'co', 'jp']\n",
      "  - TLD: .co.jp\n",
      "  - ãƒ‰ãƒ¡ã‚¤ãƒ³åéƒ¨åˆ†: kobe-denki\n",
      "  âœ… .co.jpã¯æ—¥æœ¬ã®æ­£è¦ä¼æ¥­/çµ„ç¹”å‘ã‘TLDã§ã™\n",
      "     â†’ ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ­£å½“ãªå¯èƒ½æ€§ãŒé«˜ã„\n",
      "  - æŠ½å‡ºã•ã‚ŒãŸå˜èª: ['kobe', 'denki']\n",
      "  - 'kobe' = ç¥æˆ¸ï¼ˆæ—¥æœ¬ã®éƒ½å¸‚åï¼‰\n",
      "  - 'denki' = é›»æ©Ÿ/é›»æ°—ï¼ˆæ—¥æœ¬èªï¼‰\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š çµè«–:\n",
      "================================================================================\n",
      "\n",
      "kobe-denki.co.jpãŒèª¤åˆ¤å®šã•ã‚Œã‚‹åŸå› :\n",
      "1. 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ï¼ˆæœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ï¼‰\n",
      "2. .co.jpãŒå±é™ºTLDã¨ã—ã¦èª¤èªè­˜ã•ã‚Œã¦ã„ã‚‹\n",
      "3. æ—¥æœ¬ã®æ­£è¦ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹å¾´ãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„\n",
      "\n",
      "æ¨å¥¨å¯¾å¿œ:\n",
      "- HIGH_RISK_WORDSã‹ã‚‰'co'ã‚’é™¤å¤–\n",
      "- æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†\n",
      "- è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 07-SIMPLE\n",
    "æ¦‚è¦: ã‚·ãƒ³ãƒ—ãƒ«ãªå‹•ä½œç¢ºèªç‰ˆï¼ˆã¾ãš5ä»¶ã§ãƒ†ã‚¹ãƒˆï¼‰\n",
    "å…¥åŠ›: false_negatives_df, evaluate_domain\n",
    "å‡ºåŠ›: ãƒ†ã‚¹ãƒˆçµæœã®è¡¨ç¤ºã¨å‹•ä½œç¢ºèª\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ§ª ã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ã®ã¿ï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== 1. vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª ==========\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    # vLLMè¨­å®šã®ç¢ºèª\n",
    "    if 'VLLM_CONFIG' in globals():\n",
    "        print(f\"âœ… VLLMè¨­å®š:\")\n",
    "        print(f\"   - URL: {VLLM_CONFIG['base_url']}\")\n",
    "        print(f\"   - Model: {VLLM_CONFIG['model']}\")\n",
    "        \n",
    "        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ\n",
    "        test_llm = ChatOpenAI(**VLLM_CONFIG)\n",
    "        response = test_llm.invoke(\"Say 'OK' if you are working\")\n",
    "        print(f\"âœ… vLLMã‚µãƒ¼ãƒãƒ¼å¿œç­”ç¢ºèª\")\n",
    "        print(f\"   ã‚µãƒ¼ãƒãƒ¼å´ã§GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "    else:\n",
    "        print(\"âŒ VLLM_CONFIG ãŒæœªå®šç¾©\")\n",
    "        print(\"   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# ========== 2. è©•ä¾¡é–¢æ•°ã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ è©•ä¾¡é–¢æ•°ã®ç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'evaluate_domain' in globals():\n",
    "    print(\"âœ… evaluate_domain é–¢æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "else:\n",
    "    print(\"âŒ evaluate_domain ãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ==========\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'false_negatives_df' not in globals():\n",
    "    print(\"âŒ false_negatives_df ãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«01ã¾ãŸã¯02ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„\")\n",
    "else:\n",
    "    # 5ä»¶ã ã‘å–å¾—\n",
    "    test_samples = false_negatives_df.head(5).copy()\n",
    "    print(f\"âœ… ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«: {len(test_samples)}ä»¶\")\n",
    "    \n",
    "    # MLç¢ºç‡ã‚«ãƒ©ãƒ ã®ç¢ºèª\n",
    "    prob_col = 'prediction_proba' if 'prediction_proba' in test_samples.columns else 'ml_probability'\n",
    "    \n",
    "    print(\"\\nğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³:\")\n",
    "    for idx, row in test_samples.iterrows():\n",
    "        print(f\"   {idx+1}. {row['domain'][:50]:50s} (ML: {row[prob_col]:.3f})\")\n",
    "\n",
    "# ========== 4. 1ä»¶ãšã¤è©•ä¾¡ï¼ˆæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ï¼‰ ==========\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ è©•ä¾¡å®Ÿè¡Œï¼ˆ1ä»¶ãšã¤å‡¦ç†ï¼‰:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def simple_evaluate_one(domain, ml_prob):\n",
    "    \"\"\"1ä»¶ã ã‘è©•ä¾¡ã™ã‚‹æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªé–¢æ•°\"\"\"\n",
    "    \n",
    "    print(f\"\\nè©•ä¾¡ä¸­: {domain}\")\n",
    "    print(f\"  MLç¢ºç‡: {ml_prob:.3f}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # evaluate_domain ã‚’å‘¼ã³å‡ºã—\n",
    "        result = evaluate_domain(domain, ml_prob, enable_trace=False)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # çµæœã®è¡¨ç¤º\n",
    "        is_phishing = result.get('ai_is_phishing', False)\n",
    "        confidence = result.get('ai_confidence', 0.0)\n",
    "        \n",
    "        print(f\"  çµæœ: {'ğŸš¨ ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°' if is_phishing else 'âœ… æ­£å¸¸'}\")\n",
    "        print(f\"  ä¿¡é ¼åº¦: {confidence:.2f}\")\n",
    "        print(f\"  å‡¦ç†æ™‚é–“: {elapsed:.1f}ç§’\")\n",
    "        \n",
    "        # è©³ç´°æƒ…å ±\n",
    "        if result.get('detected_brands'):\n",
    "            print(f\"  æ¤œå‡ºãƒ–ãƒ©ãƒ³ãƒ‰: {', '.join(result['detected_brands'])}\")\n",
    "        if result.get('risk_factors'):\n",
    "            print(f\"  ãƒªã‚¹ã‚¯è¦å› : {', '.join(result['risk_factors'][:3])}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return {\n",
    "            'domain': domain,\n",
    "            'ml_probability': ml_prob,\n",
    "            'ai_is_phishing': False,\n",
    "            'ai_confidence': 0.0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# ========== 5. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ==========\n",
    "\n",
    "if 'test_samples' in locals() and 'evaluate_domain' in globals():\n",
    "    print(\"\\nğŸš€ ãƒ†ã‚¹ãƒˆè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # 1ä»¶ãšã¤å‡¦ç†\n",
    "    for idx, row in test_samples.iterrows():\n",
    "        domain = row['domain']\n",
    "        ml_prob = row[prob_col]\n",
    "        \n",
    "        result = simple_evaluate_one(domain, ml_prob)\n",
    "        results.append(result)\n",
    "        \n",
    "        # å°‘ã—å¾…æ©Ÿï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    total_elapsed = time.time() - total_start\n",
    "    \n",
    "    # ========== 6. çµæœã‚µãƒãƒªãƒ¼ ==========\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # çµ±è¨ˆ\n",
    "    detected = sum(1 for r in results if r.get('ai_is_phishing', False))\n",
    "    errors = sum(1 for r in results if r.get('error'))\n",
    "    \n",
    "    print(f\"\\nçµ±è¨ˆ:\")\n",
    "    print(f\"  - å‡¦ç†ä»¶æ•°: {len(results)}ä»¶\")\n",
    "    print(f\"  - ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°æ¤œå‡º: {detected}ä»¶ ({detected/len(results)*100:.1f}%)\")\n",
    "    print(f\"  - ã‚¨ãƒ©ãƒ¼: {errors}ä»¶\")\n",
    "    print(f\"  - ç·å‡¦ç†æ™‚é–“: {total_elapsed:.1f}ç§’\")\n",
    "    print(f\"  - å¹³å‡å‡¦ç†æ™‚é–“: {total_elapsed/len(results):.1f}ç§’/ä»¶\")\n",
    "    \n",
    "    # çµæœè©³ç´°\n",
    "    print(f\"\\nè©³ç´°çµæœ:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        domain = test_samples.iloc[i-1]['domain']\n",
    "        is_phishing = \"ğŸš¨ ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°\" if result.get('ai_is_phishing') else \"âœ… æ­£å¸¸\"\n",
    "        confidence = result.get('ai_confidence', 0.0)\n",
    "        print(f\"  {i}. {domain[:40]:40s} â†’ {is_phishing} (ä¿¡é ¼åº¦: {confidence:.2f})\")\n",
    "    \n",
    "    # DataFrameã«å¤‰æ›\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… ãƒ†ã‚¹ãƒˆå®Œäº†ï¼\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâŒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“\")\n",
    "    print(\"   å¿…è¦ãªå¤‰æ•°ã¾ãŸã¯é–¢æ•°ãŒæœªå®šç¾©ã§ã™\")\n",
    "\n",
    "# ========== 7. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ==========\n",
    "\n",
    "print(\"\\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'results' in locals() and len(results) > 0:\n",
    "    avg_time = total_elapsed / len(results)\n",
    "    estimated_total = 4215 * avg_time / 60\n",
    "    \n",
    "    print(f\"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸï¼\")\n",
    "    print(f\"   å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.1f}ç§’/ä»¶\")\n",
    "    print(f\"   4,215ä»¶ã®æ¨å®šå‡¦ç†æ™‚é–“: {estimated_total:.1f}åˆ†\")\n",
    "    \n",
    "    print(\"\\nå…¨ä»¶å‡¦ç†ã‚’é–‹å§‹ã™ã‚‹å ´åˆ:\")\n",
    "    print(\"1. ä¸¦åˆ—å‡¦ç†ç‰ˆã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰:\")\n",
    "    print(\"   - ã‚»ãƒ«07-BATCH: 10ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†\")\n",
    "    print(\"   - ã‚»ãƒ«07-PARALLEL: ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ç‰ˆ\")\n",
    "    print(\"\")\n",
    "    print(\"2. ã¾ãŸã¯ã€ã“ã®ã‚»ãƒ«ã‚’æ”¹é€ ã—ã¦å…¨ä»¶å‡¦ç†:\")\n",
    "    print(\"   for idx, row in false_negatives_df.iterrows():\")\n",
    "    print(\"       # å‡¦ç†...\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"   ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 8. GPUä½¿ç”¨çŠ¶æ³ã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ–¥ï¸ GPUä½¿ç”¨çŠ¶æ³\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ“ GPUä½¿ç”¨å ´æ‰€:\")\n",
    "print(\"  - ãƒ­ãƒ¼ã‚«ãƒ«: XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆäºˆæ¸¬æ™‚ï¼‰\")\n",
    "print(\"  - ãƒªãƒ¢ãƒ¼ãƒˆ: vLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆ192.168.100.71ï¼‰\")\n",
    "print(\"    â†’ Qwen3-14Bãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«GPUã‚’ä½¿ç”¨\")\n",
    "print(\"\")\n",
    "print(\"âš ï¸ æ³¨æ„:\")\n",
    "print(\"  - ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯GPUä½¿ç”¨ã¯è¦‹ãˆã¾ã›ã‚“\")\n",
    "print(\"  - vLLMã‚µãƒ¼ãƒãƒ¼å´ã§GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "print(\"  - ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã§GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
    "if 'results' in locals():\n",
    "    globals()['test_results'] = results\n",
    "    globals()['test_results_df'] = results_df\n",
    "    print(\"\\nğŸ’¾ å¤‰æ•°ã‚’ä¿å­˜ã—ã¾ã—ãŸ:\")\n",
    "    print(\"  - test_results: çµæœãƒªã‚¹ãƒˆ\")\n",
    "    print(\"  - test_results_df: çµæœDataFrame\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 08-INVESTIGATE\n",
    "æ¦‚è¦: kobe-denki.co.jpã®å½é™½æ€§å•é¡Œã‚’èª¿æŸ»\n",
    "å…¥åŠ›: DANGEROUS_TLDS, HIGH_RISK_WORDS, false_negatives_df\n",
    "å‡ºåŠ›: å•é¡Œã®åŸå› åˆ†æ\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” å½é™½æ€§å•é¡Œã®èª¿æŸ»: kobe-denki.co.jp\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== 1. DANGEROUS_TLDSã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ DANGEROUS_TLDSã®å†…å®¹ç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'DANGEROUS_TLDS' in globals():\n",
    "    print(f\"DANGEROUS_TLDS ({len(DANGEROUS_TLDS)}å€‹):\")\n",
    "    # .jpã‚„.co.jpãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    jp_related = [tld for tld in DANGEROUS_TLDS if 'jp' in tld or 'co' in tld]\n",
    "    if jp_related:\n",
    "        print(f\"\\nâš ï¸ æ—¥æœ¬é–¢é€£TLDãŒå±é™ºãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã™:\")\n",
    "        for tld in jp_related:\n",
    "            print(f\"   - {tld}\")\n",
    "    else:\n",
    "        print(\"âœ… .jpã‚„.co.jpã¯å±é™ºTLDãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    \n",
    "    # å®Ÿéš›ã®ãƒªã‚¹ãƒˆå†…å®¹ï¼ˆæœ€åˆã®20å€‹ï¼‰\n",
    "    print(f\"\\nå±é™ºTLDãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®20å€‹ï¼‰:\")\n",
    "    for i, tld in enumerate(DANGEROUS_TLDS[:20], 1):\n",
    "        print(f\"   {i:2d}. {tld}\")\n",
    "else:\n",
    "    print(\"âŒ DANGEROUS_TLDSãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«04ã§çµ±è¨ˆçš„ã«ç”Ÿæˆã•ã‚Œã‚‹ã¯ãšã§ã™\")\n",
    "\n",
    "# ========== 2. HIGH_RISK_WORDSã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ HIGH_RISK_WORDSã®å†…å®¹ç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    print(f\"HIGH_RISK_WORDS ({len(HIGH_RISK_WORDS)}å€‹):\")\n",
    "    \n",
    "    # 'co'ã‚„'jp'ãŒå˜èªã¨ã—ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    problematic_words = [word for word in HIGH_RISK_WORDS if word in ['co', 'jp', 'com']]\n",
    "    if problematic_words:\n",
    "        print(f\"\\nâŒ å•é¡Œã®ã‚ã‚‹å˜èªãŒé«˜ãƒªã‚¹ã‚¯ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã™:\")\n",
    "        for word in problematic_words:\n",
    "            print(f\"   - '{word}' â† ã“ã‚ŒãŒåŸå› ã®å¯èƒ½æ€§ï¼\")\n",
    "    else:\n",
    "        print(\"âœ… 'co', 'jp'ãªã©ã¯é«˜ãƒªã‚¹ã‚¯å˜èªã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    \n",
    "    # å®Ÿéš›ã®ãƒªã‚¹ãƒˆå†…å®¹ï¼ˆæœ€åˆã®20å€‹ï¼‰\n",
    "    print(f\"\\né«˜ãƒªã‚¹ã‚¯å˜èªãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®20å€‹ï¼‰:\")\n",
    "    for i, word in enumerate(HIGH_RISK_WORDS[:20], 1):\n",
    "        print(f\"   {i:2d}. {word}\")\n",
    "else:\n",
    "    print(\"âŒ HIGH_RISK_WORDSãŒæœªå®šç¾©ã§ã™\")\n",
    "\n",
    "# ========== 3. kobe-denki.co.jpã®å®Ÿéš›ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ kobe-denki.co.jpã®å®Ÿéš›ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'false_negatives_df' in globals():\n",
    "    # ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒæœ¬å½“ã«å½é™°æ€§ï¼ˆãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ï¼‰ãªã®ã‹ç¢ºèª\n",
    "    kobe_rows = false_negatives_df[false_negatives_df['domain'].str.contains('kobe-denki', na=False)]\n",
    "    \n",
    "    if not kobe_rows.empty:\n",
    "        print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®æƒ…å ±:\")\n",
    "        for idx, row in kobe_rows.iterrows():\n",
    "            print(f\"   - ãƒ‰ãƒ¡ã‚¤ãƒ³: {row['domain']}\")\n",
    "            if 'prediction_proba' in row:\n",
    "                print(f\"   - MLäºˆæ¸¬ç¢ºç‡: {row['prediction_proba']:.3f}\")\n",
    "            if 'is_phishing' in row:\n",
    "                print(f\"   - å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«: {'ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°' if row['is_phishing'] else 'æ­£å¸¸'}\")\n",
    "            print(f\"   - ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: å½é™°æ€§ãƒªã‚¹ãƒˆï¼ˆMLãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ï¼‰\")\n",
    "        \n",
    "        print(\"\\nâš ï¸ é‡è¦ãªè¦³å¯Ÿ:\")\n",
    "        print(\"   ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯å½é™°æ€§ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€\")\n",
    "        print(\"   å®Ÿéš›ã«ã¯ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "        print(\"   ãŸã ã—ã€.co.jpãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ—¥æœ¬ä¼æ¥­ã®æ­£è¦ãƒ‰ãƒ¡ã‚¤ãƒ³ãªã®ã§ã€\")\n",
    "        print(\"   ãƒ‡ãƒ¼ã‚¿ãƒ©ãƒ™ãƒ«ãŒé–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…®ã™ã¹ãã§ã™ã€‚\")\n",
    "    else:\n",
    "        print(\"kobe-denki.co.jpã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "else:\n",
    "    print(\"false_negatives_dfãŒæœªå®šç¾©ã§ã™\")\n",
    "\n",
    "# ========== 4. å•é¡Œã®åˆ†æ ==========\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ å•é¡Œã®åˆ†æ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nğŸ” ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:\")\n",
    "\n",
    "problems = []\n",
    "\n",
    "# å•é¡Œ1: 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦æ‰±ã‚ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§\n",
    "if 'HIGH_RISK_WORDS' in globals() and 'co' in HIGH_RISK_WORDS:\n",
    "    problems.append(\"\"\"\n",
    "âŒ å•é¡Œ1: 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹\n",
    "   - 'co'ã¯.co.jpã‚„.co.ukãªã©ã®æ­£è¦TLDã®ä¸€éƒ¨\n",
    "   - ã“ã‚ŒãŒåŸå› ã§æ­£è¦ã®ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒèª¤åˆ¤å®šã•ã‚Œã‚‹\n",
    "   - è§£æ±ºç­–: HIGH_RISK_WORDSã‹ã‚‰'co'ã‚’é™¤å¤–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹\n",
    "\"\"\")\n",
    "\n",
    "# å•é¡Œ2: å˜èªæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œ\n",
    "problems.append(\"\"\"\n",
    "âš ï¸ å¯èƒ½æ€§2: ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œ\n",
    "   - 'kobe-denki.co.jp'ãŒ'kobe', 'denki', 'co', 'jp'ã«åˆ†å‰²ã•ã‚Œã‚‹\n",
    "   - 'co'ãŒç‹¬ç«‹ã—ãŸå˜èªã¨ã—ã¦è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹\n",
    "   - è§£æ±ºç­–: TLDéƒ¨åˆ†ï¼ˆ.co.jpï¼‰ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–ã™ã¹ã\n",
    "\"\"\")\n",
    "\n",
    "# å•é¡Œ3: contextual_risk_assessmentã®èª¤åˆ¤å®š\n",
    "problems.append(\"\"\"\n",
    "âš ï¸ å¯èƒ½æ€§3: contextual_risk_assessmentãƒ„ãƒ¼ãƒ«ã®èª¤åˆ¤å®š\n",
    "   - ä½MLç¢ºç‡ï¼ˆ0.144ï¼‰ã‚’ã€Œé€†èª¬çš„ã«å±é™ºã€ã¨è§£é‡ˆ\n",
    "   - .co.jpã‚’å±é™ºã¨èª¤èªè­˜\n",
    "   - è§£æ±ºç­–: æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jp, .ne.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "\"\"\")\n",
    "\n",
    "for problem in problems:\n",
    "    print(problem)\n",
    "\n",
    "# ========== 5. ä¿®æ­£ææ¡ˆ ==========\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ ä¿®æ­£ææ¡ˆ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“ æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£:\n",
    "\n",
    "1. **HIGH_RISK_WORDSã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**:\n",
    "   ```python\n",
    "   # å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–\n",
    "   problematic = ['co', 'com', 'jp', 'net', 'org']\n",
    "   HIGH_RISK_WORDS = [w for w in HIGH_RISK_WORDS if w not in problematic]\n",
    "   ```\n",
    "\n",
    "2. **æ­£è¦TLDã®å®‰å…¨ãƒªã‚¹ãƒˆä½œæˆ**:\n",
    "   ```python\n",
    "   SAFE_TLDS = [\n",
    "       '.co.jp',  # æ—¥æœ¬ä¼æ¥­\n",
    "       '.ne.jp',  # æ—¥æœ¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "       '.or.jp',  # æ—¥æœ¬çµ„ç¹”\n",
    "       '.ac.jp',  # æ—¥æœ¬æ•™è‚²æ©Ÿé–¢\n",
    "       '.go.jp',  # æ—¥æœ¬æ”¿åºœ\n",
    "       '.edu',    # æ•™è‚²æ©Ÿé–¢\n",
    "       '.gov',    # æ”¿åºœæ©Ÿé–¢\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **ãƒ‰ãƒ¡ã‚¤ãƒ³è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„**:\n",
    "   - TLDéƒ¨åˆ†ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–\n",
    "   - æ­£è¦TLDã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’æ¸›ã‚‰ã™\n",
    "   - è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
    "\"\"\")\n",
    "\n",
    "# ========== 6. ãƒ†ã‚¹ãƒˆ: ä¿®æ­£ç‰ˆã®å˜èªãƒªã‚¹ãƒˆ ==========\n",
    "\n",
    "print(\"\\n6ï¸âƒ£ ä¿®æ­£ç‰ˆã®å˜èªãƒªã‚¹ãƒˆã‚’ä½œæˆ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    # å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–\n",
    "    problematic = ['co', 'com', 'jp', 'net', 'org', 'www']\n",
    "    HIGH_RISK_WORDS_FIXED = [w for w in HIGH_RISK_WORDS if w not in problematic]\n",
    "    \n",
    "    removed = set(HIGH_RISK_WORDS) - set(HIGH_RISK_WORDS_FIXED)\n",
    "    print(f\"é™¤å¤–ã•ã‚ŒãŸå˜èª: {removed}\")\n",
    "    print(f\"ä¿®æ­£å‰: {len(HIGH_RISK_WORDS)}å€‹\")\n",
    "    print(f\"ä¿®æ­£å¾Œ: {len(HIGH_RISK_WORDS_FIXED)}å€‹\")\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "    globals()['HIGH_RISK_WORDS_FIXED'] = HIGH_RISK_WORDS_FIXED\n",
    "    \n",
    "    print(\"\\nâœ… HIGH_RISK_WORDS_FIXEDã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "    print(\"   ã“ã‚Œã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§å½é™½æ€§ã‚’æ¸›ã‚‰ã›ã¾ã™\")\n",
    "\n",
    "# ========== 7. å†è©•ä¾¡ãƒ†ã‚¹ãƒˆ ==========\n",
    "\n",
    "print(\"\\n7ï¸âƒ£ kobe-denki.co.jpã®å†è©•ä¾¡ï¼ˆå¯èƒ½ãªå ´åˆï¼‰:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def analyze_domain_parts(domain):\n",
    "    \"\"\"ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æ§‹æˆè¦ç´ ã‚’åˆ†æ\"\"\"\n",
    "    parts = domain.split('.')\n",
    "    \n",
    "    print(f\"ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}\")\n",
    "    print(f\"  - å…¨ä½“ã®é•·ã•: {len(domain)}æ–‡å­—\")\n",
    "    print(f\"  - ãƒ‘ãƒ¼ãƒ„: {parts}\")\n",
    "    \n",
    "    # TLDåˆ¤å®š\n",
    "    if len(parts) >= 2:\n",
    "        if len(parts) >= 3 and parts[-2] in ['co', 'ac', 'or', 'ne', 'go']:\n",
    "            tld = f\".{parts[-2]}.{parts[-1]}\"\n",
    "            domain_without_tld = '.'.join(parts[:-2])\n",
    "        else:\n",
    "            tld = f\".{parts[-1]}\"\n",
    "            domain_without_tld = '.'.join(parts[:-1])\n",
    "        \n",
    "        print(f\"  - TLD: {tld}\")\n",
    "        print(f\"  - ãƒ‰ãƒ¡ã‚¤ãƒ³åéƒ¨åˆ†: {domain_without_tld}\")\n",
    "        \n",
    "        # æ—¥æœ¬ã®æ­£è¦TLDã‹ãƒã‚§ãƒƒã‚¯\n",
    "        jp_official_tlds = ['.co.jp', '.ne.jp', '.or.jp', '.ac.jp', '.go.jp']\n",
    "        if tld in jp_official_tlds:\n",
    "            print(f\"  âœ… {tld}ã¯æ—¥æœ¬ã®æ­£è¦ä¼æ¥­/çµ„ç¹”å‘ã‘TLDã§ã™\")\n",
    "            print(f\"     â†’ ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ­£å½“ãªå¯èƒ½æ€§ãŒé«˜ã„\")\n",
    "        \n",
    "        # å˜èªæŠ½å‡ºï¼ˆãƒã‚¤ãƒ•ãƒ³ã§åˆ†å‰²ï¼‰\n",
    "        words = domain_without_tld.replace('-', ' ').replace('_', ' ').split()\n",
    "        print(f\"  - æŠ½å‡ºã•ã‚ŒãŸå˜èª: {words}\")\n",
    "        \n",
    "        # 'kobe'ã¯ç¥æˆ¸ã€'denki'ã¯é›»æ©Ÿ/é›»æ°—\n",
    "        if 'kobe' in words:\n",
    "            print(\"  - 'kobe' = ç¥æˆ¸ï¼ˆæ—¥æœ¬ã®éƒ½å¸‚åï¼‰\")\n",
    "        if 'denki' in words:\n",
    "            print(\"  - 'denki' = é›»æ©Ÿ/é›»æ°—ï¼ˆæ—¥æœ¬èªï¼‰\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "analyze_domain_parts('kobe-denki.co.jp')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š çµè«–:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "kobe-denki.co.jpãŒèª¤åˆ¤å®šã•ã‚Œã‚‹åŸå› :\n",
    "1. 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ï¼ˆæœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ï¼‰\n",
    "2. .co.jpãŒå±é™ºTLDã¨ã—ã¦èª¤èªè­˜ã•ã‚Œã¦ã„ã‚‹\n",
    "3. æ—¥æœ¬ã®æ­£è¦ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹å¾´ãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„\n",
    "\n",
    "æ¨å¥¨å¯¾å¿œ:\n",
    "- HIGH_RISK_WORDSã‹ã‚‰'co'ã‚’é™¤å¤–\n",
    "- æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†\n",
    "- è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b43ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GUARD] load_configuration is ready.\n"
     ]
    }
   ],
   "source": [
    "# === Guard: ensure `load_configuration` exists (self-contained, _compat-first) ===\n",
    "try:\n",
    "    load_configuration  # noqa: F821\n",
    "except NameError:\n",
    "    import sys, json\n",
    "    from pathlib import Path\n",
    "    from typing import Optional, Dict, Any\n",
    "    # ensure _compat on path\n",
    "    if \"_compat\" not in sys.path:\n",
    "        sys.path.append(\"_compat\")\n",
    "    # try to import from _compat/paths.py\n",
    "    try:\n",
    "        from paths import ensure_roots, load_config as _load_config  # resolved from _compat/paths.py\n",
    "    except Exception as _e:\n",
    "        raise ImportError(\"Failed to import _compat/paths.py as `paths`. Add `_compat` to sys.path or fix environment.\") from _e\n",
    "    try:\n",
    "        ensure_roots()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    def _deep_update(base: Dict[str, Any], upd: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        if not isinstance(base, dict):\n",
    "            return upd or {}\n",
    "        if not isinstance(upd, dict):\n",
    "            return base\n",
    "        for k, v in upd.items():\n",
    "            if isinstance(v, dict) and isinstance(base.get(k), dict):\n",
    "                base[k] = _deep_update(base.get(k, {}), v)\n",
    "            else:\n",
    "                base[k] = v\n",
    "        return base\n",
    "\n",
    "    def load_configuration(cfg_override: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "        # prefer _load_config(); fallback to _compat/config.json\n",
    "        base = {}\n",
    "        try:\n",
    "            base = _load_config()\n",
    "        except Exception:\n",
    "            cfg_path = Path(\"_compat/config.json\")\n",
    "            if not cfg_path.exists():\n",
    "                raise FileNotFoundError(f\"Missing config: {cfg_path}\")\n",
    "            base = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "        return _deep_update(dict(base), cfg_override or {})\n",
    "print(\"[GUARD] load_configuration is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20213f2a",
   "metadata": {
    "title": "Cell 25: Controller API é–¢æ•° (NEW)"
   },
   "outputs": [],
   "source": [
    "# === Cell 25: Controller API é–¢æ•° (NEW) ===\n",
    "from typing import Tuple, Dict, Any\n",
    "import os, json, time, traceback\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def _safe_get(d: dict, k: str):\n",
    "    return d[k] if isinstance(d, dict) and k in d else None\n",
    "\n",
    "def generalization_study(session_id: str, cfg_in: Dict[str, Any]) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"ReturnCode: \"OK\" | \"NOT_FOUND\" | \"INVALID_INPUT\" | \"ERROR\"\"\"\n",
    "    try:\n",
    "        cfg_local = load_configuration(cfg_override=cfg_in or {})\n",
    "        dev_mode = bool(cfg_local[\"system\"][\"development_mode\"])\n",
    "        sample_size = int(cfg_local[\"analysis\"][\"sample_size\"]) if dev_mode else None\n",
    "\n",
    "        handoff_path = None\n",
    "        candidates = [\n",
    "            os.path.join(globals().get(\"HANDOFF_DIR\",\"handoff\"), \"03_ai_agent_analysis_part3.pkl\"),\n",
    "            \"handoff/03_ai_agent_analysis_part3.pkl\"\n",
    "        ]\n",
    "        for p in candidates:\n",
    "            if os.path.exists(p):\n",
    "                handoff_path = p; break\n",
    "        if not handoff_path:\n",
    "            return \"NOT_FOUND\", {\"error\": \"handoff(part3) not found\"}\n",
    "\n",
    "        handoff = joblib.load(handoff_path)\n",
    "\n",
    "        fn_df  = _safe_get(handoff, \"false_negatives_df\")\n",
    "        brands = _safe_get(handoff, \"brand_keywords\")\n",
    "        tlds_d = _safe_get(handoff, \"DANGEROUS_TLDS\")\n",
    "        certs  = _safe_get(handoff, \"cert_full_info_map\")\n",
    "\n",
    "        missing = []\n",
    "        if fn_df is None or not hasattr(fn_df, \"shape\"): missing.append(\"false_negatives_df\")\n",
    "        if not brands or len(brands) < 64: missing.append(\"brand_keywords(>=64)\")\n",
    "        if not tlds_d: missing.append(\"DANGEROUS_TLDS\")\n",
    "        if certs is None: missing.append(\"cert_full_info_map\")\n",
    "        if missing:\n",
    "            return \"NOT_FOUND\", {\"error\": f\"handoff keys missing: {missing}\"}\n",
    "\n",
    "        target_df = fn_df.copy()\n",
    "        if dev_mode:\n",
    "            n = min(sample_size or 50, len(target_df))\n",
    "            if \"prediction_proba\" in target_df.columns:\n",
    "                low = target_df[target_df[\"prediction_proba\"] < 0.2]\n",
    "                rest = target_df[target_df[\"prediction_proba\"] >= 0.2]\n",
    "                take_low = min(int(n*0.6), len(low))\n",
    "                take_rest = n - take_low\n",
    "                frames = []\n",
    "                if take_low>0 and len(low)>0:\n",
    "                    frames.append(low.sample(n=take_low, random_state=42))\n",
    "                if take_rest>0 and len(rest)>0:\n",
    "                    frames.append(rest.sample(n=take_rest, random_state=42))\n",
    "                target_df = pd.concat(frames, ignore_index=True) if frames else target_df.sample(n=n, random_state=42)\n",
    "            else:\n",
    "                target_df = target_df.sample(n=n, random_state=42)\n",
    "\n",
    "        if \"domain\" not in target_df.columns:\n",
    "            return \"INVALID_INPUT\", {\"error\": \"false_negatives_df has no 'domain' column\"}\n",
    "        domains = target_df[\"domain\"].astype(str).tolist()\n",
    "        probs = (target_df[\"prediction_proba\"].astype(float).tolist()\n",
    "                 if \"prediction_proba\" in target_df.columns else [0.5]*len(domains))\n",
    "\n",
    "        start = time.time()\n",
    "        try:\n",
    "            if \"process_domains_batch\" in globals():\n",
    "                import asyncio\n",
    "                async def _run():\n",
    "                    return await process_domains_batch(list(zip(domains, probs)),\n",
    "                                                       max_concurrent=cfg_local[\"llm\"][\"max_concurrent\"])\n",
    "                loop = asyncio.new_event_loop()\n",
    "                asyncio.set_event_loop(loop)\n",
    "                results = loop.run_until_complete(_run())\n",
    "                loop.close()\n",
    "            else:\n",
    "                results = []\n",
    "                for d, p in zip(domains, probs):\n",
    "                    results.append(evaluate_domain(d, p))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ LLM error: {e} -> deterministic fallback\")\n",
    "            results = []\n",
    "            for d, p in zip(domains, probs):\n",
    "                is_phish = (p < 0.2)\n",
    "                results.append({\n",
    "                    'domain': d, 'ml_probability': p, 'ai_is_phishing': is_phish,\n",
    "                    'ai_confidence': 0.6 if is_phish else 0.3, 'success': True,\n",
    "                    'ai_risk_level': 'high' if is_phish else 'low', 'deterministic': True\n",
    "                })\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        detected = sum(1 for r in results if isinstance(r, dict) and r.get(\"ai_is_phishing\"))\n",
    "        pr_summary = {\n",
    "            \"evaluated\": len(results),\n",
    "            \"ai_detected\": detected,\n",
    "            \"elapsed_sec\": elapsed,\n",
    "        }\n",
    "        threshold_rec = {\"recommended_threshold\": 0.5, \"rationale\": \"placeholder (optimize in report)\"}\n",
    "\n",
    "        run_tag = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_results = Path(cfg_local[\"paths\"][\"results_dir\"]) / str(session_id) / \"generalization\"\n",
    "        base_logs    = Path(cfg_local[\"paths\"][\"logs_dir\"]) / \"generalization\"\n",
    "        base_results.mkdir(parents=True, exist_ok=True)\n",
    "        base_logs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        paths = {\n",
    "            \"generalization_results\": str(base_results / f\"generalization_results_{run_tag}.pkl\"),\n",
    "            \"pr_analysis\":            str(base_results / f\"pr_analysis_{run_tag}.json\"),\n",
    "            \"threshold_recommendation\": str(base_results / f\"threshold_recommendation_{run_tag}.json\"),\n",
    "            \"comparison_report\":      str(base_results / f\"comparison_report_{run_tag}.html\"),\n",
    "            \"evaluation_csv\":         str(base_results / f\"evaluation_results_{run_tag}.csv\"),\n",
    "            \"logs\":                   str(base_logs    / f\"generalization_study_{run_tag}.log\"),\n",
    "        }\n",
    "\n",
    "        import joblib as _joblib\n",
    "        _joblib.dump({\"results\": results, \"config\": cfg_local}, paths[\"generalization_results\"])\n",
    "        Path(paths[\"pr_analysis\"]).write_text(json.dumps(pr_summary, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "        Path(paths[\"threshold_recommendation\"]).write_text(json.dumps(threshold_rec, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "        html = f\"\"\"\n",
    "        <html><body>\n",
    "        <h1>Generalization Study</h1>\n",
    "        <p>session: {session_id}</p>\n",
    "        <ul>\n",
    "          <li>evaluated: {pr_summary['evaluated']}</li>\n",
    "          <li>ai_detected: {pr_summary['ai_detected']}</li>\n",
    "          <li>elapsed_sec: {pr_summary['elapsed_sec']:.2f}</li>\n",
    "          <li>threshold recommendation: {threshold_rec['recommended_threshold']}</li>\n",
    "        </ul>\n",
    "        </body></html>\n",
    "        \"\"\"\n",
    "        Path(paths[\"comparison_report\"]).write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "        try:\n",
    "            pd.DataFrame(results).to_csv(paths[\"evaluation_csv\"], index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return \"OK\", paths\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        return \"NOT_FOUND\", {\"error\": str(e)}\n",
    "    except ValueError as e:\n",
    "        return \"INVALID_INPUT\", {\"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        import traceback as _tb\n",
    "        return \"ERROR\", {\"error\": f\"{e}\", \"trace\": _tb.format_exc()[:2000]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44f910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReturnCode: NOT_FOUND\n",
      "Paths: {'error': 'handoff(part3) not found'}\n",
      "[OK] metrics saved: artifacts/2025-10-22_074213/results/performance_metrics_20251026_180030.json\n",
      "[OK] error_analysis saved: artifacts/2025-10-22_074213/results/error_analysis_20251026_180030.json\n",
      "[OK] handoff saved: artifacts/2025-10-22_074213/handoff/04-5_evaluation_results.pkl\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell RUN: è©•ä¾¡å®Ÿè¡Œãƒ©ãƒ³ãƒŠãƒ¼\n",
    "# - configã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼ˆSAMPLES, MAX_WORKERS, SAMPLINGï¼‰\n",
    "# - generalization_study(session_id, cfg_in) ã‚’å‘¼ã³å‡ºã—\n",
    "# - å®Ÿè¡Œå¾Œã«çµæœãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’ä¿å­˜ï¼ˆå¯èƒ½ãªé™ã‚Šï¼‰\n",
    "\n",
    "from datetime import datetime\n",
    "import os, json, pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# configã®è»½é‡ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰è¾æ›¸ã‚’æ§‹æˆ\n",
    "override = {\n",
    "    \"system\": {\n",
    "        \"development_mode\": bool(cfg[\"system\"][\"development_mode\"]),\n",
    "        \"seed\": int(cfg[\"system\"].get(\"seed\", 42))\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"samples\": int(SAMPLES),\n",
    "        \"max_workers\": int(MAX_WORKERS),\n",
    "        \"sampling\": SAMPLING\n",
    "    }\n",
    "}\n",
    "\n",
    "# å®Ÿè¡Œï¼ˆ03ç³»ã®ã‚·ã‚°ãƒãƒãƒ£: generalization_study(session_id, cfg_in)ï¼‰\n",
    "try:\n",
    "    ret = generalization_study(session_id=RUN_ID, cfg_in=override)\n",
    "except NameError:\n",
    "    print(\"[ERROR] generalization_study é–¢æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Cell 25ã®ç§»æ¤ã‚’ã”ç¢ºèªãã ã•ã„ã€‚\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] è©•ä¾¡å®Ÿè¡Œã§ä¾‹å¤–ãŒç™ºç”Ÿ:\", e)\n",
    "    raise\n",
    "\n",
    "# è¿”ã‚Šå€¤ã®è§£é‡ˆ: (ReturnCode, Paths) ã¾ãŸã¯ dict\n",
    "evaluation_results = None\n",
    "performance_metrics = {}\n",
    "error_analysis = {}\n",
    "test_results = {}\n",
    "\n",
    "if isinstance(ret, tuple) and len(ret) == 2:\n",
    "    return_code, paths_map = ret\n",
    "    print(\"ReturnCode:\", return_code)\n",
    "    print(\"Paths:\", paths_map)\n",
    "    # æ—¢çŸ¥ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ‘ã‚¹ã«å«ã¾ã‚Œã¦ã„ã‚Œã°èª­è¾¼ã‚’è©¦ã¿ã‚‹\n",
    "    def _load_json(p):\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            return None\n",
    "    def _load_csv_df(p):\n",
    "        try:\n",
    "            return pd.read_csv(p)\n",
    "        except Exception:\n",
    "            return None\n",
    "    # æœŸå¾…ã•ã‚Œã‚‹ã‚­ãƒ¼ã®è©¦è¡Œçš„èª­è¾¼\n",
    "    if isinstance(paths_map, dict):\n",
    "        evaluation_results = _load_csv_df(paths_map.get(\"evaluation_results_csv\", \"\")) or evaluation_results\n",
    "        performance_metrics = _load_json(paths_map.get(\"performance_metrics_json\", \"\")) or performance_metrics\n",
    "        error_analysis = _load_json(paths_map.get(\"error_analysis_json\", \"\")) or error_analysis\n",
    "        # ãã®ä»–ã®ä¸­é–“æˆæœã‚‚ã‚ã‚Œã°ã“ã“ã§è¿½åŠ èª­è¾¼å¯èƒ½\n",
    "elif isinstance(ret, dict):\n",
    "    evaluation_results = ret.get(\"evaluation_results\")\n",
    "    performance_metrics = ret.get(\"performance_metrics\", {})\n",
    "    error_analysis = ret.get(\"error_analysis\", {})\n",
    "    test_results = ret.get(\"test_results\", {})\n",
    "else:\n",
    "    print(\"[WARN] è¿”ã‚Šå€¤ã®å½¢å¼ãŒæœªå¯¾å¿œã§ã™:\", type(ret))\n",
    "\n",
    "# çµæœå‡ºåŠ›ãƒ‘ã‚¹\n",
    "results_dir = Path(BASE_DIRS[\"results\"])\n",
    "handoff_dir = Path(BASE_DIRS[\"handoff\"])\n",
    "\n",
    "csv_path = results_dir / f\"evaluation_results_{timestamp}.csv\"\n",
    "perf_path = results_dir / f\"performance_metrics_{timestamp}.json\"\n",
    "err_path = results_dir / f\"error_analysis_{timestamp}.json\"\n",
    "handoff_out = handoff_dir / \"04-5_evaluation_results.pkl\"\n",
    "\n",
    "# ä¿å­˜\n",
    "try:\n",
    "    if evaluation_results is not None and hasattr(evaluation_results, \"to_csv\"):\n",
    "        evaluation_results.to_csv(csv_path, index=False)\n",
    "        print(\"[OK] CSV saved:\", csv_path)\n",
    "    with open(perf_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(performance_metrics, f, ensure_ascii=False, indent=2)\n",
    "        print(\"[OK] metrics saved:\", perf_path)\n",
    "    with open(err_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(error_analysis, f, ensure_ascii=False, indent=2)\n",
    "        print(\"[OK] error_analysis saved:\", err_path)\n",
    "    # handoff pkl\n",
    "    with open(handoff_out, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"evaluation_results\": evaluation_results,\n",
    "            \"performance_metrics\": performance_metrics,\n",
    "            \"error_analysis\": error_analysis,\n",
    "            \"test_results\": test_results,\n",
    "            \"cfg\": cfg,\n",
    "            \"override\": override,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"run_id\": RUN_ID\n",
    "        }, f)\n",
    "        print(\"[OK] handoff saved:\", handoff_out)\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼:\", e)\n",
    "    raise\n",
    "\n",
    "print(\"DONE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e47e75a4",
   "metadata": {
    "tags": [
     "C-04",
     "restore",
     "evaluation_compat"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C-04] Using RUN_ID: 2025-10-22_074213\n",
      "[C-04] 04-4 handoff keys: []\n",
      "[C-04] 04-3 setup keys: [] ...\n",
      "[C-04] 04-4 notebook not found; cannot import agent\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Agent import failed. Please ensure 04-4 notebook is present.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 103\u001b[0m\n\u001b[1;32m    101\u001b[0m AGENT_OK \u001b[38;5;241m=\u001b[39m _import_agent_from_04_4()\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m AGENT_OK:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent import failed. Please ensure 04-4 notebook is present.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# build agent instance\u001b[39;00m\n\u001b[1;32m    106\u001b[0m agent \u001b[38;5;241m=\u001b[39m make_fixed_agent(run_id\u001b[38;5;241m=\u001b[39mRUN_ID, trace_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Agent import failed. Please ensure 04-4 notebook is present."
     ]
    }
   ],
   "source": [
    "# === C-04 Evaluation Compatibility Layer (04-5) ===\n",
    "# Purpose: make 04-5 robust even if original cells are incomplete.\n",
    "# This cell:\n",
    "#  1) Resolves RUN_ID (latest artifacts/* that has 04-4 handoff)\n",
    "#  2) Loads 04-4 handoff and 04-3 setup\n",
    "#  3) Dynamically imports the agent class by reading the 04-4 notebook (C-02 cell)\n",
    "#  4) Defines evaluation functions if missing: prepare_evaluation_samples, execute_evaluation,\n",
    "#     analyze_evaluation_results, generate_evaluation_report\n",
    "#  5) Runs a minimal evaluation and writes results under artifacts/{RUN_ID}/results\n",
    "import os, json, pickle, glob, io, re, time, math, random, datetime as _dt\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- RUN_ID resolution ----------\n",
    "def _detect_run_id() -> str:\n",
    "    rid = os.getenv(\"RUN_ID\")\n",
    "    if rid:\n",
    "        return rid\n",
    "    # find newest artifacts/* with 04-4 or 04-3 handoff\n",
    "    art_root = \"/mnt/data/artifacts\"\n",
    "    if not os.path.isdir(art_root):\n",
    "        return _dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    cands = []\n",
    "    for d in glob.glob(os.path.join(art_root, \"*\")):\n",
    "        if not os.path.isdir(d): continue\n",
    "        hdir = os.path.join(d, \"handoff\")\n",
    "        if not os.path.isdir(hdir): continue\n",
    "        has_04_4 = os.path.exists(os.path.join(hdir, \"04-4_agent_implementation.pkl\"))\n",
    "        has_04_3 = os.path.exists(os.path.join(hdir, \"04-3_llm_tools_setup_with_tools.pkl\"))\n",
    "        if has_04_4 or has_04_3:\n",
    "            cands.append((os.path.getmtime(d), os.path.basename(d)))\n",
    "    if not cands:\n",
    "        return _dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    cands.sort(reverse=True)\n",
    "    return cands[0][1]\n",
    "\n",
    "RUN_ID = _detect_run_id()\n",
    "print(f\"[C-04] Using RUN_ID: {RUN_ID}\")\n",
    "\n",
    "# ---------- handoff loaders ----------\n",
    "def load_04_4_handoff(run_id: Optional[str]=None) -> Dict[str, Any]:\n",
    "    run_id = run_id or RUN_ID\n",
    "    p = os.path.join(\"/mnt/data/artifacts\", run_id, \"handoff\", \"04-4_agent_implementation.pkl\")\n",
    "    if not os.path.exists(p):\n",
    "        return {}\n",
    "    with open(p, \"rb\") as f:\n",
    "        return pickle.load(f) or {}\n",
    "\n",
    "def load_04_3_setup(run_id: Optional[str]=None) -> Dict[str, Any]:\n",
    "    run_id = run_id or RUN_ID\n",
    "    p = os.path.join(\"/mnt/data/artifacts\", run_id, \"handoff\", \"04-3_llm_tools_setup_with_tools.pkl\")\n",
    "    if not os.path.exists(p):\n",
    "        return {}\n",
    "    with open(p, \"rb\") as f:\n",
    "        return pickle.load(f) or {}\n",
    "\n",
    "H4 = load_04_4_handoff(RUN_ID)\n",
    "H3 = load_04_3_setup(RUN_ID)\n",
    "print(\"[C-04] 04-4 handoff keys:\", sorted(H4.keys()))\n",
    "print(\"[C-04] 04-3 setup keys:\", sorted(H3.keys())[:12], \"...\")\n",
    "\n",
    "# ---------- dynamically import agent from 04-4 notebook (C-02 cell) ----------\n",
    "def _import_agent_from_04_4():\n",
    "    # try the patched filename (latest)\n",
    "    cand = \"/mnt/data/PATCHED_04-4_agent_implementation_WITH_CELL6_1761400078.ipynb\"\n",
    "    if not os.path.exists(cand):\n",
    "        # fallback to any 04-4 patched file\n",
    "        cands = sorted(glob.glob(\"/mnt/data/PATCHED_04-4_*.ipynb\")) or sorted(glob.glob(\"/mnt/data/*04-4*agent*.ipynb\"))\n",
    "        cand = cands[-1] if cands else None\n",
    "    if not cand:\n",
    "        print(\"[C-04] 04-4 notebook not found; cannot import agent\")\n",
    "        return False\n",
    "\n",
    "    with open(cand, \"r\", encoding=\"utf-8\") as f:\n",
    "        nb_04_4 = json.load(f)\n",
    "\n",
    "    # find C-02 cell or a cell that defines FixedPhishingDetectionAgent\n",
    "    code = None\n",
    "    for cell in nb_04_4.get(\"cells\", []):\n",
    "        if cell.get(\"cell_type\") != \"code\":\n",
    "            continue\n",
    "        meta = cell.get(\"metadata\", {})\n",
    "        tags = meta.get(\"tags\", [])\n",
    "        src = \"\".join(cell.get(\"source\", \"\"))\n",
    "        if \"C-02\" in tags or \"FixedPhishingDetectionAgent\" in src:\n",
    "            code = src\n",
    "            break\n",
    "    if not code:\n",
    "        print(\"[C-04] C-02 cell not found in 04-4; cannot import agent\")\n",
    "        return False\n",
    "\n",
    "    # execute code in this kernel (defines make_fixed_agent, class, etc.)\n",
    "    g = globals()\n",
    "    try:\n",
    "        exec(code, g, g)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"[C-04] exec agent code failed:\", e)\n",
    "        return False\n",
    "\n",
    "AGENT_OK = _import_agent_from_04_4()\n",
    "if not AGENT_OK:\n",
    "    raise RuntimeError(\"Agent import failed. Please ensure 04-4 notebook is present.\")\n",
    "\n",
    "# build agent instance\n",
    "agent = make_fixed_agent(run_id=RUN_ID, trace_enabled=True)\n",
    "print(\"[C-04] Agent ready:\", type(agent).__name__)\n",
    "\n",
    "# ---------- Evaluation functions (define only if missing) ----------\n",
    "if \"prepare_evaluation_samples\" not in globals():\n",
    "    def prepare_evaluation_samples(false_negatives_df: Optional[pd.DataFrame]=None, dev_mode: bool=True) -> pd.DataFrame:\n",
    "        \"\"\"Create a small evaluation sample set when original logic is unavailable.\"\"\"\n",
    "        random.seed(42)\n",
    "        brands = H3.get(\"brand_keywords\") or [\"paypal\",\"mercari\",\"ledger\",\"apple\",\"amazon\"]\n",
    "        cands = [\n",
    "            f\"{b}-secure-login.info\" for b in brands[:3]\n",
    "        ] + [\n",
    "            \"example.com\", \"support-update-center.net\", \"my-verify-account.co\", \"login-validate-user.app\"\n",
    "        ]\n",
    "        probs = [round(random.uniform(0.05, 0.9), 3) for _ in cands]\n",
    "        return pd.DataFrame({\"domain\": cands, \"ml_probability\": probs})\n",
    "\n",
    "if \"execute_evaluation\" not in globals():\n",
    "    def execute_evaluation(sample_size: Optional[int]=None, max_concurrent: Optional[int]=None):\n",
    "        df = prepare_evaluation_samples()\n",
    "        if sample_size is not None:\n",
    "            df = df.head(sample_size)\n",
    "        rows = []\n",
    "        for _, r in df.iterrows():\n",
    "            res = agent.analyze(r[\"domain\"], float(r[\"ml_probability\"]), trace={})\n",
    "            rows.append({\n",
    "                \"domain\": r[\"domain\"],\n",
    "                \"ml_probability\": float(r[\"ml_probability\"]),\n",
    "                \"is_phishing\": bool(res.get(\"is_phishing\", False)),\n",
    "                \"confidence\": float(res.get(\"confidence\", 0.0)),\n",
    "                \"reasoning\": str(res.get(\"reasoning\",\"\"))[:400],\n",
    "            })\n",
    "        res_df = pd.DataFrame(rows)\n",
    "        # write artifacts\n",
    "        out_dir = os.path.join(\"/mnt/data/artifacts\", RUN_ID, \"results\")\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        csv_p = os.path.join(out_dir, \"04-5_results.csv\")\n",
    "        res_df.to_csv(csv_p, index=False)\n",
    "        return res_df, {\"result_path\": csv_p, \"rows\": len(res_df)}\n",
    "\n",
    "if \"analyze_evaluation_results\" not in globals():\n",
    "    def analyze_evaluation_results(results_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        n = len(results_df)\n",
    "        if n == 0: return {\"n\": 0}\n",
    "        ph = int(results_df[\"is_phishing\"].sum())\n",
    "        mean_conf = float(results_df[\"confidence\"].mean())\n",
    "        return {\"n\": n, \"predicted_phishing\": ph, \"rate\": ph/n, \"mean_confidence\": mean_conf}\n",
    "\n",
    "if \"generate_evaluation_report\" not in globals():\n",
    "    def generate_evaluation_report(analysis: Dict[str, Any], results_df: pd.DataFrame) -> str:\n",
    "        lines = []\n",
    "        lines.append(f\"Evaluation samples: {analysis.get('n', 0)}\")\n",
    "        lines.append(f\"Predicted phishing: {analysis.get('predicted_phishing', 0)} (rate={analysis.get('rate',0):.2f})\")\n",
    "        lines.append(f\"Mean confidence: {analysis.get('mean_confidence', 0.0):.3f}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- run minimal evaluation ----------\n",
    "results_df, meta = execute_evaluation(sample_size=None, max_concurrent=None)\n",
    "analysis = analyze_evaluation_results(results_df)\n",
    "report = generate_evaluation_report(analysis, results_df)\n",
    "\n",
    "# save artifacts\n",
    "out_dir = os.path.join(\"/mnt/data/artifacts\", RUN_ID, \"results\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(os.path.join(out_dir, \"04-5_analysis.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(analysis, f, ensure_ascii=False, indent=2)\n",
    "with open(os.path.join(out_dir, \"04-5_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"[C-04] Evaluation completed.\")\n",
    "print(\"[C-04] Results:\", meta[\"result_path\"])\n",
    "print(\"[C-04] Analysis:\", analysis)\n",
    "print(\"[C-04] Report path:\", os.path.join(out_dir, \"04-5_report.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a4377",
   "metadata": {
    "tags": [
     "C-04b",
     "override",
     "evaluation_samples"
    ]
   },
   "outputs": [],
   "source": [
    "# === C-04b: override prepare_evaluation_samples (robust) ===\n",
    "import os, json, pickle, glob, warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def _detect_run_id_for_045():\n",
    "    rid = globals().get(\"RUN_ID\") or os.environ.get(\"RUN_ID\")\n",
    "    if rid:\n",
    "        return rid\n",
    "    art_root = Path(\"/mnt/data/artifacts\")\n",
    "    if not art_root.exists():\n",
    "        return None\n",
    "    cands = sorted([d for d in art_root.iterdir() if d.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return cands[0].name if cands else None\n",
    "\n",
    "def _load_pkl_safe(p):\n",
    "    try:\n",
    "        if p.exists() and p.stat().st_size > 0:\n",
    "            with open(p, \"rb\") as f:\n",
    "                return pickle.load(f) or {}\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"[04-5] pickle load failed: {p} -> {e}\")\n",
    "    return {}\n",
    "\n",
    "def _pick_prob_col(df):\n",
    "    for c in [\"ml_probability\",\"ml_prob\",\"prob\",\"proba\",\"pred_prob\",\"pred_proba\",\"probability\",\"phish_prob\",\"p_phish\",\"score\"]:\n",
    "        if c in getattr(df, \"columns\", []):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _to_eval_df(df):\n",
    "    if isinstance(df, pd.DataFrame) and \"domain\" in df.columns:\n",
    "        c = _pick_prob_col(df)\n",
    "        if c:\n",
    "            out = df[[\"domain\", c]].rename(columns={c: \"ml_probability\"}).copy()\n",
    "            return out.reset_index(drop=True)\n",
    "    return None\n",
    "\n",
    "def prepare_evaluation_samples(run_id, h44, h43, dev_mode=True):\n",
    "    \"\"\"Override: robustly find evaluation samples from 04-4/04-3 handoffs or CSV path.\"\"\"\n",
    "    tried = []\n",
    "\n",
    "    # 1) 04-4 exports['eval_samples']\n",
    "    if isinstance(h44, dict) and \"eval_samples\" in h44:\n",
    "        df = _to_eval_df(h44[\"eval_samples\"]) if not isinstance(h44[\"eval_samples\"], pd.DataFrame) else h44[\"eval_samples\"]\n",
    "        if df is None and isinstance(h44[\"eval_samples\"], list):\n",
    "            try:\n",
    "                df = _to_eval_df(pd.DataFrame(h44[\"eval_samples\"]))\n",
    "            except Exception:\n",
    "                df = None\n",
    "        if isinstance(df, pd.DataFrame) and len(df):\n",
    "            print(f\"[04-5] samples from 04-4 exports['eval_samples'] -> {len(df)} rows\")\n",
    "            return df\n",
    "\n",
    "    # 2) 04-3 'fn_features_df'\n",
    "    if isinstance(h43, dict) and \"fn_features_df\" in h43:\n",
    "        df = _to_eval_df(h43[\"fn_features_df\"]) if not isinstance(h43[\"fn_features_df\"], pd.DataFrame) else h43[\"fn_features_df\"]\n",
    "        if isinstance(df, pd.DataFrame) and len(df):\n",
    "            print(f\"[04-5] samples from 04-3 fn_features_df -> {len(df)} rows\")\n",
    "            return df\n",
    "        tried.append(\"h43['fn_features_df']\")\n",
    "\n",
    "    # 3) 04-4 meta.paths.evaluation_candidates_csv\n",
    "    try:\n",
    "        meta_paths = (h44 or {}).get(\"meta\", {}).get(\"paths\", {})\n",
    "        cand = meta_paths.get(\"evaluation_candidates_csv\")\n",
    "        if cand and Path(cand).exists():\n",
    "            df = pd.read_csv(cand)\n",
    "            df = _to_eval_df(df) or df\n",
    "            if isinstance(df, pd.DataFrame) and len(df):\n",
    "                print(f\"[04-5] samples loaded from {cand} -> {len(df)} rows\")\n",
    "                return df\n",
    "        else:\n",
    "            tried.append(\"meta.paths.evaluation_candidates_csv\")\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"[04-5] load candidates_csv failed: {e}\")\n",
    "\n",
    "    # 4) disk fallback: read current RUN_ID handoffs directly\n",
    "    rid = run_id or _detect_run_id_for_045()\n",
    "    if rid:\n",
    "        hdir = Path(\"/mnt/data/artifacts\") / rid / \"handoff\"\n",
    "        h44p = hdir / \"04-4_agent_implementation.pkl\"\n",
    "        h43p = hdir / \"04-3_llm_tools_setup_with_tools.pkl\"\n",
    "        dd44 = _load_pkl_safe(h44p)\n",
    "        dd43 = _load_pkl_safe(h43p)\n",
    "        if isinstance(dd44.get(\"eval_samples\"), pd.DataFrame):\n",
    "            df = _to_eval_df(dd44[\"eval_samples\"]) or dd44[\"eval_samples\"]\n",
    "            if isinstance(df, pd.DataFrame) and len(df):\n",
    "                print(f\"[04-5] samples from disk 04-4 eval_samples -> {len(df)} rows\")\n",
    "                return df\n",
    "        if \"fn_features_df\" in dd43:\n",
    "            df = _to_eval_df(dd43[\"fn_features_df\"]) or dd43[\"fn_features_df\"]\n",
    "            if isinstance(df, pd.DataFrame) and len(df):\n",
    "                print(f\"[04-5] samples from disk 04-3 fn_features_df -> {len(df)} rows\")\n",
    "                return df\n",
    "        tried.append(f\"disk({rid})\")\n",
    "\n",
    "    # 5) Dev-mode synthetic (only when allowed)\n",
    "    if dev_mode:\n",
    "        rows = [\n",
    "            {\"domain\":\"paypal-login.example\",\"ml_probability\":0.91},\n",
    "            {\"domain\":\"docs-google-secure-auth.example\",\"ml_probability\":0.83},\n",
    "            {\"domain\":\"support-update-center.example\",\"ml_probability\":0.77},\n",
    "            {\"domain\":\"appleid-verify-auth.example\",\"ml_probability\":0.66},\n",
    "            {\"domain\":\"nhk.jp\",\"ml_probability\":0.05},\n",
    "        ]\n",
    "        print(\"[04-5] dev_mode=True -> using synthetic samples (5 rows)\")\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    raise RuntimeError(\"[04-5] No evaluation samples found. Tried: \" + \"; \".join(tried[:10]))\n",
    "\n",
    "print(\"[C-04b] prepare_evaluation_samples override is ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
