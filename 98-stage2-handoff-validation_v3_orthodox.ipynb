{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c531a2d6",
   "metadata": {},
   "source": [
    "# Stage-2 Handoff Validation (Orthodox / End-to-End)\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆã¯ã€**Stage-1ï¼ˆMLï¼‰â†’ Stage-2ï¼ˆdefer gate / handoffç¸®é€€ï¼‰â†’ Agent/LLMï¼ˆå¿…è¦ãªã‚‰ï¼‰** ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã€\n",
    "**å­¦è¡“ãƒ»å®Ÿå‹™ã§ç­‹ãŒè‰¯ã„ï¼ˆã‚ªãƒ¼ã‚½ãƒ‰ãƒƒã‚¯ã‚¹ãªï¼‰** å½¢ã§æ¤œè¨¼ã—ã¾ã™ã€‚\n",
    "\n",
    "è©•ä¾¡ã¯å¤§ãã2æ®µã«åˆ†ã‘ã¾ã™ï¼š\n",
    "\n",
    "1. **Gateè©•ä¾¡ï¼ˆStage-2ã®é¸åˆ¥å“è³ªï¼‰**  \n",
    "   - Stage-1 ã®èª¤ã‚Š `err = (stage1_pred != y_true)` ã‚’ã©ã‚Œã ã‘ handoff ã«å›åã§ããŸã‹  \n",
    "   - `error_capture_recall`, `handoff_precision`  \n",
    "   - å¯èƒ½ãªã‚‰ **Stage-1 handoffé ˜åŸŸå†…**ï¼ˆ`stage1_decision==\"handoff_to_agent\"`ï¼‰ã§ã‚‚åŒæ§˜ã«å‡ºã—ã¾ã™\n",
    "\n",
    "2. **End-to-Endè©•ä¾¡ï¼ˆæœ€çµ‚åˆ¤å®šå“è³ªï¼‰**  \n",
    "   - Stage-2 handoff ã«å…¥ã£ãŸ domain ã ã‘ Agent ã‚’å®Ÿè¡Œã—ã€çµæœãŒå–ã‚ŒãŸã‚‰ **Stage-1äºˆæ¸¬ã‚’ä¸Šæ›¸ã**  \n",
    "   - å–ã‚Œãªã‹ã£ãŸå ´åˆã¯ **Stage-1ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**ï¼ˆé‹ç”¨ã§ç­‹ãŒè‰¯ã„ï¼‰  \n",
    "   - ALL / AUTOï¼ˆhandoffå¤–ï¼‰/ HANDOFFï¼ˆhandoffå†…ï¼‰ã«åˆ†ã‘ã¦æ··åŒè¡Œåˆ—ãƒ»ä¸»è¦æŒ‡æ¨™ã‚’å‡ºã—ã¾ã™\n",
    "\n",
    "---\n",
    "\n",
    "## å…¥åŠ›ï¼ˆI/Fã¯å¤‰æ›´ã—ãªã„ï¼‰\n",
    "- `artifacts/<RUN_ID>/results/stage1_decisions_latest.csv`\n",
    "- `artifacts/<RUN_ID>/handoff/handoff_candidates_latest.csv`\n",
    "\n",
    "## å‡ºåŠ›\n",
    "- GateæŒ‡æ¨™ï¼ˆALL / Stage-1 handoffé ˜åŸŸï¼‰\n",
    "- Agentå˜ä½“æŒ‡æ¨™ï¼ˆè©•ä¾¡ç”¨ã‚µãƒ³ãƒ—ãƒ«ï¼‰\n",
    "- End-to-EndæŒ‡æ¨™ï¼ˆALL / AUTO / HANDOFFï¼‰\n",
    "- `artifacts/<RUN_ID>/results/stage2_validation/` é…ä¸‹ã« CSV/JSON ã‚’ä¿å­˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5058ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ChangeLog]\n",
    "# 2026-01-01: Stage-2 handoffï¼ˆæ··åœ¨ãƒ©ãƒ™ãƒ«ï¼‰ã‚’ã€Œã‚²ãƒ¼ãƒˆè©•ä¾¡ + End-to-Endè©•ä¾¡ã€ã«åˆ†è§£ã—ãŸã‚ªãƒ¼ã‚½ãƒ‰ãƒƒã‚¯ã‚¹æ¤œè¨¼ãƒ•ãƒ­ãƒ¼ã¸æ•´ç†\n",
    "# 2026-01-01: IntCastingNaNError å¯¾ç­–ï¼ˆagentäºˆæ¸¬ãŒæ¬ æã™ã‚‹ã‚±ãƒ¼ã‚¹ã§ fillnaâ†’astype(int)ï¼‰\n",
    "# 2026-01-01: benignè¿½åŠ ã‚µãƒ³ãƒ—ãƒ«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆOFFï¼ˆå¿…è¦ãªã‚‰ hard benign ã‚’ONã«ã—ã¦ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56cc13f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SET] N_SAMPLE= 3000 N_BENIGN_SAMPLE= ALL N_BENIGN_HARD_SAMPLE= ALL RANDOM_STATE= 42\n",
      "[SET] FN_COST= 3.0 FP_COST= 1.0 HANDOFF_COST= 0.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 0: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã“ã“ã ã‘å¤‰ãˆã‚Œã°OKï¼‰\n",
    "# ============================================\n",
    "import os\n",
    "\n",
    "# â–¼ Stage-2 handoffï¼ˆ=Agentã‚’å›ã™å¯¾è±¡ï¼‰ã‹ã‚‰ä½•ä»¶è©•ä¾¡ã™ã‚‹ã‹\n",
    "#   - æ•°å€¤: ãã®ä»¶æ•°ã ã‘ã‚µãƒ³ãƒ—ãƒ«\n",
    "#   - \"ALL\" / 0 / -1: å…¨ä»¶\n",
    "DEFAULT_N_SAMPLE = \"3000\"\n",
    "\n",
    "# â–¼ è¿½åŠ benignï¼ˆã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆç”¨ï¼‰: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ 0ï¼ˆè¿½åŠ ã—ãªã„ï¼‰\n",
    "#   - hard benign ã¯ã€ŒStage-1ãŒphishå¯„ã‚Šã¨æ€ã£ã¦ã„ã‚‹æ­£å¸¸ã€ã‚’ä¸Šä½ã‹ã‚‰å–ã‚‹\n",
    "DEFAULT_N_BENIGN_SAMPLE = \"0\"\n",
    "DEFAULT_N_BENIGN_HARD_SAMPLE = \"0\"   # ä¾‹: \"100\" ã«ã™ã‚‹ã¨FPè€æ€§ã‚’ç¢ºèªã—ã‚„ã™ã„\n",
    "\n",
    "DEFAULT_RANDOM_STATE = \"42\"\n",
    "\n",
    "# â–¼ ã‚³ã‚¹ãƒˆé–¢æ•°ï¼ˆä»»æ„ï¼‰\n",
    "#   cost = FN_COST*FN + FP_COST*FP + HANDOFF_COST*handoff_count\n",
    "DEFAULT_FN_COST = \"3.0\"\n",
    "DEFAULT_FP_COST = \"1.0\"\n",
    "DEFAULT_HANDOFF_COST = \"0.0\"\n",
    "\n",
    "def _normalize_env(name: str, default: str) -> str:\n",
    "    v = os.getenv(name, default)\n",
    "    s = str(v).strip()\n",
    "    if s.upper() == \"ALL\":\n",
    "        return \"ALL\"\n",
    "    try:\n",
    "        n = int(float(s))\n",
    "        return \"ALL\" if n <= 0 else str(n)\n",
    "    except Exception:\n",
    "        # æ–‡å­—åˆ—ã®ã¾ã¾ï¼ˆå¾Œæ®µã§ã‚¨ãƒ©ãƒ¼ã«ã™ã‚‹ï¼‰\n",
    "        return s\n",
    "\n",
    "# env ã§ä¸Šæ›¸ãã§ãã‚‹å½¢ã«çµ±ä¸€\n",
    "os.environ[\"N_SAMPLE\"] = _normalize_env(\"N_SAMPLE\", DEFAULT_N_SAMPLE)\n",
    "os.environ[\"N_BENIGN_SAMPLE\"] = _normalize_env(\"N_BENIGN_SAMPLE\", DEFAULT_N_BENIGN_SAMPLE)\n",
    "os.environ[\"N_BENIGN_HARD_SAMPLE\"] = _normalize_env(\"N_BENIGN_HARD_SAMPLE\", DEFAULT_N_BENIGN_HARD_SAMPLE)\n",
    "os.environ[\"RANDOM_STATE\"] = os.getenv(\"RANDOM_STATE\", DEFAULT_RANDOM_STATE)\n",
    "\n",
    "os.environ[\"FN_COST\"] = os.getenv(\"FN_COST\", DEFAULT_FN_COST)\n",
    "os.environ[\"FP_COST\"] = os.getenv(\"FP_COST\", DEFAULT_FP_COST)\n",
    "os.environ[\"HANDOFF_COST\"] = os.getenv(\"HANDOFF_COST\", DEFAULT_HANDOFF_COST)\n",
    "\n",
    "print(\"[SET]\",\n",
    "      \"N_SAMPLE=\", os.environ[\"N_SAMPLE\"],\n",
    "      \"N_BENIGN_SAMPLE=\", os.environ[\"N_BENIGN_SAMPLE\"],\n",
    "      \"N_BENIGN_HARD_SAMPLE=\", os.environ[\"N_BENIGN_HARD_SAMPLE\"],\n",
    "      \"RANDOM_STATE=\", os.environ[\"RANDOM_STATE\"])\n",
    "print(\"[SET]\",\n",
    "      \"FN_COST=\", os.environ[\"FN_COST\"],\n",
    "      \"FP_COST=\", os.environ[\"FP_COST\"],\n",
    "      \"HANDOFF_COST=\", os.environ[\"HANDOFF_COST\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a159294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NX] RUN_ID = 2026-01-03_222059 | paths.RUN_ID = 2026-01-03_222059\n",
      "[INFO] BASE_DIR      = /data/hdd/asomura/nextstep\n",
      "[INFO] RUN_ID        = 2026-01-03_222059\n",
      "[INFO] ARTIFACTS_DIR = artifacts/2026-01-03_222059\n",
      "[INFO] RESULTS_DIR   = artifacts/2026-01-03_222059/results\n",
      "[INFO] HANDOFF_DIR   = artifacts/2026-01-03_222059/handoff\n",
      "[INFO] LOGS_DIR      = artifacts/2026-01-03_222059/logs\n",
      "\n",
      "[CHECK]\n",
      "  stage1_decisions_latest.csv: artifacts/2026-01-03_222059/results/stage1_decisions_latest.csv\n",
      "  handoff_candidates_latest.csv: artifacts/2026-01-03_222059/handoff/handoff_candidates_latest.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 1: ç’°å¢ƒè¨­å®šï¼ˆRUN_ID / paths / ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ±ºå®šï¼‰\n",
    "#  - 02/03 ã¨åŒã˜ bootstrap æ–¹å¼ã§ RUN_ID ã‚’ç¢ºå®š\n",
    "# ============================================\n",
    "import os, sys, importlib\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆæ¨å®šï¼ˆå¿…è¦ãªã‚‰ NEXTSTEP_BASE_DIR ã‚’ exportï¼‰\n",
    "BASE_DIR = Path(os.environ.get(\"NEXTSTEP_BASE_DIR\", \".\")).resolve()\n",
    "if str(BASE_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "# RUN_ID ã®æ±ºå®šï¼ˆenvâ†’run_id.txtâ†’Part3â†’latestâ€¦ï¼‰\n",
    "import run_id_registry as runreg\n",
    "rid = runreg.bootstrap()\n",
    "os.environ[\"RUN_ID\"] = rid\n",
    "\n",
    "# _compat.pathsï¼ˆenv RUN_ID ã‚’åæ˜ ï¼‰\n",
    "try:\n",
    "    import _compat.paths as paths\n",
    "except ImportError:\n",
    "    import paths as paths  # fallback\n",
    "importlib.reload(paths)\n",
    "\n",
    "print(\"[NX] RUN_ID =\", rid, \"| paths.RUN_ID =\", getattr(paths, \"RUN_ID\", None))\n",
    "assert getattr(paths, \"RUN_ID\", None) == rid, f\"RUN_ID mismatch: rid={rid} paths.RUN_ID={getattr(paths,'RUN_ID',None)}\"\n",
    "RUN_ID = rid\n",
    "\n",
    "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "ARTIFACTS_DIR = Path(paths.ARTIFACTS) if hasattr(paths, \"ARTIFACTS\") else (BASE_DIR / \"artifacts\" / RUN_ID)\n",
    "RAW_DIR       = Path(paths.compat_base_dirs[\"raw\"])\n",
    "PROCESSED_DIR = Path(paths.compat_base_dirs[\"data\"])\n",
    "MODELS_DIR    = Path(paths.compat_base_dirs[\"models\"])\n",
    "RESULTS_DIR   = Path(paths.compat_base_dirs[\"results\"])\n",
    "HANDOFF_DIR   = Path(paths.compat_base_dirs[\"handoff\"])\n",
    "LOGS_DIR      = Path(paths.compat_base_dirs[\"logs\"])\n",
    "TRACES_DIR    = Path(paths.compat_base_dirs[\"traces\"])\n",
    "\n",
    "print(f\"[INFO] BASE_DIR      = {BASE_DIR}\")\n",
    "print(f\"[INFO] RUN_ID        = {RUN_ID}\")\n",
    "print(f\"[INFO] ARTIFACTS_DIR = {ARTIFACTS_DIR}\")\n",
    "print(f\"[INFO] RESULTS_DIR   = {RESULTS_DIR}\")\n",
    "print(f\"[INFO] HANDOFF_DIR   = {HANDOFF_DIR}\")\n",
    "print(f\"[INFO] LOGS_DIR      = {LOGS_DIR}\")\n",
    "\n",
    "# å­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼ˆå¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰\n",
    "print(\"\\n[CHECK]\")\n",
    "print(\"  stage1_decisions_latest.csv:\", (RESULTS_DIR / \"stage1_decisions_latest.csv\"))\n",
    "print(\"  handoff_candidates_latest.csv:\", (HANDOFF_DIR / \"handoff_candidates_latest.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08688115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] helpers ready\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 2: æŒ‡æ¨™è¨ˆç®—ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼ˆæ··åŒè¡Œåˆ— / FÎ² / gateæŒ‡æ¨™ / costï¼‰\n",
    "# ============================================\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@dataclass\n",
    "class ConfusionResult:\n",
    "    TP: int\n",
    "    FP: int\n",
    "    TN: int\n",
    "    FN: int\n",
    "    precision: float\n",
    "    recall: float\n",
    "    f1: float\n",
    "    fbeta: float\n",
    "    fpr: float\n",
    "\n",
    "def _safe_int_series(x: pd.Series) -> pd.Series:\n",
    "    # True/False, 0/1, \"true\"/\"false\" ç­‰ã«è€æ€§ã‚’æŒãŸã›ã‚‹\n",
    "    if x.dtype == bool:\n",
    "        return x.astype(int)\n",
    "    y = pd.to_numeric(x, errors=\"coerce\")\n",
    "    # \"True\"/\"False\" å¯¾å¿œï¼ˆto_numericã§NaNã«ãªã‚‹ã®ã§ï¼‰\n",
    "    if y.isna().any():\n",
    "        x2 = x.astype(str).str.strip().str.lower()\n",
    "        y2 = x2.map({\"true\": 1, \"false\": 0})\n",
    "        y = y.fillna(y2)\n",
    "    return y.fillna(0).astype(int)\n",
    "\n",
    "def confusion_from_arrays(y_true, y_pred, *, beta: float = 2.0) -> ConfusionResult:\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "    b2 = beta * beta\n",
    "    fbeta = ((1 + b2) * precision * recall / (b2 * precision + recall)) if (b2 * precision + recall) else 0.0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "\n",
    "    return ConfusionResult(tp, fp, tn, fn, precision, recall, f1, fbeta, fpr)\n",
    "\n",
    "def compute_cls_metrics(df: pd.DataFrame, *, label_col=\"label\", pred_col=\"pred\", beta: float = 2.0) -> ConfusionResult:\n",
    "    y_true = _safe_int_series(df[label_col])\n",
    "    y_pred = _safe_int_series(df[pred_col])\n",
    "    return confusion_from_arrays(y_true, y_pred, beta=beta)\n",
    "\n",
    "def compute_gate_metrics(err: pd.Series, handoff: pd.Series) -> dict:\n",
    "    # err: 1=Stage-1ãŒèª¤ã‚Š / 0=æ­£è§£\n",
    "    # handoff: 1=handoff / 0=auto\n",
    "    e = _safe_int_series(err)\n",
    "    h = _safe_int_series(handoff)\n",
    "    tp = int(((e == 1) & (h == 1)).sum())  # captured errors\n",
    "    fn = int(((e == 1) & (h == 0)).sum())  # missed errors\n",
    "    fp = int(((e == 0) & (h == 1)).sum())  # unnecessary handoff\n",
    "    tn = int(((e == 0) & (h == 0)).sum())  # correct auto\n",
    "\n",
    "    recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) else 0.0\n",
    "    return {\n",
    "        \"TP_captured_errors\": tp,\n",
    "        \"FP_unneeded_handoff\": fp,\n",
    "        \"FN_missed_errors\": fn,\n",
    "        \"TN_correct_auto\": tn,\n",
    "        \"error_capture_recall\": recall,\n",
    "        \"handoff_precision\": precision,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "def cost_from_confusion(res: ConfusionResult, *, fn_cost: float, fp_cost: float, handoff_cost: float, handoff_count: int) -> float:\n",
    "    return fn_cost * res.FN + fp_cost * res.FP + handoff_cost * int(handoff_count)\n",
    "\n",
    "def _fmt_pct(x: float) -> str:\n",
    "    return f\"{100*x:.3f}%\"\n",
    "\n",
    "print(\"[OK] helpers ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21084211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] N_SAMPLE=3000, N_BENIGN_SAMPLE=-1, N_BENIGN_HARD_SAMPLE=-1\n",
      "[INFO] COST: FN_COST=3.0, FP_COST=1.0, HANDOFF_COST=0.0\n",
      "\n",
      "[SIZE]\n",
      "  ALL test: 128067\n",
      "  Stage-2 handoff(topK): 7710\n",
      "\n",
      "[Stage-1 baseline on ALL test] (threshold=0.5)\n",
      "ConfusionResult(TP=61811, FP=538, TN=63496, FN=2222, precision=0.9913711527049351, recall=0.9652991426295816, f1=0.978161447041509, fbeta=0.9704032579651533, fpr=0.008401786550894836)\n",
      "\n",
      "[Gate metrics: Stage-2 handoff vs Stage-1 errors] (ALL test)\n",
      "{'TP_captured_errors': 1717, 'FP_unneeded_handoff': 5993, 'FN_missed_errors': 1043, 'TN_correct_auto': 119314, 'error_capture_recall': 0.6221014492753624, 'handoff_precision': 0.22269779507133594, 'f1': 0.3279847182425979}\n",
      "\n",
      "[Gate metrics: within Stage-1 handoff region only]\n",
      "  stage1_handoff_region: 64225\n",
      "{'TP_captured_errors': 1717, 'FP_unneeded_handoff': 5993, 'FN_missed_errors': 1038, 'TN_correct_auto': 55477, 'error_capture_recall': 0.6232304900181488, 'handoff_precision': 0.22269779507133594, 'f1': 0.32814142379359773}\n",
      "\n",
      "[Stage-2 handoff label mix]\n",
      "  label_counts: {0: 5015, 1: 2695}\n",
      "\n",
      "[TARGET]\n",
      "  target_df rows: 3000\n",
      "  groups: {'stage2_handoff': 3000}\n",
      "  label mix: {0: 1957, 1: 1043}\n",
      "\n",
      "[Stage-1 baseline error rate on target_df] 0.2250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>source</th>\n",
       "      <th>ml_probability</th>\n",
       "      <th>stage1_decision</th>\n",
       "      <th>y_true</th>\n",
       "      <th>label</th>\n",
       "      <th>eval_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scelgocasa.com</td>\n",
       "      <td>jpcert</td>\n",
       "      <td>0.176781</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stage2_handoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dickrobinson.com</td>\n",
       "      <td>trusted</td>\n",
       "      <td>0.125159</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stage2_handoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4qd.co.uk</td>\n",
       "      <td>trusted</td>\n",
       "      <td>0.128810</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stage2_handoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon-xq.top</td>\n",
       "      <td>certificates</td>\n",
       "      <td>0.615849</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stage2_handoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iscarpino.com</td>\n",
       "      <td>jpcert</td>\n",
       "      <td>0.174722</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stage2_handoff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             domain        source  ml_probability   stage1_decision  y_true  \\\n",
       "0    scelgocasa.com        jpcert        0.176781  handoff_to_agent       1   \n",
       "1  dickrobinson.com       trusted        0.125159  handoff_to_agent       0   \n",
       "2         4qd.co.uk       trusted        0.128810  handoff_to_agent       0   \n",
       "3     amazon-xq.top  certificates        0.615849  handoff_to_agent       1   \n",
       "4     iscarpino.com        jpcert        0.174722  handoff_to_agent       1   \n",
       "\n",
       "   label      eval_group  \n",
       "0      1  stage2_handoff  \n",
       "1      0  stage2_handoff  \n",
       "2      0  stage2_handoff  \n",
       "3      1  stage2_handoff  \n",
       "4      1  stage2_handoff  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] stage2_budget_eval.json found: artifacts/2026-01-03_222059/results/stage2_budget_eval.json\n",
      "  summary: {'auto_errors': 1043, 'auto_error_rate': 0.008665885656837576}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 3: ãƒ‡ãƒ¼ã‚¿èª­è¾¼ï¼ˆStage-1 ALL / Stage-2 handoffï¼‰ + Gateè©•ä¾¡ + target_dfä½œæˆ\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "def _parse_n(s: str) -> int:\n",
    "    s = str(s).strip()\n",
    "    if s.upper() == \"ALL\":\n",
    "        return -1\n",
    "    return int(float(s))\n",
    "\n",
    "N_SAMPLE = _parse_n(os.environ.get(\"N_SAMPLE\", \"100\"))\n",
    "N_BENIGN_SAMPLE = _parse_n(os.environ.get(\"N_BENIGN_SAMPLE\", \"0\"))\n",
    "N_BENIGN_HARD_SAMPLE = _parse_n(os.environ.get(\"N_BENIGN_HARD_SAMPLE\", \"0\"))\n",
    "RANDOM_STATE = int(os.environ.get(\"RANDOM_STATE\", \"42\"))\n",
    "\n",
    "FN_COST = float(os.environ.get(\"FN_COST\", \"3.0\"))\n",
    "FP_COST = float(os.environ.get(\"FP_COST\", \"1.0\"))\n",
    "HANDOFF_COST = float(os.environ.get(\"HANDOFF_COST\", \"0.0\"))\n",
    "\n",
    "print(f\"[INFO] N_SAMPLE={N_SAMPLE}, N_BENIGN_SAMPLE={N_BENIGN_SAMPLE}, N_BENIGN_HARD_SAMPLE={N_BENIGN_HARD_SAMPLE}\")\n",
    "print(f\"[INFO] COST: FN_COST={FN_COST}, FP_COST={FP_COST}, HANDOFF_COST={HANDOFF_COST}\")\n",
    "\n",
    "# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "STAGE1_CSV = Path(RESULTS_DIR) / \"stage1_decisions_latest.csv\"\n",
    "HANDOFF_CSV = Path(HANDOFF_DIR) / \"handoff_candidates_latest.csv\"\n",
    "\n",
    "if not STAGE1_CSV.exists():\n",
    "    raise FileNotFoundError(f\"not found: {STAGE1_CSV}\")\n",
    "if not HANDOFF_CSV.exists():\n",
    "    raise FileNotFoundError(f\"not found: {HANDOFF_CSV}\")\n",
    "\n",
    "full_df = pd.read_csv(STAGE1_CSV)\n",
    "handoff_all = pd.read_csv(HANDOFF_CSV)\n",
    "\n",
    "# æ­£è¦åŒ–\n",
    "if \"y_true\" in full_df.columns:\n",
    "    full_df[\"label\"] = full_df[\"y_true\"].astype(int)\n",
    "elif \"label\" in full_df.columns:\n",
    "    full_df[\"label\"] = full_df[\"label\"].astype(int)\n",
    "else:\n",
    "    raise RuntimeError(\"stage1_decisions_latest.csv must have y_true or label\")\n",
    "\n",
    "full_df[\"stage1_pred\"] = (full_df[\"ml_probability\"] >= 0.5).astype(int)\n",
    "\n",
    "if \"y_true\" in handoff_all.columns:\n",
    "    handoff_all[\"label\"] = handoff_all[\"y_true\"].astype(int)\n",
    "elif \"label\" in handoff_all.columns:\n",
    "    handoff_all[\"label\"] = handoff_all[\"label\"].astype(int)\n",
    "else:\n",
    "    raise RuntimeError(\"handoff_candidates_latest.csv must have y_true or label\")\n",
    "\n",
    "# Stage-2 handoff maskï¼ˆALL testï¼‰\n",
    "handoff_domains = set(handoff_all[\"domain\"].astype(str))\n",
    "full_df[\"stage2_handoff\"] = full_df[\"domain\"].astype(str).isin(handoff_domains).astype(int)\n",
    "full_df[\"stage1_err\"] = (full_df[\"stage1_pred\"] != full_df[\"label\"]).astype(int)\n",
    "\n",
    "print(\"\\n[SIZE]\")\n",
    "print(\"  ALL test:\", len(full_df))\n",
    "print(\"  Stage-2 handoff(topK):\", int(full_df[\"stage2_handoff\"].sum()))\n",
    "\n",
    "# Stage-1 baselineï¼ˆALLï¼‰\n",
    "m_stage1_all = compute_cls_metrics(full_df.assign(pred=full_df[\"stage1_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "print(\"\\n[Stage-1 baseline on ALL test] (threshold=0.5)\")\n",
    "print(m_stage1_all)\n",
    "\n",
    "# Gateè©•ä¾¡ï¼ˆStage-2 handoff as defer gateï¼‰\n",
    "gate_all = compute_gate_metrics(full_df[\"stage1_err\"], full_df[\"stage2_handoff\"])\n",
    "print(\"\\n[Gate metrics: Stage-2 handoff vs Stage-1 errors] (ALL test)\")\n",
    "print(gate_all)\n",
    "\n",
    "# Stage-1 handoffé ˜åŸŸãŒã‚ã‚Œã°ã€ãã®ä¸­ã§ã®Gateè©•ä¾¡ã‚‚å‡ºã™ï¼ˆ02ã®Budget-Evalã¨åŒå‹ï¼‰\n",
    "if \"stage1_decision\" in full_df.columns:\n",
    "    region = (full_df[\"stage1_decision\"].astype(str) == \"handoff_to_agent\")\n",
    "    if region.any():\n",
    "        gate_region = compute_gate_metrics(full_df.loc[region, \"stage1_err\"], full_df.loc[region, \"stage2_handoff\"])\n",
    "        print(\"\\n[Gate metrics: within Stage-1 handoff region only]\")\n",
    "        print(\"  stage1_handoff_region:\", int(region.sum()))\n",
    "        print(gate_region)\n",
    "    else:\n",
    "        print(\"\\n[INFO] stage1_decision column exists but no 'handoff_to_agent' rows\")\n",
    "else:\n",
    "    print(\"\\n[INFO] stage1_decision not found; skip region-gate metrics\")\n",
    "\n",
    "# handoffå†…ã®ãƒ©ãƒ™ãƒ«æ··åœ¨ï¼ˆç ”ç©¶ä¸Šã®å‰æç¢ºèªï¼‰\n",
    "mix = handoff_all[\"label\"].value_counts().to_dict()\n",
    "print(\"\\n[Stage-2 handoff label mix]\")\n",
    "print(\"  label_counts:\", mix)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# target_dfï¼ˆAgentè©•ä¾¡å¯¾è±¡ï¼‰ã‚’ä½œã‚‹\n",
    "# ---------------------------------------------------------\n",
    "def _sample(df: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    if n <= 0 or n >= len(df):\n",
    "        return df.copy()\n",
    "    return df.sample(n=n, random_state=RANDOM_STATE).copy()\n",
    "\n",
    "parts = []\n",
    "\n",
    "# 1) Stage-2 handoffï¼ˆæœ¬ç·šï¼‰\n",
    "s2 = _sample(handoff_all, N_SAMPLE)\n",
    "s2 = s2.copy()\n",
    "s2[\"eval_group\"] = \"stage2_handoff\"\n",
    "parts.append(s2)\n",
    "\n",
    "# 2) benign randomï¼ˆä»»æ„ï¼‰\n",
    "if N_BENIGN_SAMPLE > 0:\n",
    "    pool = full_df[(full_df[\"label\"] == 0) & (~full_df[\"domain\"].astype(str).isin(handoff_domains))].copy()\n",
    "    b = _sample(pool, N_BENIGN_SAMPLE)\n",
    "    b = b.rename(columns={\"label\": \"y_true\"}) if \"y_true\" not in b.columns else b\n",
    "    b[\"y_true\"] = 0\n",
    "    b[\"eval_group\"] = \"benign_random\"\n",
    "    # å¿…è¦æœ€å°åˆ—ã«å¯„ã›ã‚‹ï¼ˆåˆ—ãŒå¤šã„ã¨mergeãŒé‡ã„ï¼‰\n",
    "    keep_cols = [c for c in [\"domain\",\"ml_probability\",\"y_true\",\"source\",\"stage1_decision\"] if c in b.columns]\n",
    "    parts.append(b[keep_cols])\n",
    "\n",
    "# 3) benign hardï¼ˆä»»æ„ï¼‰\n",
    "if N_BENIGN_HARD_SAMPLE > 0:\n",
    "    pool = full_df[(full_df[\"label\"] == 0) & (~full_df[\"domain\"].astype(str).isin(handoff_domains))].copy()\n",
    "    pool = pool.sort_values(\"ml_probability\", ascending=False)\n",
    "    b = pool.head(N_BENIGN_HARD_SAMPLE).copy()\n",
    "    b = b.rename(columns={\"label\": \"y_true\"}) if \"y_true\" not in b.columns else b\n",
    "    b[\"y_true\"] = 0\n",
    "    b[\"eval_group\"] = \"benign_hard\"\n",
    "    keep_cols = [c for c in [\"domain\",\"ml_probability\",\"y_true\",\"source\",\"stage1_decision\"] if c in b.columns]\n",
    "    parts.append(b[keep_cols])\n",
    "\n",
    "target_df = pd.concat(parts, ignore_index=True)\n",
    "target_df[\"domain\"] = target_df[\"domain\"].astype(str)\n",
    "target_df = target_df.drop_duplicates(subset=[\"domain\"], keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n[TARGET]\")\n",
    "print(\"  target_df rows:\", len(target_df))\n",
    "print(\"  groups:\", target_df[\"eval_group\"].value_counts().to_dict())\n",
    "print(\"  label mix:\", target_df[\"y_true\"].astype(int).value_counts().to_dict())\n",
    "\n",
    "# target_dfã§ã®Stage-1 baselineèª¤ã‚Šç‡ï¼ˆã–ã£ãã‚Šï¼‰\n",
    "if \"y_true\" in target_df.columns:\n",
    "    y = target_df[\"y_true\"].astype(int)\n",
    "    pred = (target_df[\"ml_probability\"] >= 0.5).astype(int)\n",
    "    err = (pred != y).mean()\n",
    "    print(f\"\\n[Stage-1 baseline error rate on target_df] {err:.4f}\")\n",
    "\n",
    "display(target_df.head(5))\n",
    "\n",
    "# Stage-2ã®budget_evalãŒã‚ã‚Œã°è¡¨ç¤ºï¼ˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰\n",
    "s2_eval_json = Path(RESULTS_DIR) / \"stage2_budget_eval.json\"\n",
    "if s2_eval_json.exists():\n",
    "    import json\n",
    "    j = json.loads(s2_eval_json.read_text(encoding=\"utf-8\"))\n",
    "    print(\"\\n[INFO] stage2_budget_eval.json found:\", s2_eval_json)\n",
    "    # ç›®ã«ã¤ãã‚­ãƒ¼ã ã‘è¡¨ç¤º\n",
    "    keys = [\"run_id\",\"handoff_budget\",\"auto_errors\",\"auto_error_rate\",\"error_capture_recall\",\"handoff_precision\"]\n",
    "    show = {k: j.get(k) for k in keys if k in j}\n",
    "    if show:\n",
    "        print(\"  summary:\", show)\n",
    "else:\n",
    "    print(\"\\n[INFO] stage2_budget_eval.json not found (skip)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75f88cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] pickle is dict but has no 'external_data'; using whole dict as external_data, keys= 10\n",
      "[INFO] brand_keywords: 100\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 4: external_data ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆå¯èƒ½ãªã‚‰ï¼‰\n",
    "#  - 98-output ã®å‰æãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°åˆ©ç”¨\n",
    "#  - ç„¡ã‘ã‚Œã°æœ€å°é™ã§ç¶šè¡Œï¼ˆagentãŒå‹•ã‹ãªã„å ´åˆã¯ã“ã“ã§æ°—ã¥ã‘ã‚‹ï¼‰\n",
    "# ============================================\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "external_data = {}\n",
    "handoff_path = Path(HANDOFF_DIR) / \"04-3_llm_tools_setup_with_tools.pkl\"\n",
    "\n",
    "if handoff_path.exists():\n",
    "    with open(handoff_path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    if isinstance(obj, dict) and \"external_data\" in obj and isinstance(obj[\"external_data\"], dict):\n",
    "        external_data = obj[\"external_data\"]\n",
    "        print(\"[OK] loaded external_data from\", handoff_path.name, \"keys=\", len(external_data))\n",
    "    elif isinstance(obj, dict):\n",
    "        # ãã®ã¾ã¾ external_data ã¨ã—ã¦æ‰±ã†ï¼ˆäº’æ›ï¼‰\n",
    "        external_data = obj\n",
    "        print(\"[WARN] pickle is dict but has no 'external_data'; using whole dict as external_data, keys=\", len(external_data))\n",
    "    else:\n",
    "        print(\"[WARN] unexpected pickle type:\", type(obj), \" -> external_data empty\")\n",
    "else:\n",
    "    print(\"[WARN] not found:\", handoff_path)\n",
    "    print(\"       -> external_data will be empty (agent may still run depending on defaults).\")\n",
    "\n",
    "# brand_keywords ãŒç„¡ã„ã¨ brand tool ãŒå¼±ããªã‚‹ã®ã§æœ€ä½é™\n",
    "external_data.setdefault(\"brand_keywords\", [])\n",
    "print(\"[INFO] brand_keywords:\", len(external_data.get(\"brand_keywords\", [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ca3dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] ENABLE_DB_TLD_REFRESH=False\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Optional Cell: TLDãƒªã‚¹ãƒˆã®å‹•çš„ç”Ÿæˆï¼ˆDBåˆ†æï¼‰\n",
    "#  - heavyãªã®ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆOFF\n",
    "# ============================================\n",
    "ENABLE_DB_TLD_REFRESH = False  # True ã«ã™ã‚‹ã¨å®Ÿè¡Œã—ã¾ã™\n",
    "\n",
    "if not ENABLE_DB_TLD_REFRESH:\n",
    "    print(\"[SKIP] ENABLE_DB_TLD_REFRESH=False\")\n",
    "else:\n",
    "    try:\n",
    "        import psycopg2\n",
    "        from psycopg2.extras import RealDictCursor\n",
    "        from collections import Counter\n",
    "        from urllib.parse import urlparse\n",
    "        import random\n",
    "\n",
    "        print(\"ğŸ”§ Generating TLD data from Database Analysis...\")\n",
    "\n",
    "        # config.json ã‹ã‚‰æ¥ç¶šå…ˆã‚’èª­ã‚€ï¼ˆç’°å¢ƒã«åˆã‚ã›ã¦ï¼‰\n",
    "        import json\n",
    "        cfg_path = Path(os.environ.get(\"CONFIG_JSON\", str(BASE_DIR / \"config.json\")))\n",
    "        if not cfg_path.exists():\n",
    "            cfg_path = Path(BASE_DIR / \"_compat\" / \"config.json\")\n",
    "        cfg = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "        db_cfg = cfg.get(\"db\", cfg.get(\"database\", {}))\n",
    "        conn = psycopg2.connect(**db_cfg)\n",
    "\n",
    "        def extract_tld(domain):\n",
    "            if not domain or '.' not in domain:\n",
    "                return None\n",
    "            return domain.split('.')[-1].lower()\n",
    "\n",
    "        with conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "            cur.execute(\"SELECT domain FROM certificates WHERE status='SUCCESS' AND domain IS NOT NULL LIMIT 200000;\")\n",
    "            phish_domains = [r[\"domain\"] for r in cur.fetchall()]\n",
    "            cur.execute(\"SELECT domain FROM trusted_certificates WHERE status='SUCCESS' AND domain IS NOT NULL LIMIT 200000;\")\n",
    "            trusted_domains = [r[\"domain\"] for r in cur.fetchall()]\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        random.seed(42)\n",
    "        phish_unique = list(set(phish_domains))\n",
    "        trusted_unique = list(set(trusted_domains))\n",
    "        m = min(len(phish_unique), len(trusted_unique))\n",
    "        phish_bal = random.sample(phish_unique, m)\n",
    "        trusted_bal = random.sample(trusted_unique, m)\n",
    "\n",
    "        phish_tlds = Counter([extract_tld(d) for d in phish_bal if extract_tld(d)])\n",
    "        trusted_tlds = Counter([extract_tld(d) for d in trusted_bal if extract_tld(d)])\n",
    "\n",
    "        # ç°¡æ˜“: phishæ¯”ãŒé«˜ã„TLDã‚’ risky ã¨ã—ã¦æ¡ç”¨\n",
    "        dangerous = []\n",
    "        for tld, pc in phish_tlds.items():\n",
    "            tc = trusted_tlds.get(tld, 0)\n",
    "            if pc >= 50:\n",
    "                ratio = pc / (tc + 1)\n",
    "                if ratio >= 5:\n",
    "                    dangerous.append(tld)\n",
    "        dangerous = sorted(set(dangerous))\n",
    "        external_data[\"dangerous_tlds\"] = dangerous\n",
    "        print(\"[OK] dangerous_tlds updated:\", len(dangerous))\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] DB TLD refresh failed:\", type(e).__name__, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947948b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CONFIG_JSON = /data/hdd/asomura/nextstep/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asomura/waseda/phish-core/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Phase6 wired\n",
      "[OK] agent initialized: <class 'phishing_agent.langgraph_module.LangGraphPhishingAgent'>\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 5: Agent åˆæœŸåŒ–ï¼ˆPhase6 wiring + LangGraphPhishingAgentï¼‰\n",
    "# ============================================\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# config.json ã®å ´æ‰€ï¼ˆç’°å¢ƒã«åˆã‚ã›ã¦ï¼‰\n",
    "cfg_path = Path(os.environ.get(\"CONFIG_JSON\", str(BASE_DIR / \"config.json\"))).resolve()\n",
    "if not cfg_path.exists():\n",
    "    alt = Path(BASE_DIR / \"_compat\" / \"config.json\")\n",
    "    if alt.exists():\n",
    "        cfg_path = alt.resolve()\n",
    "\n",
    "os.environ[\"CONFIG_JSON\"] = str(cfg_path)\n",
    "print(\"[INFO] CONFIG_JSON =\", os.environ[\"CONFIG_JSON\"])\n",
    "\n",
    "# Phase6 wiring\n",
    "from phishing_agent.phase6_wiring import wire_phase6\n",
    "wire_phase6(prefer_compat=True, fake_llm=False)\n",
    "print(\"[OK] Phase6 wired\")\n",
    "\n",
    "# Agent\n",
    "from phishing_agent.langgraph_module import LangGraphPhishingAgent\n",
    "agent = LangGraphPhishingAgent(\n",
    "    strict_mode=True,\n",
    "    use_llm_selection=True,\n",
    "    use_llm_decision=True,\n",
    "    config_path=str(cfg_path),\n",
    "    external_data=external_data,\n",
    ")\n",
    "print(\"[OK] agent initialized:\", type(agent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f703dcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROGRESS] 3000/3000  phishing=777  benign=2223  error=0  elapsed=20560.5s\n",
      "          last: ğŸŸ¢ fairlabor.org (ML=0.258 risk=low conf=0.85 t=4.45s)\n",
      "================================================================================\n",
      "[INFO] Full evaluation complete! Total time: 20560.50s\n",
      "[INFO] Results saved (append) to: artifacts/2026-01-03_222059/logs/stage2_handoff3000_full_eval__evalid_2026-01-04_012533__ts_2026-01-04_012533.csv  rows_written=3000\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 5: ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡å®Ÿè¡Œï¼ˆä»¶æ•°=len(target_df), è¶…è©³ç´°ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ç‰ˆï¼‰\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import io, sys, json, os\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "\n",
    "print(f\"[INFO] Starting FULL AGENT evaluation of {len(target_df)} domains (N_SAMPLE={N_SAMPLE})...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# LLMè¨­å®šã®ç¢ºèª\n",
    "if 'agent' in globals() and hasattr(agent, 'llm_config'):\n",
    "    llm_config = agent.llm_config\n",
    "    if getattr(llm_config, \"enabled\", False):\n",
    "        print(f\"[INFO] LLM initialized: {llm_config.model}\")\n",
    "    else:\n",
    "        print(\"[WARNING] LLM is disabled - results may be limited\")\n",
    "else:\n",
    "    print(\"[WARNING] Agent not properly initialized\")\n",
    "\n",
    "# --- Code fingerprint (reproducibility / verification) ------------------------\n",
    "# [ChangeLog] 2025-12-17: Add code fingerprint (module __file__ + sha256) to each output row.\n",
    "import hashlib\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import datetime as _dt\n",
    "\n",
    "def _sha256_of_file(path: str, chunk_size: int = 1024 * 1024) -> str:\n",
    "    \"\"\"Return sha256 hex digest for a local file. Never raises.\"\"\"\n",
    "    try:\n",
    "        h = hashlib.sha256()\n",
    "        with open(path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "                h.update(chunk)\n",
    "        return h.hexdigest()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR:{type(e).__name__}\"\n",
    "\n",
    "def _module_file_and_sha(modname: str):\n",
    "    \"\"\"Import module and return (file_path, sha256, module_obj).\"\"\"\n",
    "    try:\n",
    "        mod = importlib.import_module(modname)\n",
    "        f = getattr(mod, \"__file__\", None)\n",
    "        if f:\n",
    "            f = str(Path(f).resolve())\n",
    "            sha = _sha256_of_file(f)\n",
    "        else:\n",
    "            sha = None\n",
    "        return f, sha, mod\n",
    "    except Exception as e:\n",
    "        return None, f\"IMPORT_ERROR:{type(e).__name__}\", None\n",
    "\n",
    "def make_code_fingerprint() -> dict:\n",
    "    fp = {}\n",
    "    fp[\"eval_id\"] = _dt.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "    # Core modules used by final decision path\n",
    "    p6_file, p6_sha, _p6_mod = _module_file_and_sha(\"phishing_agent.phase6_wiring\")\n",
    "    ld_file, ld_sha, ld_mod = _module_file_and_sha(\"phishing_agent.llm_final_decision\")\n",
    "    lg_file, lg_sha, _lg_mod = _module_file_and_sha(\"phishing_agent.langgraph_module\")\n",
    "\n",
    "    fp[\"phase6_wiring_file\"] = p6_file\n",
    "    fp[\"phase6_wiring_sha256\"] = p6_sha\n",
    "    fp[\"llm_final_decision_file\"] = ld_file\n",
    "    fp[\"llm_final_decision_sha256\"] = ld_sha\n",
    "    fp[\"langgraph_module_file\"] = lg_file\n",
    "    fp[\"langgraph_module_sha256\"] = lg_sha\n",
    "\n",
    "    fp[\"phase6_policy_version_code\"] = getattr(ld_mod, \"PHASE6_POLICY_VERSION\", None) if ld_mod else None\n",
    "    fp[\"python\"] = sys.version.split()[0]\n",
    "    fp[\"dual_import_langgraph_module\"] = (\"langgraph_module\" in sys.modules) and (\"phishing_agent.langgraph_module\" in sys.modules)\n",
    "\n",
    "    return fp\n",
    "\n",
    "# Compute once per notebook run (avoid per-domain overhead)\n",
    "if \"CODE_FINGERPRINT\" not in globals():\n",
    "    CODE_FINGERPRINT = make_code_fingerprint()\n",
    "    CODE_FP_ROW = {\n",
    "        \"eval_id\": CODE_FINGERPRINT.get(\"eval_id\"),\n",
    "        \"phase6_policy_version_code\": CODE_FINGERPRINT.get(\"phase6_policy_version_code\"),\n",
    "        \"phase6_wiring_file\": CODE_FINGERPRINT.get(\"phase6_wiring_file\"),\n",
    "        \"phase6_wiring_sha256\": CODE_FINGERPRINT.get(\"phase6_wiring_sha256\"),\n",
    "        \"llm_final_decision_file\": CODE_FINGERPRINT.get(\"llm_final_decision_file\"),\n",
    "        \"llm_final_decision_sha256\": CODE_FINGERPRINT.get(\"llm_final_decision_sha256\"),\n",
    "        \"langgraph_module_file\": CODE_FINGERPRINT.get(\"langgraph_module_file\"),\n",
    "        \"langgraph_module_sha256\": CODE_FINGERPRINT.get(\"langgraph_module_sha256\"),\n",
    "        \"dual_import_langgraph_module\": CODE_FINGERPRINT.get(\"dual_import_langgraph_module\"),\n",
    "    }\n",
    "    print(\"[INFO] code_fingerprint:\", CODE_FP_ROW)\n",
    "else:\n",
    "    # Reuse existing in case notebook cells are re-run\n",
    "    CODE_FP_ROW = globals().get(\"CODE_FP_ROW\", {})\n",
    "\n",
    "results = []\n",
    "start_time = time.time()\n",
    "# --- ä¿å­˜è¨­å®š: 500ä»¶ã”ã¨ã«è¿½è¨˜ï¼ˆåˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ãªã—ï¼‰ -----------------\n",
    "SAVE_EVERY_ROWS = 500  # è¦ä»¶: 500ä»¶ã”ã¨ã«ä¿å­˜ï¼ˆè¿½è¨˜ï¼‰\n",
    "if \"LOGS_DIR\" not in globals() or not globals().get(\"LOGS_DIR\"):\n",
    "    raise RuntimeError(\"LOGS_DIR is not set. Run Cell 1 to initialize run_id/paths first.\")\n",
    "\n",
    "OUT_DIR = Path(globals()[\"LOGS_DIR\"])\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# eval_id ã¯ Notebookå®Ÿè¡Œå˜ä½ã§å›ºå®šï¼ˆCODE_FP_ROW ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†ï¼‰\n",
    "_eval_id = None\n",
    "if \"CODE_FP_ROW\" in globals() and isinstance(globals().get(\"CODE_FP_ROW\"), dict):\n",
    "    _eval_id = globals()[\"CODE_FP_ROW\"].get(\"eval_id\")\n",
    "if not _eval_id:\n",
    "    _eval_id = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "_ts = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "# è¿½è¨˜å…ˆï¼ˆrun_id/logs é…ä¸‹ã¸ï¼‰\n",
    "full_eval_path = OUT_DIR / f\"stage2_handoff{len(target_df)}_full_eval__evalid_{_eval_id}__ts_{_ts}.csv\"\n",
    "print(f\"[INFO] Results will be appended to: {full_eval_path} (flush every {SAVE_EVERY_ROWS} rows)\")\n",
    "\n",
    "_chunk_buf = []\n",
    "_written_header = False\n",
    "_written_rows = 0\n",
    "\n",
    "def _flush_chunk_if_needed(force: bool = False):\n",
    "    \"\"\"_chunk_buf ã‚’ full_eval_path ã«è¿½è¨˜ä¿å­˜ï¼ˆãƒ˜ãƒƒãƒ€ã¯æœ€åˆã ã‘ï¼‰ã€‚\"\"\"\n",
    "    global _written_header, _written_rows\n",
    "    if (not force) and (len(_chunk_buf) < SAVE_EVERY_ROWS):\n",
    "        return\n",
    "    if not _chunk_buf:\n",
    "        return\n",
    "    df_chunk = pd.DataFrame(_chunk_buf)\n",
    "    df_chunk.to_csv(full_eval_path, mode=\"a\", header=(not _written_header), index=False)\n",
    "    _written_header = True\n",
    "    _written_rows += int(len(_chunk_buf))\n",
    "    _chunk_buf.clear()\n",
    "\n",
    "# --- Notebookç”»é¢å‡ºåŠ›ã®åˆ¶å¾¡ï¼ˆdebug_log ã®æƒ…å ±é‡ã¯ç¶­æŒï¼‰ -----------------\n",
    "# [ChangeLog] 2025-12-15: å¤§é‡ä»¶æ•°ã§ã‚‚Jupyterã®å‡ºåŠ›ãŒé‡ããªã‚‰ãªã„ã‚ˆã†ã€ç”»é¢å‡ºåŠ›ã‚’é–“å¼•ã\n",
    "TOTAL_N = len(target_df)\n",
    "\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å°‘æ•°ä»¶ã¯å¾“æ¥ã©ãŠã‚Šè©³ç´°è¡¨ç¤º / å¤šæ•°ä»¶ã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆè¡¨ç¤º\n",
    "SHOW_DOMAIN_LOG_ON_SCREEN = (TOTAL_N <= 20)\n",
    "\n",
    "# ç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯: 1/true/yes/on ã§è©³ç´°è¡¨ç¤º\n",
    "_env = os.getenv(\"SHOW_DOMAIN_LOG_ON_SCREEN\")\n",
    "if _env is not None:\n",
    "    SHOW_DOMAIN_LOG_ON_SCREEN = str(_env).strip().lower() in (\"1\", \"true\", \"yes\", \"y\", \"on\")\n",
    "\n",
    "# é€²æ—è¡¨ç¤ºã®æ›´æ–°é »åº¦ï¼ˆç”»é¢ã‚’è»½ãã™ã‚‹ãŸã‚ã€ä»¶æ•°ã«å¿œã˜ã¦è‡ªå‹•èª¿æ•´ï¼‰\n",
    "if TOTAL_N <= 30:\n",
    "    PROGRESS_EVERY = 1\n",
    "elif TOTAL_N <= 300:\n",
    "    PROGRESS_EVERY = 10\n",
    "elif TOTAL_N <= 3000:\n",
    "    PROGRESS_EVERY = 25\n",
    "else:\n",
    "    PROGRESS_EVERY = 50\n",
    "\n",
    "# Jupyterãªã‚‰ clear_output ã§è¡¨ç¤ºã‚’ç½®ãæ›ãˆï¼ˆãƒ­ã‚°ã¯CSVå´ã«æ®‹ã‚‹ï¼‰\n",
    "_USE_CLEAR_OUTPUT = (not SHOW_DOMAIN_LOG_ON_SCREEN)\n",
    "try:\n",
    "    from IPython.display import clear_output  # type: ignore\n",
    "    _HAS_CLEAR_OUTPUT = True\n",
    "except Exception:\n",
    "    _HAS_CLEAR_OUTPUT = False\n",
    "\n",
    "print(f\"[INFO] Console output mode: {'VERBOSE' if SHOW_DOMAIN_LOG_ON_SCREEN else 'COMPACT'} \"\n",
    "      f\"(PROGRESS_EVERY={PROGRESS_EVERY}, clear_output={_USE_CLEAR_OUTPUT and _HAS_CLEAR_OUTPUT})\")\n",
    "\n",
    "# é€²æ—ã‚«ã‚¦ãƒ³ã‚¿ï¼ˆç”»é¢è¡¨ç¤ºç”¨ï¼‰\n",
    "_screen_counts = {\"phish\": 0, \"benign\": 0, \"error\": 0}\n",
    "\n",
    "# --- è¡¨ç¤ºç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼ ----------------------------------------------------\n",
    "def _fmt_issues(issues):\n",
    "    return \", \".join(issues) if issues else \"None\"\n",
    "\n",
    "def _safe_dict(d):\n",
    "    return d if isinstance(d, dict) else {}\n",
    "\n",
    "def _log_tool_selection(graph_state, ml_probability):\n",
    "    gs = _safe_dict(graph_state)\n",
    "    pre = _safe_dict(gs.get(\"precheck_hints\"))\n",
    "    ml_cat   = pre.get(\"ml_category\", \"-\")\n",
    "    tld_cat  = pre.get(\"tld_category\", \"-\")\n",
    "    quick_risk = pre.get(\"quick_risk\", \"-\")\n",
    "    known_info = _safe_dict(pre.get(\"known_domain_info\"))\n",
    "    known_label = known_info.get(\"label\") or known_info.get(\"kind\") or known_info.get(\"brand\")\n",
    "\n",
    "    selected_tools = gs.get(\"selected_tools\", [])\n",
    "    flags = _safe_dict(gs.get(\"tool_execution_flags\"))\n",
    "    llm_used = gs.get(\"llm_used_selection\")\n",
    "    llm_err  = gs.get(\"llm_selection_error\")\n",
    "\n",
    "    print(\"  [ToolSelection]\")\n",
    "    print(f\"    ml_probability      : {ml_probability:.3f} (category={ml_cat})\")\n",
    "    print(f\"    tld_category        : {tld_cat}  quick_risk={quick_risk}\")\n",
    "    if known_label:\n",
    "        print(f\"    known_domain_info   : {known_label}\")\n",
    "    print(f\"    selected_tools      : {selected_tools}\")\n",
    "    print(f\"    tool_execution_flags: {flags}\")\n",
    "    print(f\"    llm_used_selection  : {llm_used}\")\n",
    "    if llm_err:\n",
    "        print(f\"    llm_selection_error : {llm_err}\")\n",
    "\n",
    "    if llm_used:\n",
    "        strategy = \"llm_structured_output\"\n",
    "    else:\n",
    "        strategy = \"ml_bucket_fallback\"\n",
    "    print(f\"    selection_strategy  : {strategy}\")\n",
    "\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯ã€ã©ã®ãƒã‚±ãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã«ãªã£ãŸã‹ã‚‚è¡¨ç¤º\n",
    "    if not llm_used:\n",
    "        if ml_probability < 0.2:\n",
    "            policy = \"ML<0.2 â†’ brand+cert+domain (3 tools)\"\n",
    "        elif ml_probability < 0.5:\n",
    "            policy = \"0.2â‰¦ML<0.5 â†’ brand+cert+domain (3 tools)\"\n",
    "        else:\n",
    "            policy = \"MLâ‰§0.5 â†’ brand+cert (2 tools)\"\n",
    "        print(f\"    selection_policy    : {policy}\")\n",
    "\n",
    "def _log_brand_tool(brand_res):\n",
    "    br = _safe_dict(brand_res)\n",
    "    if not br:\n",
    "        print(\"  [BrandTool] not executed\")\n",
    "        return\n",
    "\n",
    "    details = _safe_dict(br.get(\"details\"))\n",
    "    issues  = br.get(\"detected_issues\") or []\n",
    "    # â€» brand_impersonation_check ã®æ§‹é€ ã«åˆã‚ã›ã¦ details å´ã‚’å„ªå…ˆ\n",
    "    brands  = details.get(\"detected_brands\") or br.get(\"detected_brands\") or []\n",
    "    used_llm    = details.get(\"used_llm\")\n",
    "    llm_conf    = details.get(\"llm_confidence\")\n",
    "    llm_reason  = details.get(\"llm_reasoning\")\n",
    "    #llm_reason  = (details.get(\"llm_reasoning\") or \"\")[:160]\n",
    "\n",
    "    print(\"  [BrandTool]\")\n",
    "    if br.get(\"_fallback\"):\n",
    "        print(\"    âš  fallback           : True (brand tool exception or disabled)\")\n",
    "    print(f\"    risk_score           : {br.get('risk_score')}\")\n",
    "    print(f\"    detected_issues      : {issues}\")\n",
    "    print(f\"    detected_brands      : {brands}\")\n",
    "    print(f\"    used_llm             : {used_llm}\")\n",
    "    print(f\"    llm_confidence       : {llm_conf}\")\n",
    "    print(f\"    llm_reasoning (head) : {llm_reason}\")\n",
    "    if used_llm:\n",
    "        print(\"    âœ… LLM ãŒå®Ÿè¡Œã•ã‚Œã¾ã—ãŸ (used_llm=True)\")\n",
    "    if brands:\n",
    "        print(f\"    âœ… ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œå‡ºãŒã‚ã‚Šã¾ã™: {brands}\")\n",
    "\n",
    "def _log_cert_tool(cert_res):\n",
    "    cr = _safe_dict(cert_res)\n",
    "    if not cr:\n",
    "        print(\"  [CertTool] not executed\")\n",
    "        return\n",
    "    details = _safe_dict(cr.get(\"details\"))\n",
    "    print(\"  [CertTool]\")\n",
    "    if cr.get(\"_fallback\"):\n",
    "        print(\"    âš  fallback      : True (cert tool exception or disabled)\")\n",
    "    print(f\"    risk_score      : {cr.get('risk_score')}\")\n",
    "    print(f\"    detected_issues : {cr.get('detected_issues') or []}\")\n",
    "    print(f\"    issuer          : {details.get('issuer')}\")\n",
    "    print(f\"    is_free_ca      : {details.get('is_free_ca')}\")\n",
    "    print(f\"    has_org         : {details.get('has_org')}\")\n",
    "    print(f\"    valid_days      : {details.get('valid_days')}\")\n",
    "    print(f\"    is_short_term   : {details.get('is_short_term')}\")\n",
    "    print(f\"    san_count       : {details.get('san_count')}\")\n",
    "    print(f\"    is_self_signed  : {details.get('is_self_signed')}\")\n",
    "    print(f\"    is_wildcard     : {details.get('is_wildcard')}\")\n",
    "\n",
    "def _log_domain_tool(domain_res):\n",
    "    dr = _safe_dict(domain_res)\n",
    "    if not dr:\n",
    "        print(\"  [DomainTool] not executed\")\n",
    "        return\n",
    "    details = _safe_dict(dr.get(\"details\"))\n",
    "    print(\"  [DomainTool]\")\n",
    "    if dr.get(\"_fallback\"):\n",
    "        print(\"    âš  fallback            : True (domain tool exception or disabled)\")\n",
    "    print(f\"    risk_score            : {dr.get('risk_score')}\")\n",
    "    print(f\"    detected_issues       : {dr.get('detected_issues') or []}\")\n",
    "    print(f\"    base_domain           : {details.get('base_domain')}\")\n",
    "    print(f\"    domain_length         : {details.get('domain_length')}\"\n",
    "          f\" (category={details.get('domain_length_category')})\")\n",
    "    print(f\"    tld / tld_category    : {details.get('tld')} / {details.get('tld_category')}\")\n",
    "    print(f\"    entropy               : {details.get('entropy')}\")\n",
    "    print(f\"    combo_flags           : {details.get('combo_flags')}\")\n",
    "    legit = _safe_dict(details.get('legitimate_check'))\n",
    "    if legit:\n",
    "        print(f\"    legitimate_check      : is_legit={legit.get('is_legitimate')}\"\n",
    "              f\", brand={legit.get('brand')}, conf={legit.get('confidence')}\")\n",
    "\n",
    "def _log_contextual_tool(ctx_res):\n",
    "    cx = _safe_dict(ctx_res)\n",
    "    if not cx:\n",
    "        print(\"  [Contextual] not executed\")\n",
    "        return\n",
    "    details = _safe_dict(cx.get(\"details\"))\n",
    "    print(\"  [Contextual]\")\n",
    "    if cx.get(\"_fallback\"):\n",
    "        print(\"    âš  fallback            : True (contextual tool exception or disabled)\")\n",
    "    print(f\"    risk_score            : {cx.get('risk_score')}\")\n",
    "    print(f\"    detected_issues       : {cx.get('detected_issues') or []}\")\n",
    "    print(f\"    ml_probability        : {details.get('ml_probability')}\"\n",
    "          f\" (category={details.get('ml_category')})\")\n",
    "    print(f\"    tool_average_risk     : {details.get('tool_average_risk')}\")\n",
    "    print(f\"    high_risk_hits        : {details.get('high_risk_hits')}\")\n",
    "    known = _safe_dict(details.get('known_domain'))\n",
    "    if known:\n",
    "        print(f\"    known_domain          : {known}\")\n",
    "    paradox = _safe_dict(details.get('paradox'))\n",
    "    if paradox:\n",
    "        print(f\"    paradox_info          : {paradox}\")\n",
    "\n",
    "def _log_final_decision(graph_state, result):\n",
    "    gs = _safe_dict(graph_state)\n",
    "    dbg = _safe_dict(gs.get(\"debug_llm_final\"))\n",
    "    dt_list = gs.get(\"decision_trace\") or []\n",
    "    print(\"  [FinalDecision]\")\n",
    "    print(f\"    use_llm_decision      : {dbg.get('use_llm_decision')}\")\n",
    "    print(f\"    llm_is_none           : {dbg.get('llm_is_none')}\")\n",
    "    print(f\"    path                  : {dbg.get('path')}\")\n",
    "    print(f\"    success               : {dbg.get('success')}\")\n",
    "    if dbg.get(\"error\"):\n",
    "        print(f\"    error                 : {dbg.get('error')}\")\n",
    "\n",
    "    if dt_list:\n",
    "        last = _safe_dict(dt_list[-1])\n",
    "        print(f\"    phase6_policy_version : {gs.get('phase6_policy_version', last.get('phase6_version'))}\")\n",
    "        print(f\"    ctx_risk_score        : {last.get('ctx_score')}\")\n",
    "        rules = [s.get(\"rule\") for s in last.get(\"policy_trace\", []) if isinstance(s, dict) and \"rule\" in s]\n",
    "        notes = [s.get(\"note\") for s in last.get(\"policy_trace\", []) if isinstance(s, dict) and \"note\" in s]\n",
    "        if rules:\n",
    "            print(f\"    policy_rules          : {rules}\")\n",
    "        if notes:\n",
    "            print(f\"    policy_notes          : {notes}\")\n",
    "\n",
    "    print(f\"    ai_is_phishing        : {result.get('ai_is_phishing')} \"\n",
    "          f\"(risk_level={result.get('ai_risk_level')}, conf={result.get('ai_confidence'):.2f})\")\n",
    "    print(f\"    final_reasoning(head) : {(result.get('reasoning') or '')[:160]}\")\n",
    "\n",
    "def _log_graph_messages(graph_state, prefix=\"    \"):\n",
    "    \"\"\"LangGraph ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰ã€é‡è¦ãã†ãªã‚‚ã®ã ã‘æŠœç²‹è¡¨ç¤º\"\"\"\n",
    "    gs = _safe_dict(graph_state)\n",
    "    msgs = gs.get(\"messages\") or []\n",
    "    if not msgs:\n",
    "        return\n",
    "    print(\"  [GraphMessages] (tool_selection / tool_execution / final_decision only)\")\n",
    "    for m in msgs:\n",
    "        if isinstance(m, dict):\n",
    "            content = m.get(\"content\", \"\")\n",
    "            role = m.get(\"role\", \"msg\")\n",
    "        else:\n",
    "            content = getattr(m, \"content\", \"\")\n",
    "            role = getattr(m, \"role\", \"msg\")\n",
    "        if not isinstance(content, str):\n",
    "            content = str(content)\n",
    "        if any(tag in content for tag in [\"[tool_selection]\", \"[tool_execution]\", \"[final_decision]\"]):\n",
    "            print(f\"{prefix}[{role}] {content}\")\n",
    "\n",
    "\n",
    "# --- ãƒ­ã‚°ã‚’CSVã«æ®‹ã™ãŸã‚ã®æ¨™æº–å‡ºåŠ›/æ¨™æº–ã‚¨ãƒ©ãƒ¼ã‚­ãƒ£ãƒ—ãƒãƒ£ï¼ˆprint/loggingä¸¡å¯¾å¿œï¼‰ ---\n",
    "class _TeeIO:\n",
    "    def __init__(self, primary, buffer, *, enable_primary: bool = True):\n",
    "        self.primary = primary\n",
    "        self.buffer = buffer\n",
    "        self.enable_primary = bool(enable_primary)\n",
    "    def write(self, s):\n",
    "        if self.enable_primary:\n",
    "            self.primary.write(s)\n",
    "        self.buffer.write(s)\n",
    "        return len(s)\n",
    "    def flush(self):\n",
    "        if self.enable_primary:\n",
    "            try:\n",
    "                self.primary.flush()\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            self.buffer.flush()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def _json_dumps(obj):\n",
    "    \"\"\"CSVã«å®‰å…¨ã«è¼‰ã›ã‚‹ãŸã‚ã®JSONåŒ–ï¼ˆå¤±æ•—ã—ã¦ã‚‚è½ã¨ã•ãªã„ï¼‰\"\"\"\n",
    "    try:\n",
    "        return json.dumps(obj, ensure_ascii=False, default=str)\n",
    "    except Exception:\n",
    "        return json.dumps(str(obj), ensure_ascii=False, default=str)\n",
    "\n",
    "# --- å„ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è©•ä¾¡ ---------------------------------------------------\n",
    "for idx, row in target_df.iterrows():\n",
    "    before_len = len(results)  # è¿½è¨˜ä¿å­˜ç”¨ï¼ˆã“ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§è¿½åŠ ã•ã‚ŒãŸè¡Œã ã‘æ‹¾ã†ï¼‰\n",
    "    domain = row['domain']\n",
    "    ml_prob = row['ml_probability']\n",
    "    _buf = io.StringIO()\n",
    "    _tee_out = _TeeIO(sys.stdout, _buf, enable_primary=SHOW_DOMAIN_LOG_ON_SCREEN)\n",
    "    _tee_err = _TeeIO(sys.stderr, _buf, enable_primary=SHOW_DOMAIN_LOG_ON_SCREEN)\n",
    "    with redirect_stdout(_tee_out), redirect_stderr(_tee_err):\n",
    "\n",
    "        try:\n",
    "            eval_start = time.time()\n",
    "            result = agent.evaluate(domain, ml_prob)\n",
    "            elapsed = time.time() - eval_start\n",
    "\n",
    "            # --- åŸºæœ¬æƒ…å ± ---\n",
    "            is_phishing = result.get('ai_is_phishing', False)\n",
    "            confidence  = result.get('ai_confidence', 0.0)\n",
    "            risk_level  = result.get('ai_risk_level', 'unknown')\n",
    "\n",
    "            graph_state = _safe_dict(result.get(\"graph_state\"))\n",
    "            tool_res = _safe_dict(graph_state.get(\"tool_results\")) or _safe_dict(result.get(\"tool_results\"))\n",
    "\n",
    "            # Phase4 ä»•æ§˜: brand/cert/domain/contextual_risk_assessment ã¯ data æœ¬ä½“ãŒå…¥ã£ã¦ã„ã‚‹æƒ³å®š\n",
    "            brand_res  = _safe_dict(tool_res.get('brand'))\n",
    "            cert_res   = _safe_dict(tool_res.get('cert'))\n",
    "            domain_res = _safe_dict(tool_res.get('domain'))\n",
    "            ctx_res    = _safe_dict(tool_res.get('contextual_risk_assessment') or tool_res.get('contextual'))\n",
    "\n",
    "            # Brand details\n",
    "            brand_details   = _safe_dict(brand_res.get('details'))\n",
    "            detected_brands = brand_details.get('detected_brands', [])\n",
    "\n",
    "            # brand_detected / brand_suspected ã‚’ tool å´ã®ãƒ•ãƒ©ã‚°ã‹ã‚‰å–å¾—ï¼ˆbrandsé…åˆ—ã®æœ‰ç„¡ã§æ±ºã‚ãªã„ï¼‰\n",
    "            _b_issues = brand_res.get('detected_issues', []) if isinstance(brand_res, dict) else []\n",
    "            brand_detected_flag = bool(brand_details.get('brand_detected')) or ('brand_detected' in (_b_issues or []))\n",
    "            brand_suspected_flag = bool(brand_details.get('brand_suspected')) or ('brand_suspected' in (_b_issues or [])) or ('brand_llm_candidate' in (_b_issues or []))\n",
    "\n",
    "\n",
    "            # Cert details\n",
    "            cert_details = _safe_dict(cert_res.get('details'))\n",
    "            cert_issuer  = cert_details.get('issuer', 'unknown')\n",
    "\n",
    "            # Domain details\n",
    "            domain_details = _safe_dict(domain_res.get('details'))\n",
    "            tld_cat        = domain_details.get('tld_category', '-')\n",
    "\n",
    "            # Contextual issues\n",
    "            ctx_issues = ctx_res.get('detected_issues', []) if ctx_res else []\n",
    "\n",
    "            mark = \"ğŸ”´\" if is_phishing else \"ğŸŸ¢\"\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"[{idx+1:3}/{len(target_df)}] {mark} {domain:<35} (ML: {ml_prob:.3f} / Time: {elapsed:.2f}s)\")\n",
    "            print(f\"ğŸ” Domain: {domain} (ml_probability={ml_prob:.3f})\")\n",
    "\n",
    "            # --- Tool Selection è©³ç´° ---\n",
    "            _log_tool_selection(graph_state, ml_prob)\n",
    "\n",
    "            # --- å„ãƒ„ãƒ¼ãƒ«è©³ç´° ---\n",
    "            _log_brand_tool(brand_res)\n",
    "            _log_cert_tool(cert_res)\n",
    "            _log_domain_tool(domain_res)\n",
    "            if ctx_res:\n",
    "                _log_contextual_tool(ctx_res)\n",
    "\n",
    "            # --- Final Decision è©³ç´° ---\n",
    "            _log_final_decision(graph_state, result)\n",
    "\n",
    "            # --- LangGraph ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æŠœç²‹ ---\n",
    "            _log_graph_messages(graph_state)\n",
    "\n",
    "            # --- ä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰ ---\n",
    "            dbg_final = _safe_dict(graph_state.get(\"debug_llm_final\"))\n",
    "            results.append({\n",
    "                **CODE_FP_ROW,\n",
    "                'domain': domain,\n",
    "                'ml_probability': ml_prob,\n",
    "                'ai_is_phishing': is_phishing,\n",
    "                'ai_confidence': confidence,\n",
    "                'ai_risk_level': risk_level,\n",
    "                'processing_time': elapsed,\n",
    "                'debug_log': _buf.getvalue(),\n",
    "                'graph_state_json': _json_dumps(graph_state),\n",
    "                'tool_results_json': _json_dumps(tool_res),\n",
    "                # Brand\n",
    "                'brand_detected': brand_detected_flag,\n",
    "                'brand_suspected': brand_suspected_flag,\n",
    "                'brands': detected_brands,\n",
    "                'brand_used_llm': brand_details.get('used_llm'),\n",
    "                'brand_llm_quality': brand_details.get('llm_candidate_quality'),\n",
    "                'brand_llm_evidence_token': brand_details.get('llm_evidence_token'),\n",
    "                'brand_llm_detected_brand': brand_details.get('llm_detected_brand'),\n",
    "                'brand_llm_confidence': brand_details.get('llm_confidence'),\n",
    "                'brand_risk_score': brand_res.get('risk_score'),\n",
    "                'brand_issues': brand_res.get('detected_issues', []),\n",
    "                # Cert\n",
    "                'cert_issues': cert_res.get('detected_issues', []),\n",
    "                'cert_issuer': cert_issuer,\n",
    "                'cert_score': cert_res.get('risk_score', 0.0),\n",
    "                # Domain\n",
    "                'domain_issues': domain_res.get('detected_issues', []),\n",
    "                'domain_score': domain_res.get('risk_score', 0.0),\n",
    "                'tld_category': tld_cat,\n",
    "                # Contextual\n",
    "                'ctx_issues': ctx_issues,\n",
    "                'ctx_score': ctx_res.get('risk_score', None) if ctx_res else None,\n",
    "                # Tool Selection / Final LLM debug\n",
    "                'tool_selection_llm_used': graph_state.get('llm_used_selection'),\n",
    "                'tool_selection_llm_error': graph_state.get('llm_selection_error'),\n",
    "                'final_llm_path': dbg_final.get('path'),\n",
    "                'final_llm_success': dbg_final.get('success'),\n",
    "                'phase6_policy_version': graph_state.get('phase6_policy_version'),\n",
    "                'module_version': result.get('module_version'),\n",
    "                'error': None,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - eval_start\n",
    "            print(f\"[{idx+1:3}/{len(target_df)}] âŒ ERROR: {domain} - {str(e)}\")\n",
    "            results.append({\n",
    "                **CODE_FP_ROW,\n",
    "                'domain': domain,\n",
    "                'ml_probability': ml_prob,\n",
    "                'ai_is_phishing': False,\n",
    "                'ai_confidence': 0.0,\n",
    "                'ai_risk_level': 'error',\n",
    "                'processing_time': elapsed,\n",
    "                'debug_log': _buf.getvalue(),\n",
    "                'graph_state_json': _json_dumps({}),\n",
    "                'tool_results_json': _json_dumps({}),\n",
    "                'error': str(e),\n",
    "            })\n",
    "\n",
    "    # --- è¿½è¨˜ä¿å­˜ï¼ˆ500ä»¶ã”ã¨ï¼‰: ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã®çµæœï¼ˆresults[-1]ï¼‰ã‚’ãƒãƒƒãƒ•ã‚¡ã«ç©ã¿ã€å¿…è¦ãªã‚‰è¿½è¨˜ ---\n",
    "    try:\n",
    "        if len(results) > before_len:\n",
    "            _chunk_buf.append(results[-1])\n",
    "            _flush_chunk_if_needed(force=False)\n",
    "    except Exception:\n",
    "        # è¿½è¨˜ä¿å­˜ã®å¤±æ•—ã§è©•ä¾¡è‡ªä½“ã‚’æ­¢ã‚ãªã„ï¼ˆãƒ­ã‚°ã¯çµæœå´ã«æ®‹ã™ï¼‰\n",
    "        pass\n",
    "    # --- ç”»é¢è¡¨ç¤ºï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆé€²æ—ï¼‰ ---------------------------------------\n",
    "    # NOTE: è©³ç´°ãƒ­ã‚°ã¯ _buf ã«å…¨é‡ä¿å­˜ã—ã¦ã„ã‚‹ã®ã§ã€ç”»é¢å´ã¯è»½ãã™ã‚‹\n",
    "    try:\n",
    "        _last = results[-1] if results else None\n",
    "    except Exception:\n",
    "        _last = None\n",
    "\n",
    "    if _last is not None:\n",
    "        if _last.get(\"error\"):\n",
    "            _screen_counts[\"error\"] += 1\n",
    "        else:\n",
    "            if bool(_last.get(\"ai_is_phishing\")):\n",
    "                _screen_counts[\"phish\"] += 1\n",
    "            else:\n",
    "                _screen_counts[\"benign\"] += 1\n",
    "\n",
    "        # è©³ç´°ãƒ­ã‚°ã‚’ç”»é¢ã«å‡ºã•ãªã„ãƒ¢ãƒ¼ãƒ‰ã®ã¨ãã ã‘ã€é€²æ—ã‚’é–“å¼•ã„ã¦è¡¨ç¤º\n",
    "        if (not SHOW_DOMAIN_LOG_ON_SCREEN) and (\n",
    "            ((idx + 1) % PROGRESS_EVERY == 0) or (idx == (len(target_df) - 1)) or _last.get(\"error\")\n",
    "        ):\n",
    "            if _HAS_CLEAR_OUTPUT and _USE_CLEAR_OUTPUT:\n",
    "                try:\n",
    "                    clear_output(wait=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            elapsed_total = time.time() - start_time\n",
    "            mark = \"âŒ\" if _last.get(\"error\") else (\"ğŸ”´\" if _last.get(\"ai_is_phishing\") else \"ğŸŸ¢\")\n",
    "            dom  = _last.get(\"domain\", \"-\")\n",
    "            mlp  = float(_last.get(\"ml_probability\") or 0.0)\n",
    "            conf = float(_last.get(\"ai_confidence\") or 0.0)\n",
    "            rl   = _last.get(\"ai_risk_level\", \"-\")\n",
    "            tsec = float(_last.get(\"processing_time\") or 0.0)\n",
    "\n",
    "            print(f\"[PROGRESS] {idx+1}/{len(target_df)}  phishing={_screen_counts['phish']}  benign={_screen_counts['benign']}  error={_screen_counts['error']}  elapsed={elapsed_total:.1f}s\")\n",
    "            print(f\"          last: {mark} {dom} (ML={mlp:.3f} risk={rl} conf={conf:.2f} t={tsec:.2f}s)\")\n",
    "            if _last.get(\"error\"):\n",
    "                print(f\"          last_error: {_last.get('error')}\")\n",
    "\n",
    "# --- å®Œäº†å‡¦ç† ------------------------------------------------------------\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\" * 80)\n",
    "print(f\"[INFO] Full evaluation complete! Total time: {total_time:.2f}s\")\n",
    "\n",
    "# DataFrameåŒ–ï¼ˆå¾Œç¶šã‚»ãƒ«ã®åˆ†æç”¨ã«ãƒ¡ãƒ¢ãƒªä¸Šã«ã‚‚ä¿æŒï¼‰\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# --- è¿½è¨˜ä¿å­˜: ç«¯æ•°ï¼ˆ<500ä»¶ï¼‰ã‚‚å¿…ãšä¿å­˜ã—ã¦çµ‚äº† ---\n",
    "_flush_chunk_if_needed(force=True)\n",
    "\n",
    "print(f\"[INFO] Results saved (append) to: {full_eval_path}  rows_written={_written_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0494d0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] eval_df rows: 3000\n",
      "[INFO] agent_covered: 3000 / 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>source</th>\n",
       "      <th>ml_probability</th>\n",
       "      <th>stage1_decision</th>\n",
       "      <th>y_true</th>\n",
       "      <th>label</th>\n",
       "      <th>eval_group</th>\n",
       "      <th>ai_is_phishing</th>\n",
       "      <th>ai_confidence</th>\n",
       "      <th>ai_risk_level</th>\n",
       "      <th>processing_time</th>\n",
       "      <th>error</th>\n",
       "      <th>stage1_pred</th>\n",
       "      <th>agent_pred</th>\n",
       "      <th>agent_covered</th>\n",
       "      <th>final_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scelgocasa.com</td>\n",
       "      <td>jpcert</td>\n",
       "      <td>0.176781</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stage2_handoff</td>\n",
       "      <td>False</td>\n",
       "      <td>0.75</td>\n",
       "      <td>low</td>\n",
       "      <td>4.102432</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dickrobinson.com</td>\n",
       "      <td>trusted</td>\n",
       "      <td>0.125159</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stage2_handoff</td>\n",
       "      <td>False</td>\n",
       "      <td>0.85</td>\n",
       "      <td>low</td>\n",
       "      <td>4.193526</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4qd.co.uk</td>\n",
       "      <td>trusted</td>\n",
       "      <td>0.128810</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stage2_handoff</td>\n",
       "      <td>False</td>\n",
       "      <td>0.55</td>\n",
       "      <td>medium</td>\n",
       "      <td>9.007978</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon-xq.top</td>\n",
       "      <td>certificates</td>\n",
       "      <td>0.615849</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stage2_handoff</td>\n",
       "      <td>True</td>\n",
       "      <td>0.75</td>\n",
       "      <td>high</td>\n",
       "      <td>3.198911</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iscarpino.com</td>\n",
       "      <td>jpcert</td>\n",
       "      <td>0.174722</td>\n",
       "      <td>handoff_to_agent</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>stage2_handoff</td>\n",
       "      <td>False</td>\n",
       "      <td>0.85</td>\n",
       "      <td>low</td>\n",
       "      <td>8.348804</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             domain        source  ml_probability   stage1_decision  y_true  \\\n",
       "0    scelgocasa.com        jpcert        0.176781  handoff_to_agent       1   \n",
       "1  dickrobinson.com       trusted        0.125159  handoff_to_agent       0   \n",
       "2         4qd.co.uk       trusted        0.128810  handoff_to_agent       0   \n",
       "3     amazon-xq.top  certificates        0.615849  handoff_to_agent       1   \n",
       "4     iscarpino.com        jpcert        0.174722  handoff_to_agent       1   \n",
       "\n",
       "   label      eval_group  ai_is_phishing  ai_confidence ai_risk_level  \\\n",
       "0      1  stage2_handoff           False           0.75           low   \n",
       "1      0  stage2_handoff           False           0.85           low   \n",
       "2      0  stage2_handoff           False           0.55        medium   \n",
       "3      1  stage2_handoff            True           0.75          high   \n",
       "4      1  stage2_handoff           False           0.85           low   \n",
       "\n",
       "   processing_time error  stage1_pred  agent_pred  agent_covered  final_pred  \n",
       "0         4.102432  None            0       False              1           0  \n",
       "1         4.193526  None            0       False              1           0  \n",
       "2         9.007978  None            0       False              1           0  \n",
       "3         3.198911  None            1        True              1           1  \n",
       "4         8.348804  None            0       False              1           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stage-1 baseline on eval_df]\n",
      "ConfusionResult(TP=534, FP=166, TN=1791, FN=509, precision=0.7628571428571429, recall=0.5119846596356663, f1=0.612736660929432, fbeta=0.5480295566502463, fpr=0.08482370975983648)\n",
      "\n",
      "[Agent-only on covered subset]\n",
      "  covered_rows: 3000\n",
      "ConfusionResult(TP=573, FP=204, TN=1753, FN=470, precision=0.7374517374517374, recall=0.5493767976989453, f1=0.6296703296703297, fbeta=0.578904829258436, fpr=0.10424118548799183)\n",
      "\n",
      "[Final (agent override w/ fallback) on eval_df]\n",
      "ConfusionResult(TP=573, FP=204, TN=1753, FN=470, precision=0.7374517374517374, recall=0.5493767976989453, f1=0.6296703296703297, fbeta=0.578904829258436, fpr=0.10424118548799183)\n",
      "\n",
      "[By eval_group]\n",
      "  - stage2_handoff: n=3000  ConfusionResult(TP=573, FP=204, TN=1753, FN=470, precision=0.7374517374517374, recall=0.5493767976989453, f1=0.6296703296703297, fbeta=0.578904829258436, fpr=0.10424118548799183)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 7: Agentçµæœã‚’ target_df ã«çµåˆã—ã¦è©•ä¾¡ç”¨ DataFrame ã‚’ä½œã‚‹\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if \"results_df\" not in globals():\n",
    "    raise RuntimeError(\"results_df not found. Run the agent evaluation cell first.\")\n",
    "\n",
    "# agent outputs\n",
    "agent_cols = [c for c in results_df.columns if c in [\n",
    "    \"domain\",\"ai_is_phishing\",\"ai_confidence\",\"ai_risk_level\",\"processing_time\",\"error\"\n",
    "]]\n",
    "agent_pred = results_df[agent_cols].copy()\n",
    "agent_pred[\"domain\"] = agent_pred[\"domain\"].astype(str)\n",
    "\n",
    "# merge\n",
    "eval_df = target_df.copy()\n",
    "eval_df[\"domain\"] = eval_df[\"domain\"].astype(str)\n",
    "eval_df = eval_df.merge(agent_pred, on=\"domain\", how=\"left\")\n",
    "\n",
    "# labels/preds\n",
    "eval_df[\"label\"] = eval_df[\"y_true\"].astype(int)\n",
    "eval_df[\"stage1_pred\"] = (eval_df[\"ml_probability\"] >= 0.5).astype(int)\n",
    "\n",
    "# agent_pred (numeric, may be NaN)\n",
    "eval_df[\"agent_pred\"] = pd.to_numeric(eval_df[\"ai_is_phishing\"], errors=\"coerce\")\n",
    "# \"True\"/\"False\" fallback\n",
    "if eval_df[\"agent_pred\"].isna().any():\n",
    "    x = eval_df[\"ai_is_phishing\"].astype(str).str.strip().str.lower()\n",
    "    eval_df[\"agent_pred\"] = eval_df[\"agent_pred\"].fillna(x.map({\"true\": 1, \"false\": 0}))\n",
    "eval_df[\"agent_covered\"] = eval_df[\"agent_pred\"].notna().astype(int)\n",
    "\n",
    "# final_pred (for eval subset): agent if available else stage1\n",
    "eval_df[\"final_pred\"] = eval_df[\"agent_pred\"].fillna(eval_df[\"stage1_pred\"]).astype(int)\n",
    "\n",
    "print(\"[INFO] eval_df rows:\", len(eval_df))\n",
    "print(\"[INFO] agent_covered:\", int(eval_df[\"agent_covered\"].sum()), \"/\", len(eval_df))\n",
    "\n",
    "display(eval_df.head(5))\n",
    "\n",
    "# --- æŒ‡æ¨™: Stage-1 baseline / Agent-only / Final (override) ---\n",
    "m_stage1 = compute_cls_metrics(eval_df.assign(pred=eval_df[\"stage1_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "print(\"\\n[Stage-1 baseline on eval_df]\")\n",
    "print(m_stage1)\n",
    "\n",
    "covered_df = eval_df[(eval_df[\"agent_covered\"] == 1) & (eval_df.get(\"error\").isna() if \"error\" in eval_df.columns else True)].copy()\n",
    "if len(covered_df) > 0:\n",
    "    m_agent = compute_cls_metrics(covered_df.assign(pred=covered_df[\"agent_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "    print(\"\\n[Agent-only on covered subset]\")\n",
    "    print(\"  covered_rows:\", len(covered_df))\n",
    "    print(m_agent)\n",
    "else:\n",
    "    print(\"\\n[Agent-only] no covered rows\")\n",
    "\n",
    "m_final = compute_cls_metrics(eval_df.assign(pred=eval_df[\"final_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "print(\"\\n[Final (agent override w/ fallback) on eval_df]\")\n",
    "print(m_final)\n",
    "\n",
    "# groupåˆ¥\n",
    "if \"eval_group\" in eval_df.columns:\n",
    "    print(\"\\n[By eval_group]\")\n",
    "    for g, sub in eval_df.groupby(\"eval_group\"):\n",
    "        m = compute_cls_metrics(sub.assign(pred=sub[\"final_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "        print(f\"  - {g}: n={len(sub)}  {m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "954ec3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[COVERAGE]\n",
      "  stage2_handoff_total = 7710\n",
      "  agent_covered_in_handoff = 3000  (rate=0.389)\n",
      "\n",
      "[ALL test] Stage-1 vs Final\n",
      "  Stage-1: ConfusionResult(TP=61811, FP=538, TN=63496, FN=2222, precision=0.9913711527049351, recall=0.9652991426295816, f1=0.978161447041509, fbeta=0.9704032579651533, fpr=0.008401786550894836)\n",
      "  Final  : ConfusionResult(TP=61850, FP=576, TN=63458, FN=2183, precision=0.9907730753211803, recall=0.9659082035825277, f1=0.9781826520848654, fbeta=0.9707808311202356, fpr=0.008995221288690384)\n",
      "\n",
      "[AUTO subset] (not handed off)\n",
      "  n_auto: 120357\n",
      "  Stage-1: ConfusionResult(TP=60399, FP=104, TN=58915, FN=939, precision=0.9982810769713899, recall=0.984691382177443, f1=0.9914396631675709, fbeta=0.9873796406794068, fpr=0.001762144394178146)\n",
      "  Final  : ConfusionResult(TP=60399, FP=104, TN=58915, FN=939, precision=0.9982810769713899, recall=0.984691382177443, f1=0.9914396631675709, fbeta=0.9873796406794068, fpr=0.001762144394178146)\n",
      "\n",
      "[HANDOFF subset] (Stage-2 handoff)\n",
      "  n_handoff: 7710\n",
      "  Stage-1: ConfusionResult(TP=1412, FP=434, TN=4581, FN=1283, precision=0.7648970747562297, recall=0.5239332096474953, f1=0.6218894516626294, fbeta=0.5591636306035166, fpr=0.08654037886340978)\n",
      "  Final  : ConfusionResult(TP=1451, FP=472, TN=4543, FN=1244, precision=0.7545501820072803, recall=0.538404452690167, f1=0.628410567345171, fbeta=0.5711249311186335, fpr=0.09411764705882353)\n",
      "\n",
      "[Agent-only within HANDOFF covered subset]\n",
      "  n_covered: 3000\n",
      "  Agent: ConfusionResult(TP=573, FP=204, TN=1753, FN=470, precision=0.7374517374517374, recall=0.5493767976989453, f1=0.6296703296703297, fbeta=0.578904829258436, fpr=0.10424118548799183)\n",
      "\n",
      "[COST] cost = FN_COST*FN + FP_COST*FP + HANDOFF_COST*handoff_count\n",
      "  Stage-1 only: 7204.0\n",
      "  Final (with handoff): 7125.0\n",
      "\n",
      "[Gate metrics (reprint)]\n",
      "{'TP_captured_errors': 1717, 'FP_unneeded_handoff': 5993, 'FN_missed_errors': 1043, 'TN_correct_auto': 119314, 'error_capture_recall': 0.6221014492753624, 'handoff_precision': 0.22269779507133594, 'f1': 0.3279847182425979}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 8: End-to-Endè©•ä¾¡ï¼ˆALL testï¼‰: Stage-1 â†’ Stage-2(handoff) â†’ Agentä¸Šæ›¸ãï¼ˆfallbackã‚ã‚Šï¼‰\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Stage-2 handoff domainsï¼ˆALL test ä¸Šã®ãƒã‚¹ã‚¯ï¼‰\n",
    "handoff_domains = set(handoff_all[\"domain\"].astype(str))\n",
    "full_eval = full_df.copy()\n",
    "full_eval[\"domain\"] = full_eval[\"domain\"].astype(str)\n",
    "full_eval[\"stage2_handoff\"] = full_eval[\"domain\"].isin(handoff_domains).astype(int)\n",
    "\n",
    "# agentäºˆæ¸¬ï¼ˆä»Šå›å›ã—ãŸåˆ†ã ã‘å­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰\n",
    "agent_for_merge = eval_df[[\"domain\",\"agent_pred\"]].copy() if \"eval_df\" in globals() else pd.DataFrame(columns=[\"domain\",\"agent_pred\"])\n",
    "agent_for_merge = agent_for_merge.drop_duplicates(subset=[\"domain\"], keep=\"first\")\n",
    "\n",
    "merged = full_eval.merge(agent_for_merge, on=\"domain\", how=\"left\")\n",
    "\n",
    "# final_pred: agentãŒå–ã‚ŒãŸã‚‰ä¸Šæ›¸ãã€ç„¡ã‘ã‚Œã° stage1_pred\n",
    "merged[\"final_pred\"] = pd.to_numeric(merged[\"agent_pred\"], errors=\"coerce\").fillna(merged[\"stage1_pred\"]).astype(int)\n",
    "\n",
    "# coverageï¼ˆStage-2 handoffã®ã†ã¡agent_predãŒå–ã‚ŒãŸå‰²åˆï¼‰\n",
    "handoff_mask = merged[\"stage2_handoff\"] == 1\n",
    "covered = int(merged.loc[handoff_mask, \"agent_pred\"].notna().sum())\n",
    "total_handoff = int(handoff_mask.sum())\n",
    "print(\"\\n[COVERAGE]\")\n",
    "print(f\"  stage2_handoff_total = {total_handoff}\")\n",
    "print(f\"  agent_covered_in_handoff = {covered}  (rate={covered/total_handoff if total_handoff else 0:.3f})\")\n",
    "\n",
    "# --- æŒ‡æ¨™: ALL ---\n",
    "m_stage1_all = compute_cls_metrics(merged.assign(pred=merged[\"stage1_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "m_final_all = compute_cls_metrics(merged.assign(pred=merged[\"final_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "\n",
    "print(\"\\n[ALL test] Stage-1 vs Final\")\n",
    "print(\"  Stage-1:\", m_stage1_all)\n",
    "print(\"  Final  :\", m_final_all)\n",
    "\n",
    "# --- AUTO subsetï¼ˆStage-2 handoffå¤–ï¼‰ ---\n",
    "auto_mask = merged[\"stage2_handoff\"] == 0\n",
    "m_stage1_auto = compute_cls_metrics(merged.loc[auto_mask].assign(pred=merged.loc[auto_mask, \"stage1_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "m_final_auto = compute_cls_metrics(merged.loc[auto_mask].assign(pred=merged.loc[auto_mask, \"final_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "\n",
    "print(\"\\n[AUTO subset] (not handed off)\")\n",
    "print(\"  n_auto:\", int(auto_mask.sum()))\n",
    "print(\"  Stage-1:\", m_stage1_auto)\n",
    "print(\"  Final  :\", m_final_auto)\n",
    "\n",
    "# --- HANDOFF subsetï¼ˆStage-2 handoffå†…ï¼‰ ---\n",
    "m_stage1_h = compute_cls_metrics(merged.loc[handoff_mask].assign(pred=merged.loc[handoff_mask, \"stage1_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "m_final_h = compute_cls_metrics(merged.loc[handoff_mask].assign(pred=merged.loc[handoff_mask, \"final_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "\n",
    "print(\"\\n[HANDOFF subset] (Stage-2 handoff)\")\n",
    "print(\"  n_handoff:\", int(handoff_mask.sum()))\n",
    "print(\"  Stage-1:\", m_stage1_h)\n",
    "print(\"  Final  :\", m_final_h)\n",
    "\n",
    "# agent-only on covered subset inside handoff\n",
    "covered_mask = handoff_mask & merged[\"agent_pred\"].notna()\n",
    "if covered_mask.any():\n",
    "    m_agent_h = compute_cls_metrics(merged.loc[covered_mask].assign(pred=merged.loc[covered_mask, \"agent_pred\"]), label_col=\"label\", pred_col=\"pred\", beta=2.0)\n",
    "    print(\"\\n[Agent-only within HANDOFF covered subset]\")\n",
    "    print(\"  n_covered:\", int(covered_mask.sum()))\n",
    "    print(\"  Agent:\", m_agent_h)\n",
    "else:\n",
    "    print(\"\\n[Agent-only within HANDOFF] no covered rows\")\n",
    "\n",
    "# --- costï¼ˆä»»æ„ï¼‰ ---\n",
    "cost_stage1 = cost_from_confusion(m_stage1_all, fn_cost=FN_COST, fp_cost=FP_COST, handoff_cost=HANDOFF_COST, handoff_count=0)\n",
    "cost_final = cost_from_confusion(m_final_all, fn_cost=FN_COST, fp_cost=FP_COST, handoff_cost=HANDOFF_COST, handoff_count=total_handoff)\n",
    "\n",
    "print(\"\\n[COST] cost = FN_COST*FN + FP_COST*FP + HANDOFF_COST*handoff_count\")\n",
    "print(\"  Stage-1 only:\", cost_stage1)\n",
    "print(\"  Final (with handoff):\", cost_final)\n",
    "\n",
    "# å‚è€ƒ: gate metrics ã¯ Cell3 ã§ã™ã§ã«ç®—å‡ºæ¸ˆã¿ã ãŒã€è¦‹ã‚„ã™ã„ã‚ˆã†å†æ²\n",
    "gate_all = compute_gate_metrics((merged[\"stage1_pred\"] != merged[\"label\"]).astype(int), merged[\"stage2_handoff\"])\n",
    "print(\"\\n[Gate metrics (reprint)]\")\n",
    "print(gate_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b57f8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] artifacts/2026-01-03_222059/results/stage2_validation/eval_df__n3000__ts_2026-01-04_070814.csv\n",
      "[SAVED] artifacts/2026-01-03_222059/results/stage2_validation/all_test_merged__ts_2026-01-04_070814.csv\n",
      "[SAVED] artifacts/2026-01-03_222059/results/stage2_validation/summary__ts_2026-01-04_070814.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 9: çµæœä¿å­˜ï¼ˆå…±æœ‰ç”¨ï¼‰\n",
    "# ============================================\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "OUT_DIR = Path(RESULTS_DIR) / \"stage2_validation\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "# eval_dfï¼ˆä»Šå›agentã‚’å›ã—ãŸé›†åˆï¼‰\n",
    "eval_path = OUT_DIR / f\"eval_df__n{len(eval_df)}__ts_{ts}.csv\"\n",
    "eval_df.to_csv(eval_path, index=False)\n",
    "\n",
    "# mergedï¼ˆALL test end-to-endï¼‰\n",
    "merged_path = OUT_DIR / f\"all_test_merged__ts_{ts}.csv\"\n",
    "merged.to_csv(merged_path, index=False)\n",
    "\n",
    "# summary json\n",
    "summary = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"n_all_test\": int(len(full_df)),\n",
    "    \"n_stage2_handoff\": int(full_df[\"stage2_handoff\"].sum()),\n",
    "    \"agent_covered_in_handoff\": int((merged[\"stage2_handoff\"].eq(1) & merged[\"agent_pred\"].notna()).sum()),\n",
    "    \"metrics_stage1_all\": m_stage1_all.__dict__,\n",
    "    \"metrics_final_all\": m_final_all.__dict__,\n",
    "    \"metrics_stage1_auto\": m_stage1_auto.__dict__,\n",
    "    \"metrics_final_auto\": m_final_auto.__dict__,\n",
    "    \"metrics_stage1_handoff\": m_stage1_h.__dict__,\n",
    "    \"metrics_final_handoff\": m_final_h.__dict__,\n",
    "    \"gate_all\": gate_all,\n",
    "    \"cost\": {\n",
    "        \"FN_COST\": FN_COST,\n",
    "        \"FP_COST\": FP_COST,\n",
    "        \"HANDOFF_COST\": HANDOFF_COST,\n",
    "        \"stage1_only\": cost_stage1,\n",
    "        \"final\": cost_final,\n",
    "    },\n",
    "    \"params\": {\n",
    "        \"N_SAMPLE\": os.environ.get(\"N_SAMPLE\"),\n",
    "        \"N_BENIGN_SAMPLE\": os.environ.get(\"N_BENIGN_SAMPLE\"),\n",
    "        \"N_BENIGN_HARD_SAMPLE\": os.environ.get(\"N_BENIGN_HARD_SAMPLE\"),\n",
    "        \"RANDOM_STATE\": os.environ.get(\"RANDOM_STATE\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "sum_path = OUT_DIR / f\"summary__ts_{ts}.json\"\n",
    "sum_path.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"[SAVED]\", eval_path)\n",
    "print(\"[SAVED]\", merged_path)\n",
    "print(\"[SAVED]\", sum_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d59a27-1af1-4641-8951-9a6926c1d5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
