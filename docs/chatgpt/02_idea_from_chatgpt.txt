現状の Stage2 は「Stage1 の出力（確率・エントロピー・不確実性）」だけでゲートしているため、情報量が増えていません。したがって、Stage3 流入（36,489件）を 80%以上減らすには、基本的に次のどちらか（できれば両方）が必要になります。

1. 低コストで追加情報を取って、Stage2 で確信を上げられるようにする（Stage2.5 の追加が本命）
2. 似たケースの重複を潰して、Stage3 を「同じ分析の繰り返し」に使わない（キャッシュ＋類似検索＋安全なクラスタ運用）

この方針は、打ち合わせで出ている「確信を持てる 0/1 は絶対に正しい状態に寄せ、曖昧は保留へ」という設計思想（確信度駆動の二層連携）とも整合します。 

以下、ご質問 1〜4 に対して、研究として成立しやすい形で優先順位付きに提案します。

---

## 1. 最も効果が高いアプローチはどれか

結論としては、最も効果が出やすいのは「Stage2.5 追加（低コスト追加特徴＋選択的判定）」です。次点で「クラスタリングを “代表だけ Stage3” ではなく “キャッシュと同義類似の再利用” に使う」です。

理由は明確で、80%削減（36,489 → 約7,300）には「いま Stage3 に行っている約 2.9万件を、False Negative を増やさずに自動処理へ戻す」必要があります。
そのためには Stage2 が今より賢くなる必要がありますが、Stage2 が参照しているのが Stage1 出力の派生量だけだと、境界の分離は頭打ちになりがちです（追加情報がないため）。

推奨する Stage2.5 の基本設計は次の通りです。

* Stage2.5 は三値（BENIGN / PHISHING / DEFER）を出す「リジェクト付き分類（selective classification）」にする
* DEFER のみ Stage3 に送る
* BENIGN を出す条件だけを極端に保守的にする（False Negative 最小化の要件に直結）

この「三値化」は、会話で議論されている “グレー判定を許容する” 方向性を、そのまま実装可能な形に落とせます。 

---

## 2. 他に検討すべきアプローチ

効果が出やすく、論文化もしやすい順に挙げます。

### 2.1 選択的予測を「保証付き」にする（Conformal Prediction / Risk-Controlling）

False Negative を最小化したい場合、単なる閾値調整より「保証付きの DEFER 制御」が研究の芯になりやすいです。

* 例：Conformal classification で「予測集合」が {benign} のときだけ自動 BENIGN、{phish} のときだけ自動 PHISHING、{benign, phish} は DEFER
* 設定した有意水準で、誤り率（カバレッジの保証）を制御できるため、「なぜ FN を増やさずに削減できるのか」の説明が強い

「確信度が低い領域に偽陰性が集中する」という問題設定（低確信度領域問題）は、まさにこの枠組みと相性が良いです。

### 2.2 Stage3 を教師にして Stage2.5 を育てる（蒸留・自己改善ループ）

Stage3 は高精度で説明可能な出力（リスク要因など）を返せます。これをそのまま学習信号として利用します。

* Stage3 の structured output（リスク因子ベクトル、最終判定）を教師にして、Stage2.5 に「疑似的に Stage3 を近似」させる
* Stage2.5 が “Stage3 を呼ぶ価値があるか” を推定するメタモデル（error predictor）として学習してもよい
* これは既存論文の延長ではある一方、「二層を自己改善する」設計で新規性を作りやすい

論文中でも “エージェントが抽出したパターンを ML に反映” という今後課題が言及されているので、自然に繋がります。

### 2.3 Stage3 自体の階層化（段階的ツール実行＋早期終了）

件数削減とは別軸ですが、総時間は大きく減ります。

* まず文字列・証明書・CT/WHOIS の軽量分析のみ
* そこで高確信なら終了
* 低確信のみ、Web 取得・スクリーンショット解析へ進む

すでに「確信度に応じてツールを選択し処理時間を削減」という方向性が示唆されているので、設計の一貫性があります。

### 2.4 “クラスタ代表だけ Stage3” は単独採用だと危険

クラスタの代表のみを Stage3 して全体へ伝播、は削減率が出やすい一方で、クラスタ内の少数のフィッシングを落とすリスクが常に残ります。
採用するなら「安全側に倒す伝播規則」が必須です（後述）。

---

## 3. クラスタリング採用時に適切な手法・特徴量

クラスタリングの主目的を「代表だけ Stage3」ではなく、次の 2 つに置き換えることを推奨します。

* 目的A：同一または準同一のドメイン分析の繰り返しを避ける（キャッシュ）
* 目的B：類似ケースを近傍検索で引いて、Stage2.5 の確信を上げる（類似根拠の活用）

### 3.1 特徴量設計（推奨：二視点、Domain と Certificate を分離して後で統合）

(1) Domain lexical（文字列ベース、低コストで強い）

* eTLD+1、TLD
* 文字種統計：長さ、ドット数、ハイフン数、数字比率、シャノンエントロピー、連続子音長など（すでに Stage1 に近いものがあるなら流用）
* トークン：ハイフン・数字境界で split したトークン列
* 文字 n-gram（例：3〜5gram）の TF-IDF（typosquatting に強い）
* ブランド語彙一致（動的ブランド辞書があるなら特に強い）

(2) Certificate（証明書ベース、同一運用体の手掛かりになり得る）

* Issuer（CA 名）、Subject の O/OU/CN の有無と長さ
* 有効期間、SAN 数、wildcard、自己署名、鍵種別、署名アルゴリズム
* 可能なら SPKI hash か証明書 fingerprint（同一鍵・同一証明書の束ねに使える）

注意点：Let’s Encrypt 等が多い環境では、証明書特徴は単独では識別力が弱いことがあり得ます。だからこそ Domain lexical との二視点が重要です。

### 3.2 クラスタリング手法（実装容易性とスケールを両立）

推奨は以下のどれかです（データ 3.6 万程度なら十分現実的）。

* HDBSCAN（密度ベース、ノイズ点を自然に分離できる）

  * メリット：クラスタに入らない “孤立点” を DEFER に寄せやすい
* 近傍グラフ＋連結成分（閾値付き類似度で edges を張って CC を取る）

  * メリット：実装が軽く、説明がしやすい（類似度閾値が論文に書ける）
* 階層的クラスタ（Agglomerative）＋距離閾値

  * メリット：小さな高純度クラスタを作りやすい

文字 n-gram TF-IDF のコサイン類似度で近傍探索し、証明書同一性（fingerprint など）を “強いエッジ” として追加する構成が、効果と説明性のバランスが良いです。

### 3.3 “安全に倒す” 伝播ルール（ここが肝）

クラスタを使って Stage3 を減らす場合、以下の伝播原則が False Negative 最小化に効きます。

* PHISHING は伝播してよい（同一クラスタ内で PHISHING が出たら、残りは少なくとも DEFER か PHISHING 側へ寄せる）
* BENIGN は基本的に伝播しない、または条件付き

  * 例：クラスタ内の複数サンプルで独立に BENIGN 根拠が揃った場合のみ
  * 例：過去に同クラスタが継続的に BENIGN である履歴がある場合のみ（時間軸の導入）

「代表だけ Stage3→全体 BENIGN」だけは最も危険です。削減率は出ますが、FN 制約に反します。

---

## 4. False Negative を最小化しながら処理件数を減らす工夫

ここは設計指針を「判定精度」ではなく「運用リスク制御」に置くのがポイントです。

### 4.1 BENIGN 自動判定のゲートを “多重化” する

False Negative を嫌うなら、BENIGN は単一スコア閾値で出さず、独立な証拠を複数要求します。

例（低コスト特徴で可能な範囲）：

* Stage1 が benign 側で高確信、かつ
* WHOIS でドメイン年齢が十分、かつ
* DNS が安定（A/NS が頻繁に変わらない）、かつ
* 過去に同 eTLD+1 が benign と確定している（キャッシュ）

このように “AND 条件” にするほど FN は減りやすく、なおかつ benign の大量自動処理が狙えます（多くの benign は強い benign 証拠を持つため）。

### 4.2 DEFER の基準は「曖昧」ではなく「誤りやすさ推定」にする

単純に p が 0.4〜0.6 だから曖昧、ではなく、

* 「このサンプルは Stage1/2 が外しやすい」を当てるメタモデル
  にします。

学習方法は簡単で、訓練データで

* Stage2 までの自動判定が当たりだったか（特に “benign と言ってしまって実は phishing”）
  を目的変数にして、誤り確率が高いものだけ DEFER にします。

これをやると、Stage3 は “本当に外しやすいケース” に集中し、同じ Stage3 件数でも FN 改善が最大化されます。

### 4.3 確率のキャリブレーションを必須工程にする（XGBoost を使うなら特に）

会話でも出ている「自信を持って間違える」を減らすには、確率出力の信頼性を上げるのが近道です。 

* XGBoost を捨てる必要は必ずしもない
* ただし、確率の校正（Platt scaling / Isotonic など）を入れると、閾値で三値分岐する設計が成立しやすい
* その上で「高確信で出した 0/1 は本当に正しい」領域を最大化する

これは Stage3 件数削減にも効きます。なぜなら「確信できる領域」が増えるほど DEFER が減るからです。

### 4.4 研究としての新規性を作るなら「保証付きの削減」を主張する

単に “減りました” ではなく、例えば次を論文の主張にできます。

* 目標：False Negative を所定の上限以下に抑えつつ、Stage3 流入率（coverage）を最小化する
* 手段：Conformal による集合予測、またはリスク制御付き selective classification
* 評価：risk-coverage 曲線、Stage3 流入率と FN のトレードオフ、データドリフト時の頑健性

これにより、システム研究として説得力が強くなります。

---

## あなたの「検討中アプローチ」への率直な評価（優先度順）

* Stage2.5 追加：最優先。削減率と安全性を両立しやすい
* 階層的処理：Stage3 時間短縮にも効くので並行で価値が高い
* ルールベース：BENIGN を増やす用途なら有効（ただし過信しない）
* クラスタリング：キャッシュ・類似検索としては強い。代表のみ Stage3 は慎重に
* アンサンブル：確信度推定の安定化に効く。単体で 80%削減は難しいが Stage2.5 と相性が良い
* サンプリング：運用 FN を最小化する目的とは相性が悪い（研究目的が “推定” ならありだが、検知では避けたい）

---

## 次のアクション案（研究と実装の両立を意識）

1. 現状 Stage2 HANDOFF（36,489件）の中で、実ラベル phishing の割合をスコア帯別に可視化

   * Stage1 p の bin、Stage2 LR の bin で “phish 密度” を出す
   * ここで「実は benign が多い帯」が見えると、削減余地が定量化できる

2. Stage2.5 の追加特徴を 5〜10個だけ入れてベースラインを作る（WHOIS 年齢、登録直近、CT 発行回数など）

   * まずは軽量でよい
   * 目的は 80%削減の可能性を早期に見極めること

3. 三値判定（BENIGN / PHISHING / DEFER）を導入し、FN 制約下で DEFER を最小化する最適閾値を探索

   * “FN 最小化最優先” を目的関数に入れる（制約付き最適化）

4. クラスタリングはキャッシュ目的で先に入れる

   * 重複が 10%でも消えれば、即効性がある
   * 代表伝播は後回し（安全設計が固まってから）



