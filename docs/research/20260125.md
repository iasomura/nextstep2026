# 研究日誌 2026-01-25

## 本日の作業

### パイプライン全体実行 (3 GPU並列)
- 1. run_full_pipeline.sh による全ステップ実行
- 2. Worker 0 vLLM停止バグ修正 (RecoveryManager)
- 3. parallel_config.yaml vLLM設定修正
- 4. GPU速度比に基づくドメイン分割 (speed_weight)

### Stage1 過学習検証
- 5. Stage1 XGBoost の FP=2, FN=8 は過学習か？
- 6. XGBoost単体の実用性検討

---

## 1. run_full_pipeline.sh による全ステップ実行

3 GPU (Worker 0,1,2) で全パイプラインを非対話実行:

```bash
./scripts/run_full_pipeline.sh -y --add-gpu 1,2
```

- Step 1-8 (データ準備 → 04-3 LLMツール): 正常完了
- Step 9 (evaluate_e2e_parallel.py): 初回は Worker 1,2 の vLLM 未起動で失敗、再実行で対処

---

## 2. Worker 0 vLLM停止バグ修正

### 問題
- Worker 0 完了後、`stop_on_complete` で vLLM を停止したが、RecoveryManager がヘルスチェック失敗を検知し、vLLM を再起動してしまう

### 修正
`scripts/parallel/orchestrator.py` の `_wait_for_workers` に `health_monitor.remove_port()` を追加:

```python
if wc and wc.stop_on_complete and self.vllm_cluster:
    # ヘルスモニターから除外（Recoveryによる誤再起動を防止）
    if self.health_monitor:
        self.health_monitor.remove_port(wc.port)
    msg = self.vllm_cluster.stop_by_port(wc.port)
```

---

## 3. parallel_config.yaml vLLM設定修正

`vllm.sh` と設定値を統一:

| 項目 | 修正前 | 修正後 |
|------|--------|--------|
| model | Qwen/Qwen3-4B | JunHowie/Qwen3-4B-Thinking-2507-GPTQ-Int8 |
| max_model_len | 8192 | 4096 |
| gpu_memory_utilization | 0.85 | 0.25 |

---

## 4. GPU速度比に基づくドメイン分割

### 実測速度

| Worker | GPU | 実測速度 | speed_weight |
|--------|-----|----------|-------------|
| 0 | RTX 5000 Ada | 9.0 dom/min | 1.59 |
| 1 | RTX 3080 | 10.0 dom/min | 1.77 |
| 2 | RTX 4000 Ada | 5.6 dom/min | 1.00 |

### 実装
- `config.py`: WorkerConfig に `speed_weight` フィールド追加
- `orchestrator.py`: `_split_domains()` を均等分割から重み比例分割に変更
- 全 Worker がほぼ同時に完了するよう最適化

---

## 5. Stage1 XGBoost の FP=2, FN=8 は過学習か？

### 結論: 過学習ではない

### 根拠

**データ分割構造** (`02_main.py` line 1713-1718):
- 全データ: 638,765 サンプル
- Train: 511,012 (80%) → XGBoost 学習用
- Test: 127,754 (20%) → Stage1 判定対象（学習に使用していない）

**保守的な閾値設定**:
```python
'xgb_risk_max_auto_benign': 0.001   # AUTO_BENIGN の FP 許容率 0.1%
'xgb_risk_max_auto_phish': 0.0002   # AUTO_PHISHING の FN 許容率 0.02%
```

**Stage1判定の分布** (test set 127,754件):

| 判定 | 件数 | 割合 | エラー |
|------|------|------|--------|
| AUTO_BENIGN | 6,166 | 4.8% | FN=8 |
| AUTO_PHISHING | 60,614 | 47.4% | FP=2 |
| handoff_to_agent | 60,974 | 47.7% | (Stage2/3へ) |

FP=2, FN=8 は「極めて高い確信度のサンプルだけを AUTO 判定した」結果であり、全体の52%しかカバーしていない。残り48%は判定を保留している。

---

## 6. XGBoost単体の実用性検討

### 疑問
「FP=2, FN=8 なら XGBoost だけで十分では？」

### 回答: 単体では実用不可

**理由1: 48%が未判定**
- handoff 60,974件 (phish=3,257, benign=57,717) に対して判定不能
- ML>0.5 で二値分類すると F1=66.55% しか出ない

**理由2: AUTO判定は「簡単な問題」だけ**
- AUTO_PHISHING: ランダム文字列 + LE証明書 + 危険TLD → ルールでも検出可能
- AUTO_BENIGN: EV/OV証明書 + 有名TLD → 明らかに正規

**理由3: 特徴量の限界**
- ドメイン構造 + 証明書情報のみ（42次元）
- ページ内容・ソーシャルエンジニアリング手法を分析不可
- 正規サブドメイン vs フィッシングの区別不可能（例: `appleid.apple.com` vs `apple-id-verify.com`）

**理由4: 証明書特徴量はゲーム可能**
- 攻撃者がLE以外のCA/長期証明書を使えば回避可能

### 結論
XGBoostの役割は**トリアージ（高速振り分け）**。簡単な52%を効率的に処理し、AI Agentのリソースを「本当に判断が難しい48%」に集中させる分業構造が本研究の核心。

---

## 7. 関連研究調査

査読付き論文を中心に、本研究に類似する過去研究を調査した。詳細は `docs/research/related_work.md` を参照。

### 最も近い先行研究

- **KnowPhish** (USENIX Security 2024): 20Kブランドの知識ベースでフィッシング検出を強化。Trancoドメインリストを使用。ただしロゴ画像ベースでクラウドAPI前提。
- **PhishDebate** (arXiv 2025): 4専門エージェント + Moderator + Judge のマルチエージェント構造。ただしGPT-4レベルの大型LLMを前提。
- **Small LLMs for Phishing** (arXiv 2025): Qwen-2.5-1.5B等の小型LLMでローカル推論。ファインチューニングで精度122%向上。

### 本研究の新規性

1. **3段カスケード (XGBoost → Gate → AI Agent)**: 既存研究は LLM を全件に適用するが、本研究は ML で48%に絞ってから LLM を投入
2. **4B量子化モデルでツール呼び出し型エージェント**: Debate型と異なり、構造化出力で分析ツールを順次実行
3. **証明書の二重活用**: ML特徴量 (Stage1) と LLM解釈 (Stage3) で同じ証明書データを異なる粒度で利用
4. **ドメイン名+証明書のみ、消費者GPU完結**: ページ内容取得不要、8-24GB GPU で推論可能

---

## 8. Stage3 中間性能分析 (32.8%時点)

### 処理進捗 (07:42時点)

| Worker | GPU | 完了 | 進捗 | 速度 | ETA |
|--------|-----|------|------|------|-----|
| 0 | RTX 5000 Ada | 1,831/5,715 | 32.0% | 8.5 dom/min | 15:19 |
| 1 | RTX 3080 | 2,070/6,361 | 32.5% | 9.6 dom/min | 15:09 |
| 2 | RTX 4000 Ada | 1,245/3,594 | 34.6% | 5.8 dom/min | 14:29 |

### Stage3 AI Agent vs ML閾値ベースライン (同一ドメイン集合 n=5,140)

| 指標 | ML閾値 (>0.5) | AI Agent | 差分 |
|------|--------------|---------|------|
| Precision | 0.847 | **0.889** | **+0.042** |
| FP (誤検知) | 92 | **61** | **-33.7%** |
| Recall | 0.566 | 0.541 | -0.025 |
| F1 | 0.678 | 0.673 | -0.006 |

### 考察

- F1 では AI Agent と ML 閾値がほぼ同等（中間時点）
- しかし **FP を33%削減** しており、Precision が有意に改善
- セキュリティシステムでは FP（正規サイト誤ブロック）が最も高コスト → Precision改善は実用上重要
- 前回全件評価では F1=72.5% (ML baseline 66.5%) であり、全件完了後に変化する可能性あり

### 論文での主張軸（暫定）

F1 改善だけに依存しない、多角的な主張構成:

1. **アーキテクチャの効率性**: XGBoost で52%を高速処理 → LLM コスト半減
2. **FP削減**: AI Agent は ML 閾値より FP を33%削減（実運用で最重要）
3. **解釈可能性**: 判定根拠の提示（ブランド偽装、証明書分析等）
4. **小型LLMの実用性**: 4B量子化モデルで消費者GPU (8-24GB) に収まる設計
5. **新規性**: 3段カスケード + ツール呼び出し型エージェント + 証明書二重活用

---

## 次回の作業予定

- 全件評価完了待ち (15:19完了見込み) → Before 性能確定
- Stage3 AI Agent 改善実装 (Tranco拡張、多言語検出、ランダム文字列検出強化)
- 改善後の再評価 → Before/After 比較
- F1 72.5% → 78-81% の改善目標に向けた検証
