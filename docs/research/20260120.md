# 研究日誌 2026-01-20

## 本日の作業

### データパイプライン
- 1. 3000件評価の完了と結果分析
- 2. FN分析設計
- 3. データパイプラインの設計問題の発見
- 4. データ仕様の全体設計（Parquet vs PostgreSQL）
- 5. データ仕様の全体設計
- 6. 実装方針の決定: CSV拡張
- 7. 実装完了

### 分析
- 8. FN分析結果（Stage1/2）
- 9. AI Agent 3000件評価分析
- 10. VirusTotalによるtrustedデータ検証 ← **重要発見**
- 11. Stage1 FP/FN 全件VirusTotal調査 ← **実行中**

---

## 1. 3000件評価結果

### 評価概要

| 項目 | 値 |
|------|-----|
| 全テストサンプル | 128,067 |
| Stage2 Handoff候補 | 19,479 |
| AI Agent評価済み | 3,000 |
| 実行時間 | 約6.4時間 (23,006秒) |
| エラー | 0件 |

### 性能指標

#### 全体 (All Test)

| 指標 | Stage1 | Final | 差分 |
|------|--------|-------|------|
| Precision | 0.9946 | 0.9939 | -0.0007 |
| Recall | 0.9714 | 0.9720 | +0.0006 |
| F1 | 0.9828 | 0.9828 | ±0.0000 |

#### Handoff Subset (AI Agentが判定した3000件)

| 指標 | Stage1 | Final | 差分 |
|------|--------|-------|------|
| Precision | 0.9188 | 0.9099 | -0.0089 |
| Recall | 0.7310 | 0.7388 | **+0.0078** |
| F1 | 0.8142 | 0.8155 | **+0.0012** |

### コスト分析 (FN=3.0, FP=1.0)

| 項目 | 値 |
|------|-----|
| Stage1 only cost | 5,838 |
| Final cost | 5,762 |
| Cost reduction | **76 (1.3%)** |

### エラー分析

| 指標 | Stage1 | AI Agent | 差分 |
|------|--------|----------|------|
| TP | 569 | 609 | **+40** |
| FP | 47 | 91 | +44 |
| FN | 240 | 200 | **-40** |

**考察**: AI Agentは40件のTPを追加検出し、40件のFNを削減したが、44件の追加FPが発生。コスト計算上は76の削減効果。

---

## 2. FN分析設計

### 分析目的

200件のFN（フィッシングを見逃し）の原因を特定し、改善策を導出する。

### FNの特徴（速報）

| 特徴 | 傾向 |
|------|------|
| TLD | **93.5%が`.com`** (non_danger TLD) |
| ML確率 | 0.4〜0.98（高めの値も含む） |
| 証明書 | 89-365日 |

### 分析軸

1. **ドメイン特徴分析**
   - TLD分布
   - ドメイン長、サブドメイン数
   - 数字・ハイフンの割合
   - ブランド文字列の有無

2. **証明書特徴分析**
   - 有効期間分布
   - CA分布（issuer_org）
   - SAN数分布
   - 組織名有無（DV vs OV/EV）
   - 発行からの経過日数

3. **比較分析**
   - FN vs TP: 検出成功/失敗の違い
   - FN vs Benign: なぜBenignに見えるか

---

## 3. データパイプラインの設計問題

### 発見した問題

**特徴量がパイプラインを通過するごとに脱落している**

```
cert_full_info_map (11項目)
       │
       │ ❌ 脱落: issuer_org, is_free_ca, has_organization等
       ▼
df_stage1 (2項目: cert_validity_days, cert_san_count)
       │
       ▼
eval_df (2項目のみ)
```

### 脱落箇所の特定

#### 脱落ポイント1: `02_main.py:1756-1765`

```python
df_stage1 = pd.DataFrame({
    'domain': domain_test,
    ...
    # ← ここで2列のみハードコード
    'cert_validity_days': X_test[:, 15],
    'cert_san_count': X_test[:, 17],
})
```

**問題**: `cert_full_info_map`は作成されているが、`df_stage1`には2カラムのみ選択。

#### 脱落ポイント2: `02_main.py:1891`

```python
df_handoff = df_stage1.iloc[selected_idx].copy()
```

**問題**: `df_handoff`は`df_stage1`のコピーなので2列しか継承しない。`cert_full_info_map`はpklに保存されるがCSVには反映されない。

### 利用可能なデータの比較

| データソース | 証明書カラム数 | 内容 |
|--------------|----------------|------|
| cert_full_info_map | **11** | issuer_org, cert_age_days, is_free_ca, has_organization, is_wildcard, is_self_signed, san_count, not_before, not_after, has_certificate, source |
| fn_features_df (pkl) | 9 | 上記からissuer_org, not_before, not_after除く |
| stage1_decisions.csv | 2 | cert_validity_days, cert_san_count |
| eval_df (最終出力) | 2 | cert_validity_days, cert_san_count |

### 暫定対応: 分析時マージ

FN分析のため、`cert_full_info_map`からデータをマージして拡張FNデータを作成:

```python
fn_enriched = fn_df.merge(cert_df, on='domain', how='left')
```

- マージ成功率: 200/200 (100%)
- カラム数: 15 → 26
- 保存先: `fn_cases_enriched.csv`

---

## 4. 論文品質のアーキテクチャ検討

### 現状の問題点

| 要件 | 現状 | 問題 |
|------|------|------|
| 再現性 | △ | カラム順序依存、暗黙的な仕様 |
| 特徴量追跡 | × | 散在、一貫性なし |
| 実験管理 | × | RUN_IDのみ |
| 拡張性 | × | 特徴量追加時に修正箇所多数 |

### 提案アーキテクチャ

```
artifacts/{RUN_ID}/
│
├── config/
│   ├── experiment_config.yaml      # 実験設定
│   └── feature_schema.yaml         # 特徴量定義
│
├── features/                        # 特徴量ストア
│   ├── domain_features.parquet
│   ├── cert_features.parquet       # 全項目保持
│   ├── ml_features.parquet
│   └── _manifest.json
│
├── predictions/                     # 各Stage の予測結果
│   ├── stage1_predictions.parquet
│   ├── stage2_decisions.parquet
│   └── stage3_predictions.parquet
│
├── evaluation/                      # 評価結果
│   ├── metrics.json
│   └── error_analysis/
│
└── logs/
```

### 設計原則

1. **特徴量とPredictionの分離**
   - 特徴量: 入力データから抽出した属性
   - Prediction: 各Stageの判定結果
   - 分析時に必要に応じてJOIN

2. **特徴量スキーマの明示化**
   ```yaml
   cert_features:
     issuer_org:
       type: string
       description: "Certificate issuer organization"
       source: "X.509 Issuer O field"
   ```

3. **統一アクセスインターフェース**
   ```python
   fs = FeatureStore(run_id="2026-01-17_132657")
   fn_analysis = fs.load_error_cases(
       error_type="FN",
       features=["domain", "cert", "ml"]
   )
   ```

### 実装優先順位

| 優先度 | 項目 | 理由 |
|--------|------|------|
| 1 | 特徴量スキーマ定義 | 全ての基盤 |
| 2 | 特徴量ストア (parquet) | 分析効率化 |
| 3 | FeatureStore クラス | 統一アクセス |
| 4 | 実験設定の構造化 | 再現性確保 |

---

## 生成された成果物

### 可視化ファイル

```
artifacts/2026-01-17_132657/results/analysis_visualizations/
├── confusion_matrices.png
├── ml_distribution_analysis.png
├── certificate_analysis.png
├── fn_cases.csv
├── fp_cases.csv
├── extra_fp_cases.csv
├── extra_tp_cases.csv
└── fn_cases_enriched.csv    # 拡張FNデータ（26カラム）
```

### 分析スクリプト

```
scripts/analyze_evaluation_results.py
```

---

## 次のステップ

1. **FN詳細分析の実行**
   - `fn_cases_enriched.csv`を使用
   - ドメイン・証明書パターンの抽出
   - 改善策の導出

2. **パイプライン設計の改善**
   - 特徴量スキーマの定義
   - 02_main.pyの修正（cert_full_info_map の全項目をCSVに含める）
   - または特徴量ストアの実装

3. **評価の考察**
   - Handoff SubsetでF1が+0.0012改善
   - コスト76削減の意義の検討
   - FP増加（+44）vs FN削減（-40）のトレードオフ分析

---

---

## 5. データ仕様の全体設計

### 5.1 設計の動機

パイプラインで特徴量が脱落する問題を解決するため、データ仕様の全体設計を行った。

**要件**:
1. Stage1→Stage2→Stage3で特徴量を落とさない
2. 各Stageの分類結果を全て保持する
3. 分析・デバッグ・論文執筆に必要な情報を追跡可能にする

### 5.2 保持すべきデータの整理

#### 特徴量

| カテゴリ | 項目数 | 内容 |
|----------|--------|------|
| ML特徴量 | 42 | XGBoostで使用する全特徴量 |
| 生証明書情報 | 11 | cert_full_info_mapの全項目（人間可読） |
| 分析用 | 5+ | TLD分類、ブランド検出結果 |

#### ML特徴量の内訳

- ドメイン関連: 8項目（domain_length, hyphen_count, digit_ratio等）
- 証明書関連: 27項目（cert_validity_days, cert_is_lets_encrypt, cert_serial_entropy等）
- その他: 7項目（entropy, vowel_ratio, contains_brand等）

#### 各Stageの判定結果

| Stage | 保持項目 |
|-------|----------|
| Stage1 | proba, pred, decision, confidence |
| Stage2 | candidate, selected, decision, reason, lr_proba |
| Stage3 | pred, confidence, reasoning, gate_fired, gate_decision, llm_raw_decision, brand_detected, brand_score, tool_calls, processing_time_ms, error |

### 5.3 ストレージ方式の検討

#### 案1: Parquetファイル

```
artifacts/{RUN_ID}/
├── features/
│   ├── ml_features.parquet        # 42項目
│   ├── raw_cert_info.parquet      # 11項目
│   └── analysis_features.parquet  # 5+項目
├── predictions/
│   ├── stage1_results.parquet
│   ├── stage2_results.parquet
│   └── stage3_results.parquet
└── unified/
    └── unified_results.parquet    # 全結合ビュー
```

**メリット**: 実験ごとに分離、ポータブル、pandas直接読み込み
**デメリット**: JOINにpandas必要、更新は全ファイル書き換え

#### 案2: PostgreSQL

既存の`rapids_data`データベースを拡張。

```sql
-- 実験管理
CREATE TABLE experiments (run_id, created_at, config, status);

-- 特徴量（実験横断で共有）
CREATE TABLE ml_features (domain PRIMARY KEY, ...42カラム);
CREATE TABLE raw_cert_info (domain PRIMARY KEY, ...11カラム);

-- Stage結果（実験ごと）
CREATE TABLE stage1_results (run_id, domain, y_true, proba, pred, decision, ...);
CREATE TABLE stage2_results (run_id, domain, candidate, selected, ...);
CREATE TABLE stage3_results (run_id, domain, agent_pred, gate_fired, ...);

-- 統合ビュー
CREATE VIEW unified_results AS
SELECT ... FROM stage1_results s1
LEFT JOIN ml_features mf ON s1.domain = mf.domain
LEFT JOIN stage2_results s2 ON s1.run_id = s2.run_id AND s1.domain = s2.domain
LEFT JOIN stage3_results s3 ON s1.run_id = s3.run_id AND s1.domain = s3.domain;
```

**メリット**:
- SQLで直接分析可能（FN/FPパターン分析等）
- 実験比較が容易（run_idでWHERE句）
- 特徴量の重複抽出を回避
- ビューで自動JOIN
- ACID保証、外部キー制約

**デメリット**:
- DBサーバー必要（既に稼働中なので問題なし）
- pandas連携にpsycopg2必要

### 5.4 比較表

| 観点 | Parquet | PostgreSQL |
|------|---------|------------|
| JOIN/分析 | pandas必要 | SQLで直接 |
| データ整合性 | ファイル単位 | ACID保証 |
| スキーマ強制 | 弱い | 強い |
| 更新 | 全ファイル書き換え | 行単位 |
| 実験分離 | ディレクトリ | run_idカラム |
| 既存環境 | 新規 | **rapids_data稼働中** |

### 5.5 今後の方針

**PostgreSQL案が有力**だが、全体設計を完了してから段階的に実装する。

#### 設計フェーズ（次のステップ）

1. テーブル設計の詳細化（カラム定義、制約、インデックス）
2. マイグレーション計画（既存データの移行方法）
3. アクセスパターンの整理（どのようなクエリが必要か）
4. Python APIの設計（FeatureStoreクラス等）

#### 実装フェーズ

1. スキーマ作成（CREATE TABLE）
2. 既存データからの初期投入
3. 02_main.py修正（新形式で出力）
4. evaluate_e2e.py修正
5. 分析ノートブック対応

---

## 生成された成果物

### ドキュメント

- `docs/data_specification_draft.md` - データ仕様設計書（ドラフト）

### 可視化ファイル

```
artifacts/2026-01-17_132657/results/analysis_visualizations/
├── confusion_matrices.png
├── ml_distribution_analysis.png
├── certificate_analysis.png
├── fn_cases.csv
├── fn_cases_enriched.csv    # 拡張FNデータ（26カラム）
├── fp_cases.csv
├── extra_fp_cases.csv
└── extra_tp_cases.csv
```

---

## 次のステップ

1. **データ仕様の詳細設計**
   - PostgreSQLスキーマの詳細定義
   - インデックス設計
   - アクセスパターンの整理

2. **FN詳細分析**（並行して進める）
   - 暫定対応（cert_full_info_mapマージ）で分析実施
   - 改善策の導出

3. **実装計画の策定**
   - マイグレーション手順
   - 後方互換性の確保

---

## 6. 実装方針の決定: CSV拡張

### 6.1 PostgreSQL vs CSV の再検討

PostgreSQL移行のメリットを分析した結果、**CSV拡張で十分**と判断。

| 観点 | PostgreSQL | CSV拡張 |
|------|------------|---------|
| 分析効率 | SQL直接 | pandas（現状と同じ） |
| 特徴量保持 | ✓ 解決 | ✓ 解決 |
| ストレージ | 共有で効率的 | ファイル増大（許容範囲） |
| 実装コスト | 大 | **小** |
| 現行システム | 大幅改修 | **最小限の修正** |

**決定理由**:
- 現在のシステムは正常に動作している
- 特徴量脱落の問題はCSV拡張で解決可能
- DB移行は「あれば便利」だが必須ではない
- バイナリデータ（生証明書）は`rapids_data`に残すためCSVで問題なし

### 6.2 CSV拡張の修正方針

#### 修正対象ファイル

| ファイル | 修正内容 | 影響度 |
|----------|----------|--------|
| `02_main.py` | `df_stage1`に全特徴量を追加 | **主要** |
| `evaluate_e2e.py` | 最小限（自動的に引き継がれる） | 軽微 |
| 分析スクリプト | マージ処理の削除/簡略化 | 軽微 |

#### カラム数の変化

```
現在:   7列  (domain, source, ml_probability, stage1_decision, y_true,
              cert_validity_days, cert_san_count)
    ↓
拡張後: 70列 (基本8 + ML特徴量42 + 証明書情報20)
```

#### 主な修正コード（02_main.py:1842-1875）✅ 実装完了

```python
# 基本情報カラム（8列）
df_stage1 = pd.DataFrame({
    'domain': domain_test,
    'source': source_test,
    'tld': tld_test,
    'ml_probability': p_test,
    'stage1_decision': stage1_decision,
    'stage1_pred': (p_test >= 0.5).astype(int),
    'y_true': y_test.astype(int),
    'label': y_test.astype(int),  # エイリアス
})

# ML特徴量カラム（42列、ml_ プレフィックス）
for i, feat_name in enumerate(FEATURE_ORDER):
    df_stage1[f'ml_{feat_name}'] = X_test[:, i]

# 証明書情報カラム（20列、cert_ プレフィックス）
cert_info_fields = [
    'issuer_org', 'cert_age_days', 'is_free_ca', 'san_count',
    'is_wildcard', 'is_self_signed', 'has_organization',
    'not_before', 'not_after', 'validity_days', 'valid_days',
    'has_certificate', 'has_crl_dp',
    'key_type', 'key_size', 'issuer_country', 'issuer_type',
    'signature_algorithm', 'common_name', 'subject_org',
]
for field in cert_info_fields:
    df_stage1[f'cert_{field}'] = [
        cert_full_info_map.get(d, {}).get(field) for d in domain_test
    ]
```

#### ファイルサイズ見積もり

- 現在: ~5-10 MB
- 拡張後: ~50-80 MB（許容範囲）

### 6.3 実装手順

1. **Phase 1**: `02_main.py` の修正
   - `df_stage1` 作成部分を拡張
   - テスト実行して出力確認

2. **Phase 2**: `evaluate_e2e.py` の確認
   - 拡張CSVの読み込み確認
   - 出力に特徴量が含まれることを確認

3. **Phase 3**: 分析スクリプトの更新
   - マージ処理を削除/簡略化
   - 拡張CSVを直接使用するように変更

4. **Phase 4**: ドキュメント更新
   - カラム定義のドキュメント化

---

## 生成された成果物

### ドキュメント

- `docs/data_specification_draft.md` - データ仕様設計書（ドラフト）
- `docs/postgresql_schema.sql` - PostgreSQLスキーマ定義（参考用）
- `docs/access_patterns.md` - アクセスパターン整理
- `docs/csv_extension_plan.md` - CSV拡張修正方針

### 可視化ファイル

```
artifacts/2026-01-17_132657/results/analysis_visualizations/
├── confusion_matrices.png
├── ml_distribution_analysis.png
├── certificate_analysis.png
├── fn_cases.csv
├── fn_cases_enriched.csv    # 拡張FNデータ（26カラム）
├── fp_cases.csv
├── extra_fp_cases.csv
└── extra_tp_cases.csv
```

---

## 次のステップ

1. **CSV拡張の実装**
   - `02_main.py` の修正
   - テスト実行・検証

2. **FN詳細分析**（並行して進める）
   - 暫定対応（cert_full_info_mapマージ）で分析実施
   - 改善策の導出

---

---

## 7. 実装完了

### 7.1 仕様書の作成

`docs/spec/data_specification_v1.md` を作成:
- cert_full_info_map: 20フィールド定義
- df_stage1: 70カラム定義
- 互換性要件

### 7.2 02_main.py の修正

| 修正箇所 | 内容 |
|----------|------|
| lines 1450-1473 | cert_full_info_map に `valid_days`, `has_crl_dp` 追加 |
| lines 1507-1528 | CRL Distribution Point 抽出ロジック追加 |
| lines 1842-1875 | df_stage1 を70カラムに拡張 |

### 7.3 データI/O整合性

`docs/data_io_consistency.md` を作成:
- `validity_days` vs `valid_days` の不整合を発見・解決
- `has_crl_dp` の欠落を発見・解決
- 後方互換性のためエイリアスを追加

### 7.4 検証結果

パイプライン実行により実装を検証:

```
RUN_ID: 2026-01-17_132657
Phishing: 320,166 samples
Trusted:  320,166 samples
Total:    640,332 samples
```

**出力検証**:

| 出力ファイル | 検証結果 |
|-------------|---------|
| stage1_decisions_latest.csv | ✅ 70列 (8基本 + 42ML + 20証明書) |
| handoff_candidates_latest.csv | ✅ 71列 (70 + prediction_proba) |
| cert_full_info_map.pkl | ✅ 20フィールド/ドメイン |

**cert_full_info_map フィールド一覧** (検証済み):
```
issuer_org, cert_age_days, is_free_ca, san_count, is_wildcard,
is_self_signed, has_organization, not_before, not_after, validity_days,
valid_days, has_certificate, has_crl_dp, key_type, key_size,
issuer_country, issuer_type, signature_algorithm, common_name, subject_org
```

---

## 本日の成果まとめ

### データパイプライン改善
1. **設計問題の発見**: 特徴量がパイプラインで脱落する問題を特定
2. **仕様書作成**: データ仕様書 v1.0 (`docs/spec/data_specification_v1.md`)
3. **実装完了**: 02_main.py を修正し、70カラムのCSV出力を実装
4. **検証完了**: evaluate_e2e.py, precheck_module.py の動作確認

### FN分析
5. **Stage1/2 FN分析**: 1,386件のFN原因を特定
   - 65.1%がCRL DPルールによるbenign判定
   - 8.2%がワイルドカードルール
6. **AI Agent FN救済分析**: 3000件評価データを詳細分析
   - FN救済率: ML確率0.0-0.1で8.8%、0.3-0.4で35.3%
   - CRL DP有りのFNは救済が困難

### データセット品質検証
7. **VirusTotal調査**: FP候補53件を検証
   - **26件(49%)がVT検出** → trustedデータのラベリングエラー
   - AI Agent性能を上方修正: F1 0.81→0.83, Precision 0.87→0.91
   - 高リスク: `adidas.one`(9), `ky2828.cc`(11)

8. **Stage1 FP/FN全件VT調査開始**
   - 対象: 2,172件（FP:339, FN:1,833）
   - 並列化・再開対応スクリプト作成
   - 推定所要日数: 5日（1APIキー）

### 主要発見
- **trustedデータセットに品質問題あり** - 全体のVT検証を推奨
- **AI Agentは評価以上に高性能** - "FP"の半数は実際には正解
- **CRL DPルールが最大のFN要因** - Stage2/3両方に影響

### 継続中のタスク
- [ ] Stage1 FP/FN VirusTotal調査（5日間、2026-01-26完了予定）

---

## 8. FN分析結果

### 8.1 概要

| 項目 | 値 |
|------|-----|
| 対象 | Handoff候補 19,479件 |
| FN件数 | 1,386件 |
| TP件数 | 3,767件 |

### 8.2 FN原因の内訳

| 原因 | 件数 | 割合 | 説明 |
|------|------|------|------|
| **Stage2 Cert Benignルール** | **975** | **70.3%** | |
| - CRL DPルール | 902 | 65.1% | CRL Distribution Point有りでbenign判定 |
| - ワイルドカード | 113 | 8.2% | ワイルドカード証明書でbenign判定 |
| - 長期有効 | 95 | 6.9% | >180日でbenign判定 |
| Safe Benign (p1<0.15) | 70 | 5.1% | ML確率低く自動benign |
| ボーダーライン (p1: 0.15-0.5) | 341 | 24.6% | ML決定境界付近 |

### 8.3 主要発見

#### CRL DPルールの問題

**FNの65.1%がCRL Distribution Point有りでbenign判定されている。**

| グループ | CRL DP有り率 |
|----------|-------------|
| FN | **65.1%** |
| TP | 34.3% |
| 比率 | **1.9倍** |

これはフィッシング攻撃者がCRL DPを持つCAを選択している可能性を示唆。

#### TLD分布の偏り

| TLD | FN | TP | FN/TP比 | 備考 |
|-----|-----|-----|---------|------|
| .com | 838 (60.5%) | 1543 (41.0%) | 1.48 | 最多 |
| .br | 27 | 0 | ∞ | FN専用 |
| .lat | 21 | 0 | ∞ | FN専用 |
| .in | 12 | 0 | ∞ | FN専用 |
| .ar | 10 | 0 | ∞ | FN専用 |
| .cl | 10 | 0 | ∞ | FN専用 |

一部の国別TLDがFN専用になっており、TLDフィルタリングの見直しが必要。

### 8.4 改善提案

1. **CRL DPルールの緩和/条件追加**
   - 現状: CRL DP有り → benign候補
   - 提案: CRL DP有り + 他のbenign指標（SAN数、有効期間等）の複合条件へ

2. **国別TLDの再評価**
   - `.br`, `.lat`, `.in`, `.ar`, `.cl` は現在safe扱いだが、フィッシングが存在
   - TLDリスクスコアの動的更新を検討

3. **ML確率しきい値の見直し**
   - Safe Benign閾値 0.15 は適切か？
   - ボーダーライン341件の詳細分析が必要

### 8.5 次のアクション

- [ ] CRL DPルールの条件追加を検討
- [ ] FN専用TLDのリスク再評価
- [x] Stage3 AI AgentでFNパターンの救済可能性を検証 ✅ 2026-01-20

---

## 9. AI Agent 3000件評価分析

### 9.1 概要

| 項目 | 値 |
|------|-----|
| 評価サンプル数 | 3,000件 |
| 正解ラベル | benign: 2,191, phishing: 809 |
| Stage1 FN | 240件 |
| Final FN | 200件 |

### 9.2 混同行列の変化

| 指標 | Stage1 | Final | 差分 |
|------|--------|-------|------|
| TP | 569 | 609 | **+40** |
| FN | 240 | 200 | **-40** |
| FP | 47 | 91 | +44 |
| TN | 2,144 | 2,100 | -44 |

### 9.3 AI Agentによる判定変更

| 変更タイプ | 件数 | 効果 | コスト影響 |
|------------|------|------|-----------|
| FN救済 | 44 | ✅ 良い | -132 (44×3) |
| FP救済 | 9 | ✅ 良い | -9 |
| FP新規 | 53 | ❌ 悪い | +53 |
| FN新規 | 4 | ❌ 悪い | +12 |
| **ネットコスト** | | | **-76** |

### 9.4 FN救済パターン

#### ML確率帯別の救済率

| ML確率帯 | 救済件数/FN件数 | 救済率 |
|----------|-----------------|--------|
| [0.0-0.1) | 10/114 | **8.8%** |
| [0.1-0.2) | 9/40 | 22.5% |
| [0.2-0.3) | 5/25 | 20.0% |
| [0.3-0.4) | 12/34 | **35.3%** |
| [0.4-0.5) | 8/27 | 29.6% |

**発見**: ML確率が低い（0.1未満）FNは救済率8.8%と低い。AI AgentもML特徴に影響を受けている可能性。

#### FN救済成功 vs 失敗の比較

| 特徴 | 救済成功 | 救済失敗 |
|------|----------|----------|
| ML確率 (mean) | 0.243 | 0.158 |
| CRL DP有り | 56.8% | **71.4%** |
| ワイルドカード | 0.0% | **11.2%** |

**発見**: CRL DP有りやワイルドカード証明書はAI Agentでも救済が難しい。

### 9.5 FP新規作成の分析

53件の「FP」（trusted→phishing判定）を確認した結果：

**ラベリングエラーの疑い（AI Agent正解の可能性）**:
- `grandpashabet1299.info` - ギャンブル
- `viagra-purchase.com` - 医薬品スパム
- `levcasino-online.click` - カジノ
- `pussy888.pw` - アダルト/ギャンブル
- `gabapentintabs.shop` - 医薬品

**真のFP（AI Agent誤判定）**:
- `www.gov.za` - 南アフリカ政府
- `refine.dev`, `zellij.dev` - 開発ツール

→ trustedデータセットの品質確認が必要

### 9.6 結論

1. **AI Agentは有効**: FN 40件救済、コスト76削減
2. **限界あり**: ML確率0.1未満のFNは救済率8.8%と低い
3. **CRL DPの影響**: AI AgentもCRL DP有り証明書のフィッシングを見逃しやすい
4. **データセット品質**: trustedデータに疑わしいドメインが含まれている可能性

---

## 10. VirusTotalによるtrustedデータ検証

### 10.1 調査概要

AI Agentが「FP」と判定した53件（trustedラベルだがphishing判定）をVirusTotalで検証。

| 項目 | 値 |
|------|-----|
| 調査対象 | 53ドメイン |
| VT検出あり | **26件 (49.1%)** |
| 真のFP | 27件 |

### 10.2 高リスクドメイン（VT malicious≥2）

| ドメイン | Malicious | Suspicious |
|----------|-----------|------------|
| `ky2828.cc` | **11** | 0 |
| `adidas.one` | **9** | 0 |
| `comsetups.xyz` | 4 | 0 |
| `selcuksportss.xyz` | 2 | 2 |

**`adidas.one`**: ブランド偽装。trustedデータに含まれていたのは問題。

### 10.3 中リスクドメイン（malicious=1 or suspicious≥2）

- `viagra-purchase.com` (mal=1) - 医薬品スパム
- `vardenafil.click` (mal=1, sus=1) - 医薬品
- `propecia24x365.top` (mal=1, sus=2) - 医薬品
- `mahjongwins3blackscatter.site` (mal=1) - ギャンブル
- `c9taya.shop` (mal=1) - 不審
- `corelle-outlet.top` (sus=3) - 偽ブランド?

### 10.4 AI Agent性能の再評価

| 指標 | 元評価 | VT補正後 | 改善 |
|------|--------|----------|------|
| Precision | 0.8700 | **0.9071** | **+3.71%** |
| Recall | 0.7528 | 0.7605 | +0.77% |
| F1 | 0.8072 | **0.8274** | **+2.02%** |

### 10.5 判定変更の再評価

| 変更タイプ | 元 | VT補正後 | 備考 |
|------------|-----|----------|------|
| FN救済 | 44 | 44 | ✅ |
| **TP追加（VT検出）** | 0 | **26** | ✅ 新規発見 |
| FP新規 | 53 | **27** | ❌ 減少 |
| FP救済 | 9 | 9 | ✅ |
| FN新規 | 4 | 4 | ❌ |

### 10.6 結論

1. **trustedデータに26件のラベリングエラー** が存在
2. **AI Agentは実際にはより高性能** - FP 53件のうち26件は正解だった
3. **Precision 0.87 → 0.91**, **F1 0.81 → 0.83** に上方修正
4. **trustedデータ全体のVirusTotal検証を推奨**

---

## 11. Stage1 FP/FN 全件VirusTotal調査

### 11.1 調査概要

Stage1のFP/FN全件をVirusTotalで検証し、データセットのラベリングエラーを特定する。

| 項目 | 値 |
|------|-----|
| 対象 | Stage1 FP + FN |
| FP (trusted→phishing誤判定) | 339件 |
| FN (phishing→benign誤判定) | 1,833件 |
| **合計** | **2,172件** |

### 11.2 調査スクリプト

**ファイル**: `scripts/vt_batch_investigation.py`

| 機能 | 説明 |
|------|------|
| 並列処理 | 複数APIキーで同時実行可能 |
| 増分保存 | 1件ごとにCSV保存 + fsync |
| 再開対応 | サーバ再起動後も自動再開 |
| 日次制限 | 500件/日/キーで自動停止 |

### 11.3 調査スケジュール

| APIキー数 | 1日あたり | 所要日数 |
|-----------|----------|----------|
| 1キー | 500件 | 5日 |
| 2キー | 1,000件 | 3日 |
| 3キー | 1,500件 | 2日 |

### 11.4 調査開始

- **開始日時**: 2026-01-21 03:34
- **APIキー数**: 1
- **推定完了**: 2026-01-26

### 11.5 出力ファイル

| ファイル | 内容 |
|----------|------|
| `stage1_fp_fn_domains.csv` | 調査対象2,172件 |
| `vt_investigation_results.csv` | VT調査結果（増分保存） |
| `vt_investigation_checkpoint.json` | 再開用チェックポイント |

### 11.6 再開コマンド

```bash
# サーバ再起動後の再開
python scripts/vt_batch_investigation.py

# 進捗確認
tail -f logs/vt_investigation.log

# チェックポイント確認
cat artifacts/2026-01-17_132657/results/vt_investigation_checkpoint.json
```

### 11.7 期待される成果

1. **FP調査** (339件) → trustedデータのラベリングエラー特定
2. **FN調査** (1,833件) → phishingデータのラベリングエラー特定（無効化済みドメイン等）
3. **データセット品質改善** → ML再学習時の精度向上
