{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e8ad6",
   "metadata": {
    "tags": [
     "nextstep-minfix"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b97a300f",
   "metadata": {},
   "source": [
    "# 04-3 (COMPAT FIXED2)\n",
    "\n",
    "handoff å‡ºåŠ›ã‚’ä»•æ§˜ã«çµ±ä¸€ã—ã€04-4/04-5 ã¸ç¢ºå®Ÿã«å¼•ãç¶™ã’ã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚\n",
    "- å‡ºåŠ›: `artifacts/{RUN_ID}/handoff/04-3_llm_tools_setup.pkl`\n",
    "- ç›´åˆ—åŒ–: cloudpickle å„ªå…ˆï¼ˆfallback ã‚ã‚Šï¼‰\n",
    "- `cfg/tools/llm` ã¨ä»£è¡¨çš„ãª `stats` ã‚’ä¿å­˜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c3e10",
   "metadata": {},
   "source": [
    "\n",
    "# 03_ai_agent_analysis_part3_fixed â€” Configæ©Ÿèƒ½è¿½åŠ  & Controller APIï¼ˆæ”¹ä¿®ç‰ˆï¼‰\n",
    "- ã‚»ãƒ«1: `load_configuration()`ï¼ˆConfigèª­ã¿è¾¼ã¿ãƒ»æ¤œè¨¼ãƒ»ãƒžãƒ¼ã‚¸ï¼‰\n",
    "- ã‚»ãƒ«2: Robust handoff loaderï¼ˆPart2å—ã‘å–ã‚Šï¼‰\n",
    "- ã‚»ãƒ«3: DBè¨­å®šç®¡ç†ï¼ˆcfg/handoff/envå„ªå…ˆé †ä½ï¼‰\n",
    "- ã‚»ãƒ«4: Handoffå±•é–‹ï¼ˆå¿…é ˆã‚­ãƒ¼æ¤œè¨¼ï¼‰\n",
    "- ã‚»ãƒ«5: **TLDåˆ†å¸ƒåˆ†æž**ï¼ˆä¸­æ ¸æ©Ÿèƒ½ã‚’ä¿æŒã—ãŸã¾ã¾Configåæ˜ ï¼‰\n",
    "- ã‚»ãƒ«6: Handoffä¿å­˜ï¼ˆpkl + JSONï¼‰\n",
    "- ã‚»ãƒ«7: Controller API `explanation_quality()`ï¼ˆã‚¨ã‚¤ãƒªã‚¢ã‚¹ `tld_risk_analysis`ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f5c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NX] Current Run ID: 2026-01-03_222059 (loaded from /data/hdd/asomura/nextstep/artifacts/_current/run_id.txt)\n",
      "[NX] RUN_ID = 2026-01-03_222059 | handoff = artifacts/2026-01-03_222059/handoff\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0: å…±é€šãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ï¼ˆStrict Mode: _current/run_id.txt å¿…é ˆç‰ˆï¼‰ ===\n",
    "import os, sys, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def bootstrap_run_id() -> str:\n",
    "    ROOT = Path(\".\").resolve()\n",
    "    \n",
    "    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ãƒ‘ã‚¹: artifacts/_current/run_id.txt ã‚’å‚ç…§\n",
    "    # ï¼ˆ_current ã¯é€šå¸¸ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã™ï¼‰\n",
    "    id_file = ROOT / \"artifacts\" / \"_current\" / \"run_id.txt\"\n",
    "    \n",
    "    # 1. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯\n",
    "    if not id_file.exists():\n",
    "        error_msg = (\n",
    "            f\"âŒ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼: å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {id_file}\\n\"\n",
    "            \"å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„ã‹ã€_current ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/ãƒªãƒ³ã‚¯ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\\n\"\n",
    "            \"å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚\"\n",
    "        )\n",
    "        print(f\"[NX] {error_msg}\")\n",
    "        # ã“ã“ã§ç¢ºå®Ÿã«æ­¢ã‚ã‚‹\n",
    "        raise FileNotFoundError(error_msg)\n",
    "\n",
    "    # 2. IDèª­ã¿è¾¼ã¿\n",
    "    run_id = id_file.read_text().strip()\n",
    "\n",
    "    # 3. ç©ºãƒã‚§ãƒƒã‚¯\n",
    "    if not run_id:\n",
    "        raise ValueError(f\"âŒ ã‚¨ãƒ©ãƒ¼: {id_file} ã®ä¸­èº«ãŒç©ºã§ã™ã€‚\")\n",
    "\n",
    "    print(f\"[NX] Current Run ID: {run_id} (loaded from {id_file})\")\n",
    "    \n",
    "    # ç’°å¢ƒå¤‰æ•°ã‚’ã‚»ãƒƒãƒˆï¼ˆå¾Œç¶šã®äº’æ›æ€§ç¶­æŒã®ãŸã‚ï¼‰\n",
    "    os.environ[\"RUN_ID\"] = run_id\n",
    "    return run_id\n",
    "\n",
    "# === å®Ÿè¡Œ ===\n",
    "RUN_ID = bootstrap_run_id()\n",
    "\n",
    "# --- ä»¥ä¸‹ã€å¾Œç¶šã‚»ãƒ«ã®ãŸã‚ã®äº’æ›æ€§shimï¼ˆpaths.pyã®é…ç½®ï¼‰ ---\n",
    "ROOT = Path(\".\").resolve()\n",
    "(Path(\"_compat\")).mkdir(exist_ok=True)\n",
    "\n",
    "if Path(\"paths.py\").exists():\n",
    "    target_shim = Path(\"_compat/paths.py\")\n",
    "    # å†…å®¹ãŒç•°ãªã‚‹å ´åˆã®ã¿ã‚³ãƒ”ãƒ¼\n",
    "    if not target_shim.exists() or Path(\"paths.py\").read_text() != target_shim.read_text():\n",
    "        shutil.copyfile(\"paths.py\", \"_compat/paths.py\")\n",
    "\n",
    "Path(\"_compat/__init__.py\").write_text(\"# shim pkg\\n\")\n",
    "\n",
    "from _compat.paths import ensure_roots, compat_base_dirs\n",
    "ensure_roots()\n",
    "\n",
    "print(f\"[NX] RUN_ID = {RUN_ID} | handoff = {compat_base_dirs['handoff']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca4f32-7017-4a37-bcd2-df18864edcfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab47c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… IO guard ready -> artifacts/2026-01-03_222059\n",
      "ðŸ”§ Config loaded (cert_only_mode=True, tld_analysis.enabled=True)\n"
     ]
    }
   ],
   "source": [
    "# === ã‚»ãƒ«1: ç’°å¢ƒåˆæœŸåŒ–ãƒ»ãƒ‘ã‚¹è¨­å®š + load_configuration ===\n",
    "import os, json, typing, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# --- [FIX START] ä¿®æ­£ç®‡æ‰€: pathsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ˜Žç¤ºçš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨BASE_DIRSã®å®šç¾© ---\n",
    "try:\n",
    "    # Cell 0 ã§ç”Ÿæˆã•ã‚ŒãŸ _compat ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
    "    import _compat.paths as paths\n",
    "except ImportError:\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "    import paths\n",
    "\n",
    "# paths.py ã® compat_base_dirs ã‚’ BASE_DIRS ã¨ã—ã¦å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹\n",
    "BASE_DIRS = paths.compat_base_dirs\n",
    "# --- [FIX END] -------------------------------------------------------------\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "except Exception:\n",
    "    yaml = None\n",
    "\n",
    "RUN_ID = getattr(paths, 'RUN_ID', None)\n",
    "ARTIFACTS = Path(BASE_DIRS['handoff']).parent\n",
    "\n",
    "RAW, PROCESSED, MODELS, RESULTS, HANDOFF, LOGS, TRACES = [ARTIFACTS / d for d in [\"raw\",\"processed\",\"models\",\"results\",\"handoff\",\"logs\",\"traces\"]]\n",
    "RAW_DIR, PROCESSED_DIR, MODELS_DIR, RESULTS_DIR, HANDOFF_DIR, LOGS_DIR, TRACES_DIR = map(str, [RAW, PROCESSED, MODELS, RESULTS, HANDOFF, LOGS, TRACES])\n",
    "print(f\"âœ… IO guard ready -> artifacts/{RUN_ID}\")\n",
    "\n",
    "def _deep_update(base: dict, override: dict) -> dict:\n",
    "    base = dict(base or {})\n",
    "    for k, v in (override or {}).items():\n",
    "        if isinstance(v, dict) and isinstance(base.get(k), dict):\n",
    "            base[k] = _deep_update(base[k], v)\n",
    "        else:\n",
    "            base[k] = v\n",
    "    return base\n",
    "\n",
    "from typing import Optional, Dict, Any\n",
    "def load_configuration(config_path: Optional[str] = None,\n",
    "                       cfg_override: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    defaults = {\n",
    "        \"system\": {\"cert_only_mode\": True, \"seed\": 42},\n",
    "        \"db\": {\"dbname\":\"rapids_data\",\"user\":\"postgres\",\"password\":\"asomura\",\"host\":\"localhost\",\"port\":\"5432\",\"timeout_s\":30,\"read_only\":True},\n",
    "        \"tld_analysis\": {\"enabled\":True,\"use_all_data\":True,\"balance_data\":True,\"sample_size\":None,\"min_sample_size\":10,\n",
    "                         \"max_dangerous_tlds\":30,\"max_legitimate_tlds\":30,\"percentile_thresholds\":[50,75,90],\"fn_weight\":1.5},\n",
    "        \"engine\": {\"max_concurrent\":20,\"max_retries\":3,\"batch_size\":10000},\n",
    "        \"paths\": {\"base_dir\":\"artifacts\"},\n",
    "        \"llm\": {\"enabled\":False}\n",
    "    }\n",
    "    cfg = dict(defaults)\n",
    "    \n",
    "    # config.json ã®ãƒ‘ã‚¹è£œæ­£ï¼ˆ_compatã«ã‚ã‚‹å ´åˆï¼‰\n",
    "    if not config_path and Path(\"_compat/config.json\").exists():\n",
    "        config_path = \"_compat/config.json\"\n",
    "        \n",
    "    if config_path and Path(config_path).exists():\n",
    "        try:\n",
    "            text = Path(config_path).read_text(encoding=\"utf-8\")\n",
    "            if Path(config_path).suffix.lower() in (\".yml\",\".yaml\") and yaml:\n",
    "                cfg = _deep_update(cfg, yaml.safe_load(text) or {})\n",
    "            else:\n",
    "                cfg = _deep_update(cfg, json.loads(text))\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­è¾¼å¤±æ•—: {e}\")\n",
    "    env_map = {\"USE_ALL_DATA\":(\"tld_analysis\",\"use_all_data\"),\n",
    "               \"BALANCE_DATA\":(\"tld_analysis\",\"balance_data\"),\n",
    "               \"PGDATABASE\":(\"db\",\"dbname\"),\n",
    "               \"PGUSER\":(\"db\",\"user\"),\n",
    "               \"PGPASSWORD\":(\"db\",\"password\")}\n",
    "    for env, path in env_map.items():\n",
    "        if env in os.environ and os.environ[env]!=\"\":\n",
    "            val = os.environ[env]\n",
    "            if isinstance(val,str) and val.lower() in (\"true\",\"false\"):\n",
    "                val = (val.lower()==\"true\")\n",
    "            node = cfg\n",
    "            for k in path[:-1]:\n",
    "                node = node.setdefault(k,{})\n",
    "            node[path[-1]] = val\n",
    "    if isinstance(cfg_override, dict):\n",
    "        cfg = _deep_update(cfg, cfg_override)\n",
    "    if not cfg[\"system\"].get(\"cert_only_mode\", False):\n",
    "        raise ValueError(\"system.cert_only_mode ã¯ True å¿…é ˆã§ã™\")\n",
    "    if not cfg.get(\"tld_analysis\",{}).get(\"enabled\", False):\n",
    "        raise ValueError(\"tld_analysis.enabled ã¯ True å¿…é ˆã§ã™\")\n",
    "    for k in (\"dbname\",\"user\",\"host\",\"port\"):\n",
    "        if not str(cfg[\"db\"].get(k,\"\")).strip():\n",
    "            raise ValueError(f\"DBå¿…é ˆé …ç›®ä¸è¶³: db.{k}\")\n",
    "    globals()[\"cfg\"] = cfg\n",
    "    print(\"ðŸ”§ Config loaded (cert_only_mode=True, tld_analysis.enabled=True)\")\n",
    "    return cfg\n",
    "\n",
    "# ä¿®æ­£å‰:\n",
    "# cfg = load_configuration(os.getenv(\"CONFIG_PATH\"), None)\n",
    "\n",
    "# ä¿®æ­£å¾Œ: system.cert_only_mode ã‚’å¼·åˆ¶çš„ã« True ã«è¨­å®šã—ã¦èª­ã¿è¾¼ã‚€\n",
    "cfg = load_configuration(os.getenv(\"CONFIG_PATH\"), {\"system\": {\"cert_only_mode\": True}})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "052b2f47-5e1c-4c1f-946f-37039d0ce873",
   "metadata": {},
   "source": [
    "\n",
    "# === ã‚»ãƒ«2: Robust handoff loader ===\n",
    "import joblib, glob\n",
    "Path(HANDOFF_DIR).mkdir(parents=True, exist_ok=True)\n",
    "cands = [str(Path(HANDOFF_DIR)/\"03_ai_agent_analysis_part2.pkl\"),\n",
    "         \"handoff/03_ai_agent_analysis_part2.pkl\"]\n",
    "if not any(Path(p).exists() for p in cands):\n",
    "    cands += sorted(glob.glob(\"artifacts/*/handoff/03_ai_agent_analysis_part2.pkl\"))\n",
    "src = next((p for p in cands if Path(p).exists()), None)\n",
    "if not src:\n",
    "    raise FileNotFoundError(\"Part2 handoff ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "handoff = joblib.load(src)\n",
    "print(f\"âœ… handoff loaded: {src}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e6189e-9fe5-4696-bb6d-e623026987eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading handoff from: artifacts/2026-01-03_222059/handoff/04-2_statistical_analysis.pkl\n",
      "âœ… handoff loaded successfully.\n",
      "âœ… keys: ['cfg', 'RUN_ID', 'SESSION_ID', 'output_dirs', 'brand_keywords', 'cert_full_info_map', 'false_negatives_df', 'fn_features_df', 'HIGH_RISK_WORDS', 'suspicious_words_stats'] ...\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4 (FIXED): Robust handoff loader (04-2 â†’ 04-3) ===\n",
    "import joblib, glob, warnings\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºä¿\n",
    "Path(HANDOFF_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# âœ… æ­£ã—ã„å€™è£œãƒªã‚¹ãƒˆï¼ˆ04-2 ã®å‡ºåŠ›ã‚’å„ªå…ˆï¼‰\n",
    "cands = [\n",
    "    str(Path(HANDOFF_DIR) / \"04-2_statistical_analysis.pkl\"),\n",
    "    \"handoff/04-2_statistical_analysis.pkl\",\n",
    "]\n",
    "\n",
    "# ã‚‚ã—è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ artifacts/*/handoff ã‹ã‚‰æŽ¢ç´¢\n",
    "if not any(Path(p).exists() for p in cands):\n",
    "    cands += sorted(glob.glob(\"artifacts/*/handoff/04-2_statistical_analysis.pkl\"))\n",
    "\n",
    "src = next((p for p in cands if Path(p).exists()), None)\n",
    "if not src:\n",
    "    raise FileNotFoundError(\"04-2 handoff ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "\n",
    "print(f\"ðŸ“‚ Loading handoff from: {src}\")\n",
    "\n",
    "# === ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆå¯¾ç­– ===\n",
    "handoff = {}\n",
    "try:\n",
    "    # é€šå¸¸ã®ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ\n",
    "    handoff = joblib.load(src)\n",
    "    print(f\"âœ… handoff loaded successfully.\")\n",
    "except (TypeError, ValueError, ImportError) as e:\n",
    "    # Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³é•ã„(3.10/3.11 -> 3.12)ãªã©ã§ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "    warnings.warn(f\"âš ï¸ Pickle load failed due to version mismatch: {e}\")\n",
    "    print(\"ðŸ”„ Attempting fallback reconstruction from CSV/Defaults...\")\n",
    "\n",
    "    # 1. false_negatives_df ã‚’CSVã‹ã‚‰å¾©å…ƒï¼ˆã‚ã‚Œã°ï¼‰\n",
    "    # é€šå¸¸ artifacts/{RUN_ID}/results/false_negatives.csv ã«ã‚ã‚‹\n",
    "    fn_csv = Path(src).parent.parent / \"results\" / \"false_negatives.csv\"\n",
    "    if fn_csv.exists():\n",
    "        try:\n",
    "            handoff['false_negatives_df'] = pd.read_csv(fn_csv)\n",
    "            print(f\"  - Recovered 'false_negatives_df' from {fn_csv.name}\")\n",
    "        except Exception as csv_e:\n",
    "            print(f\"  - Failed to read CSV: {csv_e}\")\n",
    "            handoff['false_negatives_df'] = pd.DataFrame(columns=['domain','ml_probability'])\n",
    "    else:\n",
    "        print(\"  - CSV not found. Initializing empty 'false_negatives_df'.\")\n",
    "        handoff['false_negatives_df'] = pd.DataFrame(columns=['domain','ml_probability'])\n",
    "\n",
    "    # 2. ãã®ä»–ã®å¿…é ˆã‚­ãƒ¼ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–\n",
    "    # configã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—ã‚’è©¦ã¿ã‚‹\n",
    "    default_keywords = cfg.get(\"brand_keywords\", [\"paypal\", \"mercari\", \"amazon\", \"apple\", \"google\"])\n",
    "    if isinstance(default_keywords, dict): # è¾žæ›¸ãªã‚‰ã‚­ãƒ¼ãƒªã‚¹ãƒˆåŒ–\n",
    "        default_keywords = list(default_keywords.keys())\n",
    "    \n",
    "    handoff['brand_keywords'] = default_keywords\n",
    "    handoff['cert_full_info_map'] = {}  # è¨¼æ˜Žæ›¸æƒ…å ±ã¯ç©ºã§åˆæœŸåŒ–\n",
    "    handoff['fn_features_df'] = pd.DataFrame()\n",
    "    \n",
    "    print(\"âœ… Fallback handoff created with default values.\")\n",
    "\n",
    "# å†…å®¹ç¢ºèª\n",
    "print(f\"âœ… keys: {list(handoff.keys())[:10]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e137611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ DB_CONFIG ready: {'dbname': 'rapids_data', 'host': 'localhost', 'port': '5432', 'user': 'postgres'} (timeout=30s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === ã‚»ãƒ«3: DBè¨­å®šç®¡ç† ===\n",
    "import os\n",
    "def _db_from_env():\n",
    "    return {'dbname':os.getenv('PGDATABASE','rapids_data'),\n",
    "            'user':os.getenv('PGUSER','postgres'),\n",
    "            'password':os.getenv('PGPASSWORD','asomura'),\n",
    "            'host':os.getenv('PGHOST','localhost'),\n",
    "            'port':os.getenv('PGPORT','5432')}\n",
    "def build_db_config(cfg: dict, handoff: dict) -> dict:\n",
    "    if isinstance(cfg, dict) and 'db' in cfg:\n",
    "        base = {k:str(cfg['db'].get(k)) for k in ['dbname','user','password','host','port']}\n",
    "    elif isinstance(handoff, dict) and 'DB_CONFIG' in handoff:\n",
    "        base = dict(handoff['DB_CONFIG'])\n",
    "    else:\n",
    "        base = _db_from_env()\n",
    "    base['connect_timeout'] = int((cfg.get('db',{}) or {}).get('timeout_s',30)) if isinstance(cfg,dict) else 30\n",
    "    base['_read_only'] = bool((cfg.get('db',{}) or {}).get('read_only',True)) if isinstance(cfg,dict) else True\n",
    "    return base\n",
    "DB_CONFIG = build_db_config(cfg, handoff)\n",
    "print(\"ðŸ”§ DB_CONFIG ready:\", {k: DB_CONFIG[k] for k in ['dbname','host','port','user']}, f\"(timeout={DB_CONFIG['connect_timeout']}s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cd98b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… unpack: fn_rows=7710, brand_keywords=100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === ã‚»ãƒ«4: Handoffå±•é–‹ ===\n",
    "required = ['false_negatives_df','brand_keywords','cert_full_info_map','fn_features_df']\n",
    "miss = [k for k in required if k not in handoff]\n",
    "if miss: raise KeyError(f\"handoff keys missing: {miss}\")\n",
    "false_negatives_df = handoff['false_negatives_df']\n",
    "brand_keywords = handoff['brand_keywords']\n",
    "cert_full_info_map = handoff['cert_full_info_map']\n",
    "fn_features_df = handoff['fn_features_df']\n",
    "if 'DB_CONFIG' in handoff: DB_CONFIG = {**DB_CONFIG, **handoff['DB_CONFIG']}\n",
    "print(f\"âœ… unpack: fn_rows={getattr(false_negatives_df,'shape',[None,None])[0]}, brand_keywords={len(brand_keywords) if isinstance(brand_keywords,(list,dict,set,tuple)) else 'n/a'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3793169e-11f4-4efc-b3b7-67118e01fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04-3][CHECK] aliases -> BRAND_KEYWORDS=100, CERT_FULL_INFO_MAP=7710, TOOLS_DICT=0\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6.5: Promote handoff keys to UPPER aliases & guard types ===\n",
    "# å‰æ: Cell6 ã§ handoff ã‚’å°æ–‡å­—ã®å¤‰æ•°ã«å±•é–‹æ¸ˆã¿\n",
    "# ç›®çš„: å¾Œç¶š writer ã‚»ãƒ«ãŒå‚ç…§ã™ã‚‹å¤§æ–‡å­—å(BRAND_KEYWORDS, CERT_FULL_INFO_MAP, TOOLS_DICT)ã‚’å¿…ãšå®šç¾©ã™ã‚‹\n",
    "\n",
    "# 1) brand_keywords -> BRAND_KEYWORDS\n",
    "if 'brand_keywords' in globals():\n",
    "    if isinstance(brand_keywords, (set, tuple)):\n",
    "        BRAND_KEYWORDS = list(brand_keywords)\n",
    "    elif isinstance(brand_keywords, list):\n",
    "        BRAND_KEYWORDS = brand_keywords\n",
    "    else:\n",
    "        try:\n",
    "            BRAND_KEYWORDS = list(brand_keywords)\n",
    "        except Exception:\n",
    "            BRAND_KEYWORDS = []\n",
    "            print(\"[04-3][WARN] brand_keywords ã®åž‹ãŒæƒ³å®šå¤–ã ã£ãŸãŸã‚ç©ºã§ä»£æ›¿ã€‚\")\n",
    "else:\n",
    "    BRAND_KEYWORDS = handoff.get('brand_keywords', [])\n",
    "\n",
    "# 2) cert_full_info_map -> CERT_FULL_INFO_MAP\n",
    "if 'cert_full_info_map' in globals():\n",
    "    if not isinstance(cert_full_info_map, dict):\n",
    "        try:\n",
    "            CERT_FULL_INFO_MAP = dict(cert_full_info_map)\n",
    "        except Exception:\n",
    "            CERT_FULL_INFO_MAP = {}\n",
    "            print(\"[04-3][WARN] cert_full_info_map ã®åž‹ãŒæƒ³å®šå¤–ã ã£ãŸãŸã‚ç©ºã§ä»£æ›¿ã€‚\")\n",
    "    else:\n",
    "        CERT_FULL_INFO_MAP = cert_full_info_map\n",
    "else:\n",
    "    CERT_FULL_INFO_MAP = handoff.get('cert_full_info_map', {})\n",
    "\n",
    "# 3) TOOLS_DICT ã®åˆæœŸåŒ–ï¼ˆ04-2 handoff ã«ã¯é€šå¸¸å«ã¾ã‚Œãªã„ï¼‰\n",
    "if 'TOOLS_DICT' not in globals():\n",
    "    TOOLS_DICT = handoff.get('tools', {})\n",
    "    if not isinstance(TOOLS_DICT, dict):\n",
    "        TOOLS_DICT = {}\n",
    "\n",
    "print(f\"[04-3][CHECK] aliases -> BRAND_KEYWORDS={len(BRAND_KEYWORDS)}, \"\n",
    "      f\"CERT_FULL_INFO_MAP={len(CERT_FULL_INFO_MAP)}, TOOLS_DICT={len(TOOLS_DICT)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f761a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DB connected\n",
      "  phishtank: 54,362ä»¶ (Top5: .com(23196), .dev(5865), .ly(3191), .me(2732), .de(2705))\n",
      "  jpcert: 116,647ä»¶ (Top5: .com(35795), .cn(34513), .top(6972), .org(5610), .shop(4572))\n",
      "  certificates: 196,392ä»¶ (Top5: .org(102083), .com(30237), .cn(27498), .top(7450), .xyz(6099))\n",
      "âœ… TLDåˆ†æžå®Œäº†: dangerous=24, legitimate=8, neutral=1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === ã‚»ãƒ«5: TLDåˆ†å¸ƒåˆ†æžï¼ˆä¸­æ ¸ä¿æŒ + Configï¼‰ ===\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np, random\n",
    "\n",
    "ta = cfg.get('tld_analysis', {})\n",
    "use_all_data   = bool(ta.get('use_all_data', True))\n",
    "balance_data   = bool(ta.get('balance_data', True))\n",
    "sample_size    = ta.get('sample_size', None)\n",
    "min_sample     = int(ta.get('min_sample_size', 10))\n",
    "max_dangerous  = int(ta.get('max_dangerous_tlds', 30))\n",
    "max_legitimate = int(ta.get('max_legitimate_tlds', 30))\n",
    "pct_thresholds = list(ta.get('percentile_thresholds', [50,75,90]))\n",
    "fn_weight      = float(ta.get('fn_weight', 1.5))\n",
    "random.seed(cfg.get('system',{}).get('seed',42))\n",
    "\n",
    "def extract_tld(domain):\n",
    "    if not domain: return None\n",
    "    if '://' in domain:\n",
    "        try: domain = urlparse(domain).netloc\n",
    "        except Exception: pass\n",
    "    domain = domain.split(':')[0]\n",
    "    parts = domain.split('.')\n",
    "    if len(parts) >= 2:\n",
    "        if len(parts) >= 3 and parts[-2] in ['co','ac','or','ne','go']:\n",
    "            return f'.{parts[-2]}.{parts[-1]}'\n",
    "        return f'.{parts[-1]}'\n",
    "    return None\n",
    "\n",
    "conn = psycopg2.connect(dbname=DB_CONFIG['dbname'], user=DB_CONFIG['user'], password=DB_CONFIG['password'],\n",
    "                        host=DB_CONFIG['host'], port=DB_CONFIG['port'],\n",
    "                        connect_timeout=int(DB_CONFIG.get('connect_timeout',30)))\n",
    "cur = conn.cursor(cursor_factory=RealDictCursor)\n",
    "print(\"âœ… DB connected\")\n",
    "\n",
    "phishing_queries = {\n",
    "    'phishtank': \"SELECT cert_domain as domain FROM phishtank_entries WHERE cert_status='SUCCESS' AND cert_data IS NOT NULL AND cert_domain IS NOT NULL\",\n",
    "    'jpcert':    \"SELECT domain FROM jpcert_phishing_urls WHERE status='SUCCESS' AND cert_data IS NOT NULL AND domain IS NOT NULL\",\n",
    "    'certificates': \"SELECT domain FROM certificates WHERE status='SUCCESS' AND cert_data IS NOT NULL AND domain IS NOT NULL\"\n",
    "}\n",
    "ph_dom = []; ph_tlds = Counter()\n",
    "for name, q in phishing_queries.items():\n",
    "    if not use_all_data and sample_size: q += f\" LIMIT {int(sample_size)}\"\n",
    "    cur.execute(q); rows = cur.fetchall()\n",
    "    st = Counter()\n",
    "    for r in rows:\n",
    "        d = r['domain']\n",
    "        if d:\n",
    "            ph_dom.append(d)\n",
    "            t = extract_tld(d)\n",
    "            if t:\n",
    "                st[t]+=1; ph_tlds[t]+=1\n",
    "    print(f\"  {name}: {len(rows):,}ä»¶ (Top5: {', '.join([f'{t}({c})' for t,c in st.most_common(5)])})\")\n",
    "\n",
    "q = \"SELECT domain FROM trusted_certificates WHERE status='SUCCESS' AND cert_data IS NOT NULL AND domain IS NOT NULL\"\n",
    "if not use_all_data and sample_size: q += f\" LIMIT {int(sample_size)}\"\n",
    "cur.execute(q); rows = cur.fetchall()\n",
    "tr_dom = []; tr_tlds = Counter()\n",
    "for r in rows:\n",
    "    d = r['domain']\n",
    "    if d:\n",
    "        tr_dom.append(d)\n",
    "        t = extract_tld(d)\n",
    "        if t: tr_tlds[t]+=1\n",
    "\n",
    "if balance_data:\n",
    "    minc = min(len(set(ph_dom)), len(set(tr_dom)))\n",
    "    ph_u = list(set(ph_dom)); tr_u = list(set(tr_dom))\n",
    "    random.seed(cfg.get('system',{}).get('seed',42))\n",
    "    ph_bal = random.sample(ph_u, minc) if len(ph_u)>minc else ph_u\n",
    "    tr_bal = random.sample(tr_u, minc) if len(tr_u)>minc else tr_u\n",
    "else:\n",
    "    ph_bal, tr_bal = ph_dom, tr_dom\n",
    "\n",
    "ph_bal_t = Counter(extract_tld(d) for d in ph_bal if extract_tld(d))\n",
    "tr_bal_t = Counter(extract_tld(d) for d in tr_bal if extract_tld(d))\n",
    "\n",
    "from collections import Counter as _Ctr\n",
    "fn_tlds = _Ctr()\n",
    "if hasattr(false_negatives_df,'columns') and 'domain' in false_negatives_df.columns:\n",
    "    for d in false_negatives_df['domain']:\n",
    "        t = extract_tld(d)\n",
    "        if t: fn_tlds[t]+=1\n",
    "\n",
    "dangerous_ratio = {}\n",
    "for t in ph_bal_t:\n",
    "    ph = ph_bal_t[t]; tr = tr_bal_t.get(t,0)\n",
    "    if ph >= min_sample:\n",
    "        ratio = float('inf') if tr==0 else ph/(tr+1)\n",
    "        dangerous_ratio[t] = {'ratio':ratio,'phishing_count':ph,'trusted_count':tr,\n",
    "                              'phishing_pct': ph/max(len(ph_bal),1)*100}\n",
    "\n",
    "ratios = [s['ratio'] for s in dangerous_ratio.values() if s['ratio']!=float('inf')]\n",
    "if ratios:\n",
    "    pcts = np.percentile(ratios, pct_thresholds).tolist()\n",
    "else:\n",
    "    pcts = [10,10,10]\n",
    "p50,p75,p90 = (pcts+[10,10,10])[:3]\n",
    "\n",
    "DANGEROUS_TLDS = []\n",
    "for t,s in sorted(dangerous_ratio.items(), key=lambda x:x[1]['ratio'], reverse=True):\n",
    "    if ((s['ratio']==float('inf') and s['phishing_count']>=10) or\n",
    "        (s['ratio']>=p90 and s['phishing_pct']>=0.1) or\n",
    "        (s['ratio']>=10 and s['phishing_pct']>=1.0)):\n",
    "        DANGEROUS_TLDS.append(t)\n",
    "        if len(DANGEROUS_TLDS)>=max_dangerous: break\n",
    "\n",
    "LEGITIMATE_TLDS = []\n",
    "for t,c in tr_bal_t.most_common(max_legitimate*2):\n",
    "    tr_pct = c/max(len(tr_bal),1)*100\n",
    "    if tr_pct>=1.0 and c>=1000:\n",
    "        ph = ph_bal_t.get(t,0)\n",
    "        ratio = ph/c if c>0 else 0.0\n",
    "        if ratio<0.5:\n",
    "            LEGITIMATE_TLDS.append(t)\n",
    "    if len(LEGITIMATE_TLDS)>=max_legitimate: break\n",
    "\n",
    "if len(fn_tlds):\n",
    "    for t,c in fn_tlds.most_common():\n",
    "        fn_pct = c/max(len(false_negatives_df),1)*100\n",
    "        if fn_pct>=1.0 and t not in DANGEROUS_TLDS and t not in LEGITIMATE_TLDS:\n",
    "            if t in dangerous_ratio and dangerous_ratio[t]['ratio']>=5:\n",
    "                DANGEROUS_TLDS.append(t)\n",
    "\n",
    "NEUTRAL_TLDS = []\n",
    "for t in ['.com','.org','.net','.info','.biz']:\n",
    "    if t not in DANGEROUS_TLDS and t not in LEGITIMATE_TLDS:\n",
    "        if t in ph_bal_t and t in tr_bal_t:\n",
    "            ph_pct = ph_bal_t[t]/max(len(ph_bal),1)*100\n",
    "            tr_pct = tr_bal_t[t]/max(len(tr_bal),1)*100\n",
    "            if ph_pct>=0.5 or tr_pct>=0.5:\n",
    "                NEUTRAL_TLDS.append(t)\n",
    "\n",
    "globals().update({'DANGEROUS_TLDS':DANGEROUS_TLDS,'LEGITIMATE_TLDS':LEGITIMATE_TLDS,'NEUTRAL_TLDS':NEUTRAL_TLDS,\n",
    "                  'phishing_tld_stats':ph_bal_t,'trusted_tld_stats':tr_bal_t})\n",
    "cur.close(); conn.close()\n",
    "print(f\"âœ… TLDåˆ†æžå®Œäº†: dangerous={len(DANGEROUS_TLDS)}, legitimate={len(LEGITIMATE_TLDS)}, neutral={len(NEUTRAL_TLDS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69b19c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04-3] Skipped legacy part3.pkl write (by design).\n",
      "[04-3] Saved JSON: artifacts/2026-01-03_222059/results/tld_statistics.json\n"
     ]
    }
   ],
   "source": [
    "# === ã‚»ãƒ«6: Handoffä¿å­˜ï¼ˆJSONã®ã¿ï¼›ä¸»PKLä¿å­˜ã¯ FINAL v3 ã‚»ãƒ«ã§å®Ÿæ–½ï¼‰ ===\n",
    "import json, time\n",
    "from pathlib import Path\n",
    "\n",
    "# Note: legacy write to 03_ai_agent_analysis_part3.pkl is intentionally disabled to avoid cross-module overwrite.\n",
    "# The single-file handoff PKL is saved in the FINAL v3 cell as \"04-3_llm_tools_setup_with_tools.pkl\".\n",
    "\n",
    "Path(RESULTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "with open(Path(RESULTS_DIR)/\"tld_statistics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({'ts': time.time(),\n",
    "               'dangerous_tlds': list(DANGEROUS_TLDS),\n",
    "               'legitimate_tlds': list(LEGITIMATE_TLDS),\n",
    "               'neutral_tlds': list(NEUTRAL_TLDS)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[04-3] Skipped legacy part3.pkl write (by design).\")\n",
    "print(f\"[04-3] Saved JSON: {Path(RESULTS_DIR)/'tld_statistics.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac0fc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Controller API ready: explanation_quality() / tld_risk_analysis()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === ã‚»ãƒ«7: Controller APIé–¢æ•° ===\n",
    "from typing import Tuple, Dict, Any\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "def explanation_quality(session_id: str, cfg: Dict[str, Any]) -> Tuple[str, Dict[str, str]]:\n",
    "    try:\n",
    "        if not isinstance(cfg, dict):\n",
    "            return \"INVALID_INPUT\", {\"error\":\"cfgã¯è¾žæ›¸åž‹ã§æŒ‡å®šã—ã¦ãã ã•ã„\"}\n",
    "        _ = load_configuration(cfg_override=cfg)\n",
    "        if not _.get('tld_analysis',{}).get('enabled', False):\n",
    "            return \"INVALID_INPUT\", {\"error\":\"tld_analysis.enabled ãŒç„¡åŠ¹\"}\n",
    "        if not Path(HANDOFF_DIR, \"03_ai_agent_analysis_part2.pkl\").exists():\n",
    "            return \"NOT_FOUND\", {\"error\":\"Part2 handoff ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\"}\n",
    "        paths = {\n",
    "            \"dangerous_tlds\": str(Path(RESULTS_DIR)/\"tld_statistics.json\"),\n",
    "            \"legitimate_tlds\": str(Path(RESULTS_DIR)/\"tld_statistics.json\"),\n",
    "            \"neutral_tlds\": str(Path(RESULTS_DIR)/\"tld_statistics.json\"),\n",
    "            \"tld_statistics\": str(Path(RESULTS_DIR)/\"tld_statistics.json\"),\n",
    "            \"handoff\": str(Path(HANDOFF_DIR)/\"03_ai_agent_analysis_part3.pkl\"),\n",
    "            \"logs\": LOGS_DIR\n",
    "        }\n",
    "        if len(DANGEROUS_TLDS)==0 and len(LEGITIMATE_TLDS)==0:\n",
    "            return \"NOT_FOUND\", {\"error\":\"TLDãƒªã‚¹ãƒˆãŒç©ºã§ã™\", **paths}\n",
    "        return \"OK\", paths\n",
    "    except Exception as e:\n",
    "        return \"ERROR\", {\"error\": str(e), \"trace\": traceback.format_exc()}\n",
    "\n",
    "tld_risk_analysis = explanation_quality\n",
    "print(\"âœ… Controller API ready: explanation_quality() / tld_risk_analysis()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e1a8f3-98d4-4469-b1a6-ee203bcbb9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab1ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tool registered: brand_impersonation_check\n"
     ]
    }
   ],
   "source": [
    "# === P1-2: brand_impersonation_check (compat wrapper; Structured Output) ===\n",
    "from typing import Dict, Any, Iterable, List, Tuple, Optional\n",
    "import re\n",
    "\n",
    "_P1_2_RISK_WORDS = {'secure','login','verify','update','account','wallet','support','confirm','bank','invoice','payment'}\n",
    "\n",
    "def _p1_2_risk_level_from_confidence(conf: float) -> str:\n",
    "    if conf >= 0.85: return \"critical\"\n",
    "    if conf >= 0.70: return \"high\"\n",
    "    if conf >= 0.50: return \"medium-high\"\n",
    "    if conf >= 0.30: return \"medium\"\n",
    "    return \"low\"\n",
    "\n",
    "def _p1_2_iter_keywords(bk: Any):\n",
    "    if bk is None: return []\n",
    "    if isinstance(bk, dict):\n",
    "        for k, v in bk.items():\n",
    "            if isinstance(k, str) and k: yield k.lower()\n",
    "            if isinstance(v, (list, tuple, set)):\n",
    "                for x in v:\n",
    "                    if isinstance(x, str) and x: yield x.lower()\n",
    "            elif isinstance(v, str) and v: yield v.lower()\n",
    "        return\n",
    "    if isinstance(bk, (list, tuple, set)):\n",
    "        for x in bk:\n",
    "            if isinstance(x, str) and x: yield x.lower()\n",
    "        return\n",
    "    try:\n",
    "        for x in list(bk):\n",
    "            if isinstance(x, str) and x: yield x.lower()\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _p1_2_tokenize_domain(d: str) -> List[str]:\n",
    "    try: d = d.encode('idna').decode('ascii')\n",
    "    except Exception: pass\n",
    "    return [t for t in re.split(r'[^a-z0-9]+', (d or '').lower()) if t]\n",
    "\n",
    "def _p1_2_is_one_edit(a: str, b: str) -> bool:\n",
    "    la, lb = len(a), len(b)\n",
    "    if abs(la - lb) > 1: return False\n",
    "    if la > lb: a, b, la, lb = b, a, lb, la\n",
    "    i = j = edits = 0\n",
    "    while i < la and j < lb:\n",
    "        if a[i] == b[j]: i += 1; j += 1\n",
    "        else:\n",
    "            edits += 1\n",
    "            if edits > 1: return False\n",
    "            if la == lb: i += 1; j += 1\n",
    "            else: j += 1\n",
    "    edits += (lb - j) + (la - i)\n",
    "    return edits <= 1\n",
    "\n",
    "def brand_impersonation_check(domain: str, ml_probability: float = 0.0, cfg: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    d = (domain or \"\").strip().lower()\n",
    "    try: p = float(ml_probability or 0.0)\n",
    "    except Exception: p = 0.0\n",
    "\n",
    "    g = globals()\n",
    "    keywords_src = g.get(\"BRAND_KEYWORDS\", g.get(\"brand_keywords\", []))\n",
    "    brands = sorted(set(_p1_2_iter_keywords(keywords_src)))\n",
    "    tokens = _p1_2_tokenize_domain(d)\n",
    "\n",
    "    substr_hits = [b for b in brands if b and b in d]\n",
    "    token_hits  = [b for b in brands if b in tokens]\n",
    "\n",
    "    typo_hits: List[Tuple[str, str]] = []\n",
    "    cand = [t for t in tokens if len(t) >= 3]\n",
    "    for b in brands:\n",
    "        if not b or len(b) < 4: continue\n",
    "        if b in substr_hits or b in token_hits: continue\n",
    "        for t in cand:\n",
    "            if _p1_2_is_one_edit(b, t):\n",
    "                typo_hits.append((b, t))\n",
    "                break\n",
    "\n",
    "    risk_kw = [kw for kw in _P1_2_RISK_WORDS if kw in d]\n",
    "\n",
    "    score = 0.0\n",
    "    if substr_hits or token_hits: score += 0.60\n",
    "    if typo_hits: score += 0.25\n",
    "    if risk_kw: score += 0.10 + 0.02 * min(len(risk_kw), 5)\n",
    "    score += 0.30 * p\n",
    "    score = max(0.0, min(score, 0.99))\n",
    "\n",
    "    return {\n",
    "        \"is_impersonation\": bool(substr_hits or token_hits or typo_hits),\n",
    "        \"confidence\": score,\n",
    "        \"risk_level\": _p1_2_risk_level_from_confidence(score),\n",
    "        \"detected_brands\": sorted(set(substr_hits + token_hits)),\n",
    "        \"risk_factors\": {\n",
    "            \"typosquatting_matches\": typo_hits,\n",
    "            \"risk_keywords\": risk_kw,\n",
    "            \"ml_probability\": p,\n",
    "            \"tokens\": tokens[:8],\n",
    "        },\n",
    "        \"reasons\": [\n",
    "            \"brand keyword present\" if (substr_hits or token_hits) else \"no direct brand keyword found\",\n",
    "            \"typosquatting candidate detected\" if typo_hits else \"no typo candidate\",\n",
    "            f\"ml_probability contribution={round(0.30*p,3)}\"\n",
    "        ],\n",
    "        \"success\": True,\n",
    "    }\n",
    "\n",
    "# Register into TOOLS_DICT for inventory\n",
    "try:\n",
    "    if \"TOOLS_DICT\" not in globals() or not isinstance(TOOLS_DICT, dict):\n",
    "        TOOLS_DICT = {}\n",
    "    TOOLS_DICT[\"brand_impersonation_check\"] = {\n",
    "        \"type\": \"tool\", \"version\": \"p1-2\",\n",
    "        \"inputs\": [\"domain\", \"ml_probability\"],\n",
    "        \"returns\": [\"is_impersonation\",\"confidence\",\"risk_level\",\"detected_brands\",\"risk_factors\",\"reasons\",\"success\"],\n",
    "        \"notes\": \"Local wrapper; uses BRAND_KEYWORDS when available; no external I/O.\"\n",
    "    }\n",
    "    print(\"âœ… tool registered: brand_impersonation_check\")\n",
    "except Exception as _e:\n",
    "    print(\"âš ï¸ tool registration failed:\", _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74098ffc-0def-48ef-9c1b-d0f0d85d45b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dabbd10",
   "metadata": {
    "name": "cell_98_sanity"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04-3][SANITY] present: ['brand_keywords', 'cert_full_info_map']\n",
      "[04-3][SANITY][WARN] missing (ensure these are produced upstream in this notebook): ['HIGH_RISK_WORDS', 'brand_domains_map', 'cert_anomaly_rules', 'stat_feature_defs']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 98: Sanity (keys expected to be produced by 04-3)\n",
    "_expect = [\"brand_keywords\",\"cert_full_info_map\",\"HIGH_RISK_WORDS\",\n",
    "           \"brand_domains_map\",\"cert_anomaly_rules\",\"stat_feature_defs\"]\n",
    "present = [k for k in _expect if k in globals()]\n",
    "missing = [k for k in _expect if k not in globals()]\n",
    "print(\"[04-3][SANITY] present:\", present)\n",
    "if missing:\n",
    "    print(\"[04-3][SANITY][WARN] missing (ensure these are produced upstream in this notebook):\", missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "748af39d",
   "metadata": {
    "name": "cell_99_save"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04-3][SAVE] Saved (cloudpickle) -> artifacts/2026-01-03_222059/handoff/04-3_llm_tools_setup.pkl\n",
      "[04-3][SAVE] manifest -> artifacts/2026-01-03_222059/handoff/04-3_manifest.json\n",
      "[04-3][SAVE] output size = 1023748 bytes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 99: Save handoff (robust cloudpickle-first)\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import cloudpickle as _cp\n",
    "except Exception:\n",
    "    _cp = None\n",
    "import pickle as _sp\n",
    "\n",
    "def _is_picklable(x):\n",
    "    try:\n",
    "        if _cp is not None:\n",
    "            _cp.dumps(x)\n",
    "        else:\n",
    "            _sp.dumps(x)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "payload = {}\n",
    "payload[\"cfg\"]   = globals().get(\"cfg\")\n",
    "payload[\"tools\"] = globals().get(\"tools\")\n",
    "payload[\"llm\"]   = globals().get(\"llm\")\n",
    "\n",
    "stats = {}\n",
    "maybe_names = [\n",
    "    \"brand_keywords\",\"cert_full_info_map\",\"HIGH_RISK_WORDS\",\n",
    "    \"brand_domains_map\",\"cert_anomaly_rules\",\"stat_feature_defs\",\n",
    "    \"tool_specs\",\"llm_params\",\"tokenizer_info\"\n",
    "]\n",
    "for name in maybe_names:\n",
    "    if name in globals():\n",
    "        stats[name] = globals()[name]\n",
    "\n",
    "if \"stats\" in globals() and isinstance(globals()[\"stats\"], dict):\n",
    "    for k, v in globals()[\"stats\"].items():\n",
    "        if k not in stats:\n",
    "            stats[k] = v\n",
    "\n",
    "payload[\"stats\"] = stats\n",
    "\n",
    "out_dir = Path(f\"artifacts/{RUN_ID}/handoff\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_pkl = out_dir / \"04-3_llm_tools_setup.pkl\"\n",
    "\n",
    "def _try_dump(obj, path, use_cloud=True):\n",
    "    if use_cloud and _cp is not None:\n",
    "        with open(path, \"wb\") as f:\n",
    "            _cp.dump(obj, f)\n",
    "        return \"cloudpickle\"\n",
    "    else:\n",
    "        with open(path, \"wb\") as f:\n",
    "            _sp.dump(obj, f)\n",
    "        return \"pickle\"\n",
    "\n",
    "ok = False\n",
    "try:\n",
    "    m = _try_dump(payload, out_pkl, use_cloud=True)\n",
    "    ok = True\n",
    "    print(f\"[04-3][SAVE] Saved ({m}) ->\", out_pkl)\n",
    "except Exception as e:\n",
    "    print(\"[04-3][WARN] cloudpickle save failed for whole payload:\", e)\n",
    "    # prune unpicklables\n",
    "    for k in list(payload.keys()):\n",
    "        try:\n",
    "            if _cp is not None:\n",
    "                _cp.dumps(payload[k])\n",
    "            else:\n",
    "                _sp.dumps(payload[k])\n",
    "        except Exception:\n",
    "            print(\"  - pruning top-level:\", k)\n",
    "            payload[k] = None\n",
    "    for k in list(stats.keys()):\n",
    "        try:\n",
    "            if _cp is not None:\n",
    "                _cp.dumps(stats[k])\n",
    "            else:\n",
    "                _sp.dumps(stats[k])\n",
    "        except Exception:\n",
    "            print(\"  - pruning stats:\", k)\n",
    "            stats[k] = None\n",
    "    try:\n",
    "        m = _try_dump(payload, out_pkl, use_cloud=(_cp is not None))\n",
    "        ok = True\n",
    "        print(f\"[04-3][SAVE] Saved (fallback:{m}) ->\", out_pkl)\n",
    "    except Exception as e2:\n",
    "        print(\"[04-3][FATAL] save failed:\", e2)\n",
    "\n",
    "# manifest\n",
    "man = {\"run_id\": RUN_ID, \"path\": str(out_pkl), \"top_level\": sorted(list(payload.keys())), \"stats_keys\": sorted(list(stats.keys()))}\n",
    "try:\n",
    "    with open(out_dir / \"04-3_manifest.json\", \"w\", encoding=\"utf-8\") as mf:\n",
    "        json.dump(man, mf, ensure_ascii=False, indent=2)\n",
    "    print(\"[04-3][SAVE] manifest ->\", out_dir / \"04-3_manifest.json\")\n",
    "except Exception as _:\n",
    "    pass\n",
    "\n",
    "if ok:\n",
    "    sz = out_pkl.stat().st_size if out_pkl.exists() else 0\n",
    "    print(f\"[04-3][SAVE] output size = {sz} bytes\")\n",
    "    assert sz > 0, \"handoff file is empty (0 bytes)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f31e03d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tool registered: certificate_analysis\n"
     ]
    }
   ],
   "source": [
    "# === P1-3: certificate_analysis (compat wrapper; Structured Output) ===\n",
    "# Behavior:\n",
    "#  - Uses CERT_FULL_INFO_MAP when available; otherwise degrades gracefully\n",
    "#  - Heuristics per paper (validity days, issuer/free CA, wildcard, SAN count, self-signed, org presence)\n",
    "#  - Returns fields consumed by certificate_analysis_node in 04-4\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "_FREE_CA_HINTS = (\"let's encrypt\",\"zerossl\",\"buypass\",\"trustasia\",\"gandi\",\"cloudflare\",\"google internet authority\",\"isrg\",\"actalis\")\n",
    "def _p1_3_risk_level(score: float) -> str:\n",
    "    if score >= 80: return \"critical\"\n",
    "    if score >= 60: return \"high\"\n",
    "    if score >= 45: return \"medium-high\"\n",
    "    if score >= 25: return \"medium\"\n",
    "    return \"low\"\n",
    "\n",
    "def _p1_3_norm_dt(s: Any) -> Optional[datetime]:\n",
    "    if not s: return None\n",
    "    if isinstance(s, datetime): return s\n",
    "    if isinstance(s, (int, float)): \n",
    "        try:\n",
    "            # seconds epoch\n",
    "            return datetime.utcfromtimestamp(float(s))\n",
    "        except Exception:\n",
    "            return None\n",
    "    # string\n",
    "    s = str(s).strip().replace(\"Z\", \"\")\n",
    "    for fmt in (\"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt)\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        # ISO 8601 fallback\n",
    "        return datetime.fromisoformat(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _p1_3_days_between(a: Any, b: Any) -> Optional[int]:\n",
    "    da, db = _p1_3_norm_dt(a), _p1_3_norm_dt(b)\n",
    "    if da and db:\n",
    "        try:\n",
    "            return abs((db - da).days)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _p1_3_pick_cert_record(cfmap: Any, domain: str) -> Dict[str, Any]:\n",
    "    if isinstance(cfmap, dict):\n",
    "        # direct\n",
    "        rec = cfmap.get(domain) or cfmap.get(domain.lower())\n",
    "        if rec: return rec if isinstance(rec, dict) else {}\n",
    "        # try www/no-www variants\n",
    "        if domain.startswith(\"www.\"):\n",
    "            rec = cfmap.get(domain[4:])\n",
    "            if rec: return rec if isinstance(rec, dict) else {}\n",
    "        else:\n",
    "            rec = cfmap.get(\"www.\"+domain)\n",
    "            if rec: return rec if isinstance(rec, dict) else {}\n",
    "        # fuzzy: try eTLD+1 fallback (simple split)\n",
    "        parts = domain.split(\".\")\n",
    "        if len(parts) >= 2:\n",
    "            etld1 = \".\".join(parts[-2:])\n",
    "            rec = cfmap.get(etld1)\n",
    "            if rec: return rec if isinstance(rec, dict) else {}\n",
    "    return {}\n",
    "\n",
    "def certificate_analysis(domain: str, ml_probability: float = 0.0, cfg: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    d = (domain or \"\").strip().lower()\n",
    "    try:\n",
    "        p = float(ml_probability or 0.0)\n",
    "    except Exception:\n",
    "        p = 0.0\n",
    "\n",
    "    g = globals()\n",
    "    cfmap = g.get(\"CERT_FULL_INFO_MAP\") or g.get(\"cert_full_info_map\") or {}\n",
    "    rec = _p1_3_pick_cert_record(cfmap, d)\n",
    "\n",
    "    # Extract fields (robustly)\n",
    "    issuer = (str(rec.get(\"issuer\") or rec.get(\"issuer_cn\") or rec.get(\"ca\") or \"\")).strip()\n",
    "    org    = (str(rec.get(\"organization\") or rec.get(\"org\") or \"\")).strip()\n",
    "    valid_days = rec.get(\"valid_days\")\n",
    "    if valid_days is None:\n",
    "        valid_days = _p1_3_days_between(rec.get(\"valid_from\") or rec.get(\"not_before\"),\n",
    "                                        rec.get(\"valid_to\")   or rec.get(\"not_after\"))\n",
    "    try:\n",
    "        valid_days = int(valid_days) if valid_days is not None else None\n",
    "    except Exception:\n",
    "        valid_days = None\n",
    "\n",
    "    san_count = rec.get(\"san_count\")\n",
    "    if san_count is None:\n",
    "        sans = rec.get(\"subject_alt_names\") or rec.get(\"dns_names\") or rec.get(\"san\") or []\n",
    "        try: san_count = len(sans)\n",
    "        except Exception: san_count = None\n",
    "\n",
    "    self_signed = bool(rec.get(\"self_signed\") or rec.get(\"is_self_signed\", False))\n",
    "    wildcard    = bool(rec.get(\"wildcard\") or rec.get(\"is_wildcard\", False))\n",
    "\n",
    "    issuer_lc = issuer.lower()\n",
    "    free_ca = any(h in issuer_lc for h in _FREE_CA_HINTS) if issuer else False\n",
    "    very_new = (valid_days is not None and valid_days <= 120)\n",
    "\n",
    "    # Score (bounded)\n",
    "    score = 0.0\n",
    "    if self_signed: score += 35\n",
    "    if not org:     score += 15\n",
    "    if free_ca:     score += 10\n",
    "    if very_new:    score += 10\n",
    "    if wildcard:    score += 5\n",
    "    if isinstance(san_count, int) and san_count <= 1: score += 5\n",
    "    score += 8.0 * float(p or 0.0)  # ML probability contributes modestly\n",
    "    score = max(0.0, min(score, 99.0))\n",
    "\n",
    "    level = _p1_3_risk_level(score)\n",
    "    conf  = round(min(1.0, max(0.0, (score / 100.0) + 0.10 * float(p or 0.0))), 3)\n",
    "\n",
    "    reasons: List[str] = []\n",
    "    if self_signed: reasons.append(\"self-signed certificate\")\n",
    "    if not org:     reasons.append(\"organization field missing (DV)\")\n",
    "    if free_ca:     reasons.append(\"issued by widely-used free CA\")\n",
    "    if very_new:    reasons.append(f\"short validity ({valid_days} days)\" if valid_days is not None else \"short validity period\")\n",
    "    if wildcard:    reasons.append(\"wildcard certificate detected\")\n",
    "    if isinstance(san_count, int) and san_count <= 1: reasons.append(\"SAN count is small\")\n",
    "    if not reasons:\n",
    "        reasons.append(\"no strong anomalies found in certificate metadata\")\n",
    "\n",
    "    result = {\n",
    "        \"confidence\": conf,\n",
    "        \"risk_level\": level,\n",
    "        \"risk_factors\": {\n",
    "            \"issuer\": issuer or None,\n",
    "            \"organization_present\": bool(org),\n",
    "            \"valid_days\": valid_days,\n",
    "            \"san_count\": san_count,\n",
    "            \"self_signed\": self_signed,\n",
    "            \"wildcard\": wildcard,\n",
    "            \"free_ca\": free_ca,\n",
    "            \"very_new_cert\": very_new,\n",
    "            \"ml_probability\": p,\n",
    "        },\n",
    "        \"reasons\": reasons,\n",
    "        \"success\": True,\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Register into TOOLS_DICT for inventory/handoff\n",
    "try:\n",
    "    if \"TOOLS_DICT\" not in globals() or not isinstance(TOOLS_DICT, dict):\n",
    "        TOOLS_DICT = {}\n",
    "    TOOLS_DICT[\"certificate_analysis\"] = {\n",
    "        \"type\": \"tool\", \"version\": \"p1-3\",\n",
    "        \"inputs\": [\"domain\", \"ml_probability\"],\n",
    "        \"returns\": [\"confidence\",\"risk_level\",\"risk_factors\",\"reasons\",\"success\"],\n",
    "        \"notes\": \"Local wrapper; uses CERT_FULL_INFO_MAP when available; no external I/O.\"\n",
    "    }\n",
    "    print(\"âœ… tool registered: certificate_analysis\")\n",
    "except Exception as _e:\n",
    "    print(\"âš ï¸ tool registration failed:\", _e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd9a1883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1-3 test result: {'confidence': 0.261, 'risk_level': 'low', 'risk_factors': {'issuer': None, 'organization_present': False, 'valid_days': None, 'san_count': 0, 'self_signed': False, 'wildcard': False, 'free_ca': False, 'very_new_cert': False, 'ml_probability': 0.34}, 'reasons': ['organization field missing (DV)', 'SAN count is small'], 'success': True}\n",
      "P1-3 smoketest: OK\n"
     ]
    }
   ],
   "source": [
    "# === P1-3 TEST (local, no external I/O) ===\n",
    "# Prepare a minimal cert map only if not provided upstream\n",
    "if not globals().get(\"CERT_FULL_INFO_MAP\"):\n",
    "    CERT_FULL_INFO_MAP = {\n",
    "        \"paypal-secure-login.info\": {\n",
    "            \"issuer\": \"Let's Encrypt\",\n",
    "            \"organization\": \"\",\n",
    "            \"valid_days\": 90,\n",
    "            \"san_count\": 1,\n",
    "            \"self_signed\": False,\n",
    "            \"wildcard\": False\n",
    "        }\n",
    "    }\n",
    "out = certificate_analysis(\"paypal-secure-login.info\", 0.34)\n",
    "print(\"P1-3 test result:\", out)\n",
    "assert isinstance(out, dict) and \"confidence\" in out and \"risk_level\" in out and \"risk_factors\" in out and \"success\" in out\n",
    "assert isinstance(out[\"risk_factors\"], dict) and \"free_ca\" in out[\"risk_factors\"]\n",
    "print(\"P1-3 smoketest: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b4c0be",
   "metadata": {
    "tags": [
     "PATCH",
     "HANDOFF_FINAL_V3",
     "SINGLE_FILE"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04-3] Handoff (single-file) saved: artifacts/2026-01-03_222059/handoff/04-3_llm_tools_setup_with_tools.pkl\n",
      "[04-3] Size (bytes): 1023756\n",
      "[04-3] Present top-level keys: ['cfg', 'stats'] | Missing: ['tools', 'llm']\n",
      "[04-3] Present stats keys: ['brand_keywords', 'cert_full_info_map'] | Missing: ['HIGH_RISK_WORDS']\n",
      "[04-3] Manifest: artifacts/2026-01-03_222059/handoff/04-3_manifest.json\n",
      "[04-3] paths module: _compat.paths | RUN_ID: 2026-01-03_222059\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Cell [FINAL v3]: Handoff save (single-file, robust, _compat.paths + runreg) ===\n",
    "from __future__ import annotations\n",
    "import os, io, json, time, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# Prefer cloudpickle; fallback to pickle\n",
    "try:\n",
    "    import cloudpickle as _cp\n",
    "except Exception:\n",
    "    _cp = None\n",
    "import pickle as _pickle\n",
    "\n",
    "def _import_paths():\n",
    "    \"\"\"Prefer `_compat.paths`, then `paths`, then file-based fallbacks.\"\"\"\n",
    "    try:\n",
    "        import _compat.paths as _paths\n",
    "        return _paths\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import paths as _paths\n",
    "        return _paths\n",
    "    except Exception:\n",
    "        pass\n",
    "    import importlib.util\n",
    "    candidates = [\n",
    "        Path(\"paths.py\"),\n",
    "        Path.cwd() / \"paths.py\",\n",
    "        Path(\"_compat/paths.py\"),\n",
    "        Path(\"./_compat/paths.py\"),\n",
    "        Path(\"../_compat/paths.py\"),\n",
    "        Path(\"compat/paths.py\"),\n",
    "        Path(\"./compat/paths.py\"),\n",
    "        Path(\"../compat/paths.py\"),\n",
    "        Path(\"/mnt/data/paths.py\"),\n",
    "    ]\n",
    "    env_hint = os.getenv(\"PATHS_PY\")\n",
    "    if env_hint:\n",
    "        candidates.insert(0, Path(env_hint))\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            if p.exists():\n",
    "                spec = importlib.util.spec_from_file_location(\"paths\", str(p))\n",
    "                mod = importlib.util.module_from_spec(spec)\n",
    "                spec.loader.exec_module(mod)  # type: ignore\n",
    "                return mod\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _make_paths_shim():\n",
    "    \"\"\"Fallback shim if paths.py is unavailable (AC#3: ç„¡ã‘ã‚Œã°è‡ªå‹•ä½œæˆ).\"\"\"\n",
    "    class _PathsShim:\n",
    "        # Decide RUN_ID with env > runreg.bootstrap() > timestamp\n",
    "        try:\n",
    "            _rid_env = os.environ.get(\"RUN_ID\")\n",
    "            if _rid_env:\n",
    "                RUN_ID = _rid_env\n",
    "            else:\n",
    "                try:\n",
    "                    import run_id_registry as runreg  # optional\n",
    "                    RUN_ID = runreg.bootstrap() or time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "                except Exception:\n",
    "                    RUN_ID = time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "        except Exception:\n",
    "            RUN_ID = time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "        ARTIFACTS_ROOT = Path(os.getenv(\"ARTIFACTS_ROOT\") or \"artifacts\")\n",
    "        compat_base_dirs = {\n",
    "            \"root\":    ARTIFACTS_ROOT / RUN_ID,\n",
    "            \"handoff\": ARTIFACTS_ROOT / RUN_ID / \"handoff\",\n",
    "            \"logs\":    ARTIFACTS_ROOT / RUN_ID / \"logs\",\n",
    "            \"reports\": ARTIFACTS_ROOT / RUN_ID / \"reports\",\n",
    "        }\n",
    "        @classmethod\n",
    "        def ensure_roots(cls):\n",
    "            for p in cls.compat_base_dirs.values():\n",
    "                p.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"[04-3] INFO: paths shim in use (paths.py not importable).\")\n",
    "    return _PathsShim\n",
    "\n",
    "# Resolve paths (prefer project module)\n",
    "paths = _import_paths()\n",
    "if paths is None:\n",
    "    paths = _make_paths_shim()\n",
    "\n",
    "# Derive RUN_ID (prefer module's value; otherwise env > runreg > timestamp)\n",
    "try:\n",
    "    RUN_ID = getattr(paths, \"RUN_ID\", None)\n",
    "except Exception:\n",
    "    RUN_ID = None\n",
    "if not RUN_ID:\n",
    "    RUN_ID = os.environ.get(\"RUN_ID\")\n",
    "if not RUN_ID:\n",
    "    try:\n",
    "        import run_id_registry as runreg\n",
    "        RUN_ID = runreg.bootstrap() or time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "    except Exception:\n",
    "        RUN_ID = time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "# Handoff dir (prefer module's compat paths)\n",
    "try:\n",
    "    handoff_dir = Path(getattr(paths, \"compat_base_dirs\", {}).get(\"handoff\"))\n",
    "except Exception:\n",
    "    handoff_dir = None\n",
    "if not handoff_dir:\n",
    "    ARTIFACTS_ROOT = Path(os.getenv(\"ARTIFACTS_ROOT\") or \"artifacts\")\n",
    "    handoff_dir = ARTIFACTS_ROOT / RUN_ID / \"handoff\"\n",
    "handoff_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PKL_MAIN      = handoff_dir / \"04-3_llm_tools_setup_with_tools.pkl\"\n",
    "PKL_WITHTOOLS = handoff_dir / \"04-3_llm_tools_setup.pkl\"  # legacy / secondary\n",
    "MANIFEST_JSON = handoff_dir / \"04-3_manifest.json\"\n",
    "\n",
    "# ===== Collect variables from previous cells (non-intrusive) =====\n",
    "def _find_first(names):\n",
    "    g = globals()\n",
    "    for n in names:\n",
    "        if n in g:\n",
    "            return g[n]\n",
    "    return None\n",
    "\n",
    "def _gather_stats():\n",
    "    st = {}\n",
    "    if \"stats\" in globals() and isinstance(globals()[\"stats\"], dict):\n",
    "        st.update(globals()[\"stats\"])\n",
    "    bk = _find_first([\"brand_keywords\",\"BRAND_KEYWORDS\"])\n",
    "    if bk is not None:\n",
    "        try: st[\"brand_keywords\"] = list(bk)\n",
    "        except Exception: st[\"brand_keywords\"] = bk\n",
    "    cfm = _find_first([\"cert_full_info_map\",\"CERT_FULL_INFO_MAP\"])\n",
    "    if cfm is not None:\n",
    "        st[\"cert_full_info_map\"] = cfm\n",
    "    hrw = _find_first([\"HIGH_RISK_WORDS\",\"high_risk_words\"])\n",
    "    if hrw is not None:\n",
    "        try: st[\"HIGH_RISK_WORDS\"] = list(hrw) if not isinstance(hrw, dict) else hrw\n",
    "        except Exception: st[\"HIGH_RISK_WORDS\"] = hrw\n",
    "    return st\n",
    "\n",
    "def _prune_tool_obj(obj):\n",
    "    try:\n",
    "        name = getattr(obj, \"name\", None) or getattr(obj, \"__name__\", None) or type(obj).__name__\n",
    "        mod  = getattr(obj, \"__module__\", None)\n",
    "        desc = getattr(obj, \"description\", None) or getattr(obj, \"__doc__\", None)\n",
    "        try:\n",
    "            import inspect; sig = str(inspect.signature(getattr(obj, \"__call__\", obj)))\n",
    "        except Exception:\n",
    "            sig = None\n",
    "        return {\"_kind\":\"tool\",\"name\":str(name),\"module\":str(mod) if mod else None,\n",
    "                \"description\": str(desc)[:400] if desc else None,\n",
    "                \"signature\": sig, \"repr\": repr(obj)[:400]}\n",
    "    except Exception as e:\n",
    "        return {\"_kind\":\"tool\",\"name\":type(obj).__name__,\"error\":str(e)}\n",
    "\n",
    "def _prune_tools(tools):\n",
    "    try:\n",
    "        if tools is None: return None\n",
    "        if isinstance(tools, dict):             return {str(k): _prune_tools(v) for k,v in tools.items()}\n",
    "        if isinstance(tools, (list,tuple,set)): return [_prune_tools(t) for t in tools]\n",
    "        if callable(tools) or isinstance(tools, object): return _prune_tool_obj(tools)\n",
    "    except Exception as e:\n",
    "        return {\"_kind\":\"tools\",\"error\":str(e)}\n",
    "    return tools\n",
    "\n",
    "def _prune_llm(llm_obj, cfg_llm):\n",
    "    base = {}\n",
    "    if isinstance(cfg_llm, dict) and cfg_llm:\n",
    "        base.update({k: cfg_llm.get(k) for k in [\"provider\",\"base_url\",\"model\"]})\n",
    "    if llm_obj is not None:\n",
    "        base.update({\"_type\": type(llm_obj).__name__, \"repr\": repr(llm_obj)[:400]})\n",
    "    return base or None\n",
    "\n",
    "def _try_dump(obj, path, use_cloudpickle=True):\n",
    "    tmp = Path(str(path) + \".tmp\")\n",
    "    try:\n",
    "        with open(tmp, \"wb\") as f:\n",
    "            if use_cloudpickle and _cp is not None:\n",
    "                _cp.dump(obj, f)\n",
    "            else:\n",
    "                _pickle.dump(obj, f, protocol=_pickle.HIGHEST_PROTOCOL)\n",
    "            f.flush(); os.fsync(f.fileno())\n",
    "        size = tmp.stat().st_size\n",
    "        if size <= 0: raise IOError(\"dump produced 0 bytes\")\n",
    "        tmp.replace(path)\n",
    "        return True, size, None\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            if tmp.exists(): tmp.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False, 0, e\n",
    "\n",
    "# ===== Build payload (no changes to upstream logic) =====\n",
    "_cfg = _find_first([\"cfg\"]) or {}\n",
    "if not _cfg:\n",
    "    try:\n",
    "        with open(\"/mnt/data/config.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "            _cfg = json.load(f)\n",
    "    except Exception:\n",
    "        _cfg = {}\n",
    "\n",
    "try: _cfg_llm = dict((_cfg or {}).get(\"llm\") or {})\n",
    "except Exception: _cfg_llm = {}\n",
    "\n",
    "_tools = _find_first([\"tools\",\"TOOLS\",\"agent_tools\",\"toolset\",\"available_tools\"])\n",
    "_llm   = _find_first([\"llm\",\"LLM\",\"client\",\"openai_client\"])\n",
    "_stats = _gather_stats()\n",
    "\n",
    "payload = {\"cfg\": _cfg, \"tools\": _tools, \"llm\": _llm, \"stats\": _stats}\n",
    "for key in [\"brand_keywords\",\"cert_full_info_map\",\"HIGH_RISK_WORDS\"]:\n",
    "    if key in _stats and key not in payload:\n",
    "        payload[key] = _stats[key]\n",
    "\n",
    "# Try save: cloudpickle â†’ prune+cloudpickle â†’ prune+pickle\n",
    "ok, size, err = _try_dump(payload, PKL_MAIN, use_cloudpickle=True)\n",
    "if not ok:\n",
    "    print(f\"[04-3] WARN: cloudpickle dump failed: {err}\")\n",
    "    pruned = dict(payload)\n",
    "    pruned[\"tools\"] = _prune_tools(_tools)\n",
    "    pruned[\"llm\"]   = _prune_llm(_llm, _cfg_llm)\n",
    "    ok2, size2, err2 = _try_dump(pruned, PKL_MAIN, use_cloudpickle=True)\n",
    "    if not ok2:\n",
    "        print(f\"[04-3] WARN: cloudpickle dump (pruned) failed: {err2}\")\n",
    "        ok3, size3, err3 = _try_dump(pruned, PKL_MAIN, use_cloudpickle=False)\n",
    "        if not ok3:\n",
    "            raise RuntimeError(f\"[04-3] Failed to persist handoff pkl even after pruning: {err3}\")\n",
    "        ok, size, err = ok3, size3, None\n",
    "    else:\n",
    "        ok, size, err = ok2, size2, None\n",
    "\n",
    "# Remove legacy\n",
    "try:\n",
    "    if PKL_WITHTOOLS.exists():\n",
    "        PKL_WITHTOOLS.unlink()\n",
    "except Exception as e:\n",
    "    print(f\"[04-3] NOTE: Failed to remove legacy: {PKL_WITHTOOLS} ({e})\")\n",
    "\n",
    "# Sanity + Manifest\n",
    "required_top   = [\"cfg\",\"tools\",\"llm\",\"stats\"]\n",
    "required_stats = [\"brand_keywords\",\"cert_full_info_map\",\"HIGH_RISK_WORDS\"]\n",
    "present_top    = [k for k in required_top if k in payload and payload[k] is not None]\n",
    "missing_top    = [k for k in required_top if k not in payload or payload[k] is None]\n",
    "_stats_keys    = list((_stats or {}).keys()) if isinstance(_stats, dict) else []\n",
    "present_stats  = [k for k in required_stats if k in (_stats or {})]\n",
    "missing_stats  = [k for k in required_stats if k not in (_stats or {})]\n",
    "\n",
    "manifest = {\n",
    "    \"version\": \"04-3_handoff_v3\",\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"pkl_path\": str(PKL_MAIN),\n",
    "    \"byte_size\": int(size),\n",
    "    \"top_level_keys\": sorted(list(payload.keys())),\n",
    "    \"stats_keys\": sorted(_stats_keys),\n",
    "    \"required_top\": required_top,\n",
    "    \"required_stats\": required_stats,\n",
    "    \"present_top\": present_top,\n",
    "    \"missing_top\": missing_top,\n",
    "    \"present_stats\": present_stats,\n",
    "    \"missing_stats\": missing_stats,\n",
    "}\n",
    "with open(MANIFEST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[04-3] Handoff (single-file) saved:\", PKL_MAIN)\n",
    "print(\"[04-3] Size (bytes):\", size)\n",
    "print(\"[04-3] Present top-level keys:\", present_top, \"| Missing:\", missing_top)\n",
    "print(\"[04-3] Present stats keys:\", present_stats, \"| Missing:\", missing_stats)\n",
    "print(\"[04-3] Manifest:\", MANIFEST_JSON)\n",
    "print(\"[04-3] paths module:\", getattr(paths, \"__name__\", type(paths).__name__), \"| RUN_ID:\", RUN_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d96c6d9c",
   "metadata": {
    "tags": [
     "nextstep-minfix"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04-3][FIX] safe_cfg_dict ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚\n",
      "[04-3][FIX] wrote handoff: artifacts/2026-01-03_222059/handoff/04-3_llm_tools_setup_with_tools.pkl | size = 976815\n"
     ]
    }
   ],
   "source": [
    "# === 04-3 â€” æœ€çµ‚ã‚»ãƒ«: handoff ä¸»ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆä¿å­˜ï¼ˆã‚¬ãƒ¼ãƒ‰ä»˜ãï¼‰ ===\n",
    "# å…¥åŠ›: safe_cfg_dict?, BRAND_KEYWORDS, CERT_FULL_INFO_MAP, TOOLS_DICT, LLM_CONN_INFO?, ASYNC_HINTS?\n",
    "# å‡ºåŠ›: artifacts/{RUN_ID}/handoff/04-3_llm_tools_setup_with_tools.pkl (+ .meta.json)\n",
    "import os, json, pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from _compat.paths import compat_base_dirs\n",
    "\n",
    "# cfg ã®ç¢ºä¿ï¼ˆCell1æƒ³å®šã€ãªã‘ã‚Œã° config.json ã‹ã‚‰ï¼‰\n",
    "if 'cfg' not in globals():\n",
    "    try:\n",
    "        cfg = json.loads(open(\"config.json\",\"r\").read())\n",
    "        print(\"[04-3][FIX] cfg ã‚’ config.json ã‹ã‚‰è£œå®Œã—ã¾ã—ãŸã€‚\")\n",
    "    except Exception:\n",
    "        cfg = {}\n",
    "\n",
    "# safe_cfg_dict ã®ã‚¬ãƒ¼ãƒ‰\n",
    "if 'safe_cfg_dict' not in globals():\n",
    "    safe_cfg_dict = {\n",
    "        \"run_id\": os.environ.get(\"RUN_ID\"),\n",
    "        \"paths\": compat_base_dirs,\n",
    "        \"llm\": {k: cfg.get(\"llm\", {}).get(k) for k in [\"provider\",\"base_url\",\"model\"]} if isinstance(cfg, dict) else {},\n",
    "        \"brand_keywords_cfg\": (cfg.get(\"brand_keywords\", {}) if isinstance(cfg, dict) else {}),\n",
    "    }\n",
    "    print(\"[04-3][FIX] safe_cfg_dict ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "# æœ€ä½Žé™ã®è£œå®Œï¼ˆä¸ŠæµãŒæœªå®šç¾©ãªã‚‰ä½œã‚‹ãŒã€æ—¢å­˜ãŒã‚ã‚Œã°å°Šé‡ï¼‰\n",
    "if 'LLM_CONN_INFO' not in globals():\n",
    "    LLM_CONN_INFO = {\n",
    "        \"provider\": cfg.get(\"llm\", {}).get(\"provider\") if isinstance(cfg, dict) else None,\n",
    "        \"base_url\": cfg.get(\"llm\", {}).get(\"base_url\") if isinstance(cfg, dict) else None,\n",
    "        \"model\":    cfg.get(\"llm\", {}).get(\"model\")    if isinstance(cfg, dict) else None,\n",
    "        \"api_key_ref\": None,\n",
    "    }\n",
    "if 'ASYNC_HINTS' not in globals():\n",
    "    ASYNC_HINTS = {}\n",
    "\n",
    "REQUIRED_KEYS = [\"cfg\",\"brand_keywords\",\"cert_full_info_map\",\"tools\",\"llm\",\"async_client\"]\n",
    "\n",
    "handoff_dir = Path(compat_base_dirs[\"handoff\"])\n",
    "handoff_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "payload = {\n",
    "    \"cfg\": safe_cfg_dict,\n",
    "    \"brand_keywords\": BRAND_KEYWORDS,          # ä¸Šæµã‚»ãƒ«ãŒç”Ÿæˆã™ã‚‹ã“ã¨ã‚’å‰æ\n",
    "    \"cert_full_info_map\": CERT_FULL_INFO_MAP,  # åŒä¸Š\n",
    "    \"tools\": TOOLS_DICT,                       # åŒä¸Š\n",
    "    \"llm\": LLM_CONN_INFO,\n",
    "    \"async_client\": ASYNC_HINTS,\n",
    "}\n",
    "\n",
    "out_pkl = handoff_dir / \"04-3_llm_tools_setup_with_tools.pkl\"\n",
    "with out_pkl.open(\"wb\") as f:\n",
    "    pickle.dump(payload, f)\n",
    "\n",
    "meta = {\n",
    "    \"run_id\": os.environ.get(\"RUN_ID\"),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"file\": str(out_pkl), \"size\": out_pkl.stat().st_size,\n",
    "    \"required_keys\": REQUIRED_KEYS,\n",
    "    \"types\": {k: type(v).__name__ for k,v in payload.items()},\n",
    "}\n",
    "(out_pkl.with_suffix(\".meta.json\")).write_text(json.dumps(meta, ensure_ascii=False, indent=2))\n",
    "print(\"[04-3][FIX] wrote handoff:\", out_pkl, \"| size =\", out_pkl.stat().st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47881b-7aba-454e-aecd-9298d7d5ca0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb2e3584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1-2 test result: {'is_impersonation': True, 'confidence': 0.842, 'risk_level': 'high', 'detected_brands': ['paypal'], 'risk_factors': {'typosquatting_matches': [], 'risk_keywords': ['login', 'secure'], 'ml_probability': 0.34, 'tokens': ['paypal', 'secure', 'login', 'info']}, 'reasons': ['brand keyword present', 'no typo candidate', 'ml_probability contribution=0.102'], 'success': True}\n",
      "P1-2 smoketest: OK\n"
     ]
    }
   ],
   "source": [
    "# === P1-2 TEST (local, no external I/O) ===\n",
    "if \"BRAND_KEYWORDS\" not in globals():\n",
    "    BRAND_KEYWORDS = [\"paypal\",\"mercari\",\"ledger\"]\n",
    "res = brand_impersonation_check(\"paypal-secure-login.info\", 0.34)\n",
    "print(\"P1-2 test result:\", res)\n",
    "assert isinstance(res, dict) and \"confidence\" in res and \"risk_level\" in res and \"success\" in res\n",
    "if \"paypal\" in [k.lower() for k in globals().get(\"BRAND_KEYWORDS\", [])]:\n",
    "    assert res.get(\"is_impersonation\") is True\n",
    "print(\"P1-2 smoketest: OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5413f432",
   "metadata": {
    "tags": [
     "C-01",
     "restore",
     "initialize_llm_client"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C-01] initialize_llm_client is ready.\n"
     ]
    }
   ],
   "source": [
    "# === C-01 initialize_llm_client (04-3) ===\n",
    "# NOTE: This cell was appended by the automation to restore missing functionality\n",
    "# from 03_ai_agent_analysis.ipynb while keeping minimum diffs.\n",
    "# - Reads /mnt/data/config.json (llm.provider/base_url/model)\n",
    "# - Creates a lightweight \"client stub\" (OpenAI-compatible) without doing network I/O\n",
    "# - Ensures artifacts/{RUN_ID}/handoff/04-3_llm_tools_setup_with_tools.pkl (+ .meta.json)\n",
    "#   exist and include required keys used by downstream notebooks (04-4, 04-5).\n",
    "import os, json, pickle, datetime as _dt\n",
    "\n",
    "def _load_config(path=\"/mnt/data/config.json\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def initialize_llm_client(cfg=None, run_id=None):\n",
    "    \"\"\"\n",
    "    Initialize LLM client with proper error handling (vLLM / OpenAI-compatible).\n",
    "\n",
    "    Inputs:\n",
    "        cfg: dict (optional). If None, load from /mnt/data/config.json\n",
    "        run_id: str (optional). If None, use env RUN_ID or timestamp.\n",
    "\n",
    "    Returns:\n",
    "        dict: client_stub = {\n",
    "            'provider', 'base_url', 'model', 'api_key_ref',\n",
    "            'client_type', 'async_client', 'version'\n",
    "        }\n",
    "\n",
    "    Side effects:\n",
    "        - Write/Update handoff pickle & meta:\n",
    "          artifacts/{RUN_ID}/handoff/04-3_llm_tools_setup_with_tools.pkl\n",
    "          artifacts/{RUN_ID}/handoff/04-3_llm_tools_setup_with_tools.pkl.meta.json\n",
    "        - Ensure presence of required keys expected by downstream code.\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg = _load_config()\n",
    "    llm_cfg = cfg.get(\"llm\", {})\n",
    "\n",
    "    provider = llm_cfg.get(\"provider\")\n",
    "    base_url = llm_cfg.get(\"base_url\")\n",
    "    model = llm_cfg.get(\"model\")\n",
    "    api_key_ref = llm_cfg.get(\"api_key\") or None\n",
    "\n",
    "    # Client stub (no external network; OpenAI-compatible style)\n",
    "    client_stub = {\n",
    "        \"provider\": provider,\n",
    "        \"base_url\": base_url,\n",
    "        \"model\": model,\n",
    "        \"api_key_ref\": \"ENV:OPENAI_API_KEY\" if (api_key_ref in (None, \"\", \"null\")) else api_key_ref,\n",
    "        \"client_type\": \"openai-compatible\",\n",
    "        \"async_client\": False,\n",
    "        \"version\": \"0.1\",\n",
    "    }\n",
    "\n",
    "    # Prepare handoff paths\n",
    "    run_id = run_id or os.getenv(\"RUN_ID\") or _dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    art_dir = os.path.join(\"/mnt/data\", \"artifacts\", run_id, \"handoff\")\n",
    "    os.makedirs(art_dir, exist_ok=True)\n",
    "    pkl_path = os.path.join(art_dir, \"04-3_llm_tools_setup_with_tools.pkl\")\n",
    "    meta_path = pkl_path + \".meta.json\"\n",
    "\n",
    "    # Load existing if any\n",
    "    state = {}\n",
    "    if os.path.exists(pkl_path):\n",
    "        try:\n",
    "            with open(pkl_path, \"rb\") as f:\n",
    "                state = pickle.load(f) or {}\n",
    "        except Exception:\n",
    "            state = {}\n",
    "\n",
    "    # Ensure keys presence (keep existing values unless missing)\n",
    "    def _get(key, default):\n",
    "        return state[key] if key in state and state[key] not in (None, \"\") else default\n",
    "\n",
    "    state.update({\n",
    "        \"provider\": provider,\n",
    "        \"base_url\": base_url,\n",
    "        \"model\": model,\n",
    "        \"llm\": client_stub,\n",
    "        \"async_client\": _get(\"async_client\", False),\n",
    "        \"brand_keywords\": _get(\"brand_keywords\", []),\n",
    "        \"brand_keywords_cfg\": _get(\"brand_keywords_cfg\", {}),\n",
    "        \"required_stats\": _get(\"required_stats\", []),\n",
    "        \"required_top\": _get(\"required_top\", []),\n",
    "        \"present_stats\": _get(\"present_stats\", []),\n",
    "        \"present_top\": _get(\"present_top\", []),\n",
    "        \"missing_stats\": _get(\"missing_stats\", []),\n",
    "        \"missing_top\": _get(\"missing_top\", []),\n",
    "        \"stats_keys\": _get(\"stats_keys\", []),\n",
    "        \"top_level_keys\": _get(\"top_level_keys\", []),\n",
    "        \"paths\": _get(\"paths\", {}),\n",
    "        \"run_id\": run_id,\n",
    "        \"timestamp\": _dt.datetime.now().isoformat(),\n",
    "        \"version\": _get(\"version\", \"1\"),\n",
    "        \"pkl_path\": pkl_path,\n",
    "    })\n",
    "\n",
    "    # Save, then record byte_size both in meta and state (compat)\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(state, f)\n",
    "    byte_size = os.path.getsize(pkl_path)\n",
    "    state[\"byte_size\"] = byte_size\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(state, f)\n",
    "\n",
    "    meta = {\"keys\": sorted(list(state.keys())), \"byte_size\": byte_size, \"pkl_path\": pkl_path, \"run_id\": run_id}\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return client_stub\n",
    "\n",
    "print(\"[C-01] initialize_llm_client is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b3ae11b",
   "metadata": {
    "tags": [
     "C-03a",
     "handoff",
     "export"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04-3] handoff updated: artifacts/2026-01-03_222059/handoff/04-3_llm_tools_setup_with_tools.pkl | keys: ['async_client', 'brand_keywords', 'cert_full_info_map', 'cfg', 'llm', 'tools']\n"
     ]
    }
   ],
   "source": [
    "# === C-03a: Re-export fn_features_df into 04-3 handoff (minimal) ===\n",
    "import warnings, pickle, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# æ¨™æº–ãƒ‘ã‚¹è§£æ±º\n",
    "try:\n",
    "    import importlib, run_id_registry as runreg, _compat.paths as paths\n",
    "    rid = runreg.bootstrap()\n",
    "    importlib.reload(paths); \n",
    "    paths.ensure_roots()\n",
    "    RUN_ID = paths.RUN_ID or os.environ.get(\"RUN_ID\")\n",
    "    HANDOFF_DIR = Path(paths.compat_base_dirs[\"handoff\"])\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"[04-3] compat paths fallback: {e}\")\n",
    "    RUN_ID = os.environ.get(\"RUN_ID\")\n",
    "    HANDOFF_DIR = Path(f\"artifacts/{RUN_ID}/handoff\")\n",
    "\n",
    "pkl_path = HANDOFF_DIR / \"04-3_llm_tools_setup_with_tools.pkl\"\n",
    "\n",
    "# æ—¢å­˜handoffã‚’èª­ã¿å‡ºã—\n",
    "payload = {}\n",
    "try:\n",
    "    if pkl_path.exists() and pkl_path.stat().st_size > 0:\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            payload = pickle.load(f) or {}\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"[04-3] load existing handoff failed: {e}; recreate empty\")\n",
    "    payload = {}\n",
    "\n",
    "def _pick_prob_col(df):\n",
    "    for c in [\"ml_probability\",\"ml_prob\",\"prob\",\"proba\",\"pred_prob\",\"pred_proba\",\"probability\",\"phish_prob\",\"p_phish\",\"score\"]:\n",
    "        if c in getattr(df, \"columns\", []): \n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã« fn_features_df ãŒã‚ã‚Œã°è»½é‡å†ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆæœ€å¤§ 5000 è¡Œï¼‰\n",
    "try:\n",
    "    if \"fn_features_df\" in globals():\n",
    "        _df = globals()[\"fn_features_df\"]\n",
    "        if isinstance(_df, pd.DataFrame) and \"domain\" in _df.columns:\n",
    "            col = _pick_prob_col(_df)\n",
    "            if col:\n",
    "                small = _df[[\"domain\", col]].rename(columns={col: \"ml_probability\"}).copy()\n",
    "                if len(small) > 5000:\n",
    "                    small = small.sample(5000, random_state=42)\n",
    "                payload[\"fn_features_df\"] = small.reset_index(drop=True)\n",
    "                print(f\"[04-3] fn_features_df -> payload (rows={len(small)})\")\n",
    "    else:\n",
    "        print(\"[04-3] fn_features_df not found in globals(); skip export\")\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"[04-3] skip re-export fn_features_df: {e}\")\n",
    "\n",
    "# ä¸Šæ›¸ãä¿å­˜\n",
    "HANDOFF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(pkl_path, \"wb\") as f:\n",
    "    pickle.dump(payload, f)\n",
    "print(\"[04-3] handoff updated:\", pkl_path, \"| keys:\", sorted(payload.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f22428-3cc8-4a52-ac3e-fb4339eb87a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df4e29d7-fd1f-4f5b-9ab3-416e6ae49f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C-05-FIX] HANDOFF_DIR = artifacts/2026-01-03_222059/handoff\n",
      "[C-05-FIX] exported -> artifacts/2026-01-03_222059/handoff/04-3_llm_tools_setup_with_tools.pkl\n"
     ]
    }
   ],
   "source": [
    "# === C-05-FIX: robust handoff resolver + short_domain_analysis export ===\n",
    "import os, json, pickle, warnings\n",
    "from pathlib import Path\n",
    "import datetime as _dt\n",
    "\n",
    "def resolve_artifact_dirs(run_id: str = None):\n",
    "    \"\"\"\n",
    "    å„ªå…ˆ: _compat.paths + run_id_registry â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ­ãƒ¼ã‚«ãƒ« ./artifacts\n",
    "    æˆ»ã‚Šå€¤: (RUN_ID, HANDOFF_DIR(Path), RESULTS_DIR(Path))\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import importlib, run_id_registry as runreg, _compat.paths as paths\n",
    "        rid = run_id or runreg.bootstrap()\n",
    "        importlib.reload(paths); paths.ensure_roots()\n",
    "        base = paths.compat_base_dirs  # {'handoff': ..., 'results': ..., ...}\n",
    "        handoff = Path(base[\"handoff\"])\n",
    "        results = Path(base.get(\"results\", Path(base.get(\"root\", \".\")) / \"results\"))\n",
    "        return rid, handoff, results\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"[resolver] fallback to local ./artifacts (reason: {e})\")\n",
    "        rid = run_id or os.environ.get(\"RUN_ID\") or _dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        root = Path(os.environ.get(\"ARTIFACTS_ROOT\", \"artifacts\"))\n",
    "        handoff = root / rid / \"handoff\"\n",
    "        results = root / rid / \"results\"\n",
    "        handoff.mkdir(parents=True, exist_ok=True)\n",
    "        results.mkdir(parents=True, exist_ok=True)\n",
    "        return rid, handoff, results\n",
    "\n",
    "# short_domain_analysis ãŒæœªå®šç¾©ãªã‚‰å®šç¾©ï¼ˆC-05ã¨åŒç­‰ã®æœ€å°å®Ÿè£…ï¼‰\n",
    "try:\n",
    "    short_domain_analysis\n",
    "except NameError:\n",
    "    def short_domain_analysis(domain: str, ml_probability: float) -> dict:\n",
    "        import math\n",
    "        def calculate_entropy(text: str) -> float:\n",
    "            if not text: return 0.0\n",
    "            probs = [text.count(c)/len(text) for c in set(text)]\n",
    "            return -sum(p*math.log(p, 2) for p in probs if p > 0)\n",
    "        d = (domain or \"\").strip(); dl = d.lower()\n",
    "        parts = dl.split(\".\"); without_tld = \".\".join(parts[:-1]) if len(parts)>1 else dl\n",
    "        ent = calculate_entropy(without_tld); hyphens = d.count(\"-\")\n",
    "        digit_ratio = sum(c.isdigit() for c in without_tld) / max(1, len(without_tld))\n",
    "        res = {\"is_phishing\": False, \"confidence\": float(ml_probability), \"risk_factors\": {}, \"reasoning\": \"\"}\n",
    "        reasons = []\n",
    "        if len(without_tld) <= 10: res[\"risk_factors\"][\"short_name\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.30); reasons.append(\"çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³å\")\n",
    "        if ent > 4.0: res[\"risk_factors\"][\"high_entropy\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.55); reasons.append(f\"é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ {ent:.2f}\")\n",
    "        if hyphens > 2: res[\"risk_factors\"][\"many_hyphens\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.45); reasons.append(f\"ãƒã‚¤ãƒ•ãƒ³ãŒå¤šã„ ({hyphens})\")\n",
    "        if digit_ratio > 0.30: res[\"risk_factors\"][\"digit_ratio_high\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.40); reasons.append(f\"æ•°å­—ã®æ¯”çŽ‡ãŒé«˜ã„ ({digit_ratio:.2f})\")\n",
    "        if dl.startswith(\"xn--\"): res[\"risk_factors\"][\"punycode_like\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.60); reasons.append(\"Punycode ã£ã½ã„è¡¨è¨˜\")\n",
    "        if (parts[-1] if len(parts)>1 else \"\") in {\"zip\",\"mov\",\"rest\",\"xyz\",\"top\",\"mom\",\"cam\",\"bar\",\"quest\"}:\n",
    "            res[\"risk_factors\"][\"suspicious_tld\"]=parts[-1]; res[\"confidence\"]=max(res[\"confidence\"],0.50); reasons.append(f\"ç–‘ã‚ã—ã„TLD({parts[-1]})\")\n",
    "        res[\"is_phishing\"] = res[\"confidence\"] >= 0.50\n",
    "        res[\"reasoning\"] = \" / \".join(reasons)[:1500] if reasons else \"no-issues\"\n",
    "        return res\n",
    "\n",
    "# handoff ã« tools_code ã‚’è¿½è¨˜ï¼ˆæ—¢å­˜ã‚­ãƒ¼ã¯ä¿æŒï¼‰\n",
    "SOURCE_short = \"\"\"{}\"\"\".format(short_domain_analysis.__code__.co_consts[0] or \"\")  # Docstringã¯ç©ºãªã‚‰ç©º\n",
    "\n",
    "RUN_ID, HANDOFF_DIR, RESULTS_DIR = resolve_artifact_dirs(os.environ.get(\"RUN_ID\"))\n",
    "pkl = HANDOFF_DIR / \"04-3_llm_tools_setup_with_tools.pkl\"\n",
    "state = {}\n",
    "if pkl.exists() and pkl.stat().st_size > 0:\n",
    "    try:\n",
    "        with open(pkl, \"rb\") as f: state = pickle.load(f) or {}\n",
    "    except Exception: state = {}\n",
    "tools_code = state.get(\"tools_code\", {})\n",
    "tools_code[\"short_domain_analysis\"] = (\n",
    "    # ã‚½ãƒ¼ã‚¹æ–‡å­—åˆ—ãŒç©ºã®å ´åˆã¯å®‰å…¨å´ã®æœ€å°ç‰ˆã‚’æ ¼ç´\n",
    "    '''def short_domain_analysis(domain: str, ml_probability: float) -> dict:\n",
    "    import math\n",
    "    def calculate_entropy(text: str) -> float:\n",
    "        if not text: return 0.0\n",
    "        probs = [text.count(c)/len(text) for c in set(text)]\n",
    "        return -sum(p*math.log(p, 2) for p in probs if p > 0)\n",
    "    d=(domain or \"\").strip(); dl=d.lower()\n",
    "    parts=dl.split(\".\"); without_tld=\".\".join(parts[:-1]) if len(parts)>1 else dl\n",
    "    ent=calculate_entropy(without_tld); hyphens=d.count(\"-\")\n",
    "    digit_ratio=sum(c.isdigit() for c in without_tld)/max(1,len(without_tld))\n",
    "    res={\"is_phishing\":False,\"confidence\":float(ml_probability),\"risk_factors\":{},\"reasoning\":\"\"}; reasons=[]\n",
    "    if len(without_tld)<=10: res[\"risk_factors\"][\"short_name\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.30); reasons.append(\"çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³å\")\n",
    "    if ent>4.0: res[\"risk_factors\"][\"high_entropy\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.55); reasons.append(f\"é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ {ent:.2f}\")\n",
    "    if hyphens>2: res[\"risk_factors\"][\"many_hyphens\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.45); reasons.append(f\"ãƒã‚¤ãƒ•ãƒ³ãŒå¤šã„ ({hyphens})\")\n",
    "    if digit_ratio>0.30: res[\"risk_factors\"][\"digit_ratio_high\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.40); reasons.append(f\"æ•°å­—ã®æ¯”çŽ‡ãŒé«˜ã„ ({digit_ratio:.2f})\")\n",
    "    if dl.startswith(\"xn--\"): res[\"risk_factors\"][\"punycode_like\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.60); reasons.append(\"Punycode ã£ã½ã„è¡¨è¨˜\")\n",
    "    if (parts[-1] if len(parts)>1 else \"\") in {\"zip\",\"mov\",\"rest\",\"xyz\",\"top\",\"mom\",\"cam\",\"bar\",\"quest\"}:\n",
    "        res[\"risk_factors\"][\"suspicious_tld\"]=parts[-1]; res[\"confidence\"]=max(res[\"confidence\"],0.50); reasons.append(f\"ç–‘ã‚ã—ã„TLD({parts[-1]})\")\n",
    "    res[\"is_phishing\"]=res[\"confidence\"]>=0.50; res[\"reasoning\"]=\" / \".join(reasons)[:1500] if reasons else \"no-issues\"; return res\n",
    "    '''\n",
    ")\n",
    "state[\"tools_code\"] = tools_code\n",
    "state.setdefault(\"present_top\", [])\n",
    "if \"short_domain_analysis\" not in state[\"present_top\"]:\n",
    "    state[\"present_top\"].append(\"short_domain_analysis\")\n",
    "state.setdefault(\"top_level_keys\", [])\n",
    "for k in (\"tools_code\",\"present_top\"):\n",
    "    if k not in state[\"top_level_keys\"]:\n",
    "        state[\"top_level_keys\"].append(k)\n",
    "state[\"timestamp\"] = _dt.datetime.now().isoformat()\n",
    "\n",
    "with open(pkl, \"wb\") as f: pickle.dump(state, f)\n",
    "print(\"[C-05-FIX] HANDOFF_DIR =\", HANDOFF_DIR)\n",
    "print(\"[C-05-FIX] exported ->\", pkl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ed934e1-2ed3-49df-9035-9f989e6d669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C-05-REBASE] RUN_ID      = 2026-01-03_222059\n",
      "[C-05-REBASE] HANDOFF_DIR = artifacts/2026-01-03_222059/handoff\n",
      "[C-05-REBASE] exported    -> artifacts/2026-01-03_222059/handoff/04-3_llm_tools_setup_with_tools.pkl\n",
      "[C-05-REBASE] keys        -> ['async_client', 'brand_keywords', 'cert_full_info_map', 'cfg', 'llm', 'present_top', 'timestamp', 'tools', 'tools_code', 'top_level_keys']\n",
      "[C-05-REBASE] tools_code  -> ['short_domain_analysis']\n"
     ]
    }
   ],
   "source": [
    "# === C-05-REBASE: robust handoff resolver + short_domain_analysis export (04-3) ===\n",
    "import os, json, pickle, warnings\n",
    "from pathlib import Path\n",
    "import datetime as _dt\n",
    "\n",
    "def resolve_artifact_dirs(run_id: str = None):\n",
    "    \"\"\"\n",
    "    å„ªå…ˆ: run_id_registry + _compat.paths.ensure_roots() ã® handoff/results\n",
    "    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚«ãƒ¬ãƒ³ãƒˆé…ä¸‹ ./artifacts/{RUN_ID}/handoff|results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import importlib, run_id_registry as runreg, _compat.paths as paths\n",
    "        rid = run_id or runreg.bootstrap()\n",
    "        importlib.reload(paths); paths.ensure_roots()\n",
    "        base = paths.compat_base_dirs           # ä¾‹: {'handoff': 'artifacts/<RID>/handoff', 'results': ...}\n",
    "        handoff = Path(base[\"handoff\"])\n",
    "        results = Path(base.get(\"results\", Path(base.get(\"root\", \".\")) / \"results\"))\n",
    "        # å¿µã®ãŸã‚ä½œæˆï¼ˆæ¨©é™ã‚„ãƒ‘ã‚¹ä¸æ•´åˆæ™‚ã¯ä¾‹å¤–â†’fallbackã¸ï¼‰\n",
    "        handoff.mkdir(parents=True, exist_ok=True)\n",
    "        results.mkdir(parents=True, exist_ok=True)\n",
    "        return rid, handoff, results\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"[resolver] fallback to local ./artifacts (reason: {e})\")\n",
    "        rid = run_id or os.environ.get(\"RUN_ID\") or _dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        root = Path(os.environ.get(\"ARTIFACTS_ROOT\", \"artifacts\"))\n",
    "        handoff = root / rid / \"handoff\"\n",
    "        results = root / rid / \"results\"\n",
    "        handoff.mkdir(parents=True, exist_ok=True)\n",
    "        results.mkdir(parents=True, exist_ok=True)\n",
    "        return rid, handoff, results\n",
    "\n",
    "# `short_domain_analysis` ãŒæœªå®šç¾©ãªã‚‰æœ€å°å®Ÿè£…ã‚’å®šç¾©ï¼ˆ03ç”±æ¥ãƒ»æ§‹é€ åŒ–å‡ºåŠ›ä¿è¨¼ï¼‰\n",
    "try:\n",
    "    short_domain_analysis\n",
    "except NameError:\n",
    "    def short_domain_analysis(domain: str, ml_probability: float) -> dict:\n",
    "        import math\n",
    "        def calculate_entropy(text: str) -> float:\n",
    "            if not text: return 0.0\n",
    "            probs = [text.count(c)/len(text) for c in set(text)]\n",
    "            return -sum(p*math.log(p, 2) for p in probs if p > 0)\n",
    "        d = (domain or \"\").strip(); dl = d.lower()\n",
    "        parts = dl.split(\".\"); without_tld = \".\".join(parts[:-1]) if len(parts)>1 else dl\n",
    "        ent = calculate_entropy(without_tld); hyphens = d.count(\"-\")\n",
    "        digit_ratio = sum(c.isdigit() for c in without_tld) / max(1, len(without_tld))\n",
    "        res = {\"is_phishing\": False, \"confidence\": float(ml_probability), \"risk_factors\": {}, \"reasoning\": \"\"}\n",
    "        reasons = []\n",
    "        if len(without_tld) <= 10: res[\"risk_factors\"][\"short_name\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.30); reasons.append(\"çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³å\")\n",
    "        if ent > 4.0: res[\"risk_factors\"][\"high_entropy\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.55); reasons.append(f\"é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ {ent:.2f}\")\n",
    "        if hyphens > 2: res[\"risk_factors\"][\"many_hyphens\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.45); reasons.append(f\"ãƒã‚¤ãƒ•ãƒ³ãŒå¤šã„ ({hyphens})\")\n",
    "        if digit_ratio > 0.30: res[\"risk_factors\"][\"digit_ratio_high\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.40); reasons.append(f\"æ•°å­—ã®æ¯”çŽ‡ãŒé«˜ã„ ({digit_ratio:.2f})\")\n",
    "        if dl.startswith(\"xn--\"): res[\"risk_factors\"][\"punycode_like\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.60); reasons.append(\"Punycode ã£ã½ã„è¡¨è¨˜\")\n",
    "        if (parts[-1] if len(parts)>1 else \"\") in {\"zip\",\"mov\",\"rest\",\"xyz\",\"top\",\"mom\",\"cam\",\"bar\",\"quest\"}:\n",
    "            res[\"risk_factors\"][\"suspicious_tld\"]=parts[-1]; res[\"confidence\"]=max(res[\"confidence\"],0.50); reasons.append(f\"ç–‘ã‚ã—ã„TLD({parts[-1]})\")\n",
    "        res[\"is_phishing\"] = res[\"confidence\"] >= 0.50\n",
    "        res[\"reasoning\"] = \" / \".join(reasons)[:1500] if reasons else \"no-issues\"\n",
    "        return res\n",
    "\n",
    "# handoffã« tools_code ã¨ã—ã¦åŒæ¢±ï¼ˆæ—¢å­˜ã‚­ãƒ¼ã¯ä¿æŒãƒ»éžç ´å£Šï¼‰\n",
    "# ã“ã“ã§ã¯é–¢æ•°æœ¬ä½“ã‚’å®‰å…¨ãªæœ€å°ç‰ˆã¨ã—ã¦æ–‡å­—åˆ—åŒ–ï¼ˆç’°å¢ƒå·®ã®ãªã„ exec ç”¨ï¼‰\n",
    "SOURCE_short = '''\n",
    "def short_domain_analysis(domain: str, ml_probability: float) -> dict:\n",
    "    import math\n",
    "    def calculate_entropy(text: str) -> float:\n",
    "        if not text: return 0.0\n",
    "        probs = [text.count(c)/len(text) for c in set(text)]\n",
    "        return -sum(p*math.log(p, 2) for p in probs if p > 0)\n",
    "    d=(domain or \"\").strip(); dl=d.lower()\n",
    "    parts=dl.split(\".\"); without_tld=\".\".join(parts[:-1]) if len(parts)>1 else dl\n",
    "    ent=calculate_entropy(without_tld); hyphens=d.count(\"-\")\n",
    "    digit_ratio=sum(c.isdigit() for c in without_tld)/max(1,len(without_tld))\n",
    "    res={\"is_phishing\":False,\"confidence\":float(ml_probability),\"risk_factors\":{},\"reasoning\":\"\"}; reasons=[]\n",
    "    if len(without_tld)<=10: res[\"risk_factors\"][\"short_name\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.30); reasons.append(\"çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³å\")\n",
    "    if ent>4.0: res[\"risk_factors\"][\"high_entropy\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.55); reasons.append(f\"é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ {ent:.2f}\")\n",
    "    if hyphens>2: res[\"risk_factors\"][\"many_hyphens\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.45); reasons.append(f\"ãƒã‚¤ãƒ•ãƒ³ãŒå¤šã„ ({hyphens})\")\n",
    "    if digit_ratio>0.30: res[\"risk_factors\"][\"digit_ratio_high\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.40); reasons.append(f\"æ•°å­—ã®æ¯”çŽ‡ãŒé«˜ã„ ({digit_ratio:.2f})\")\n",
    "    if dl.startswith(\"xn--\"): res[\"risk_factors\"][\"punycode_like\"]=True; res[\"confidence\"]=max(res[\"confidence\"],0.60); reasons.append(\"Punycode ã£ã½ã„è¡¨è¨˜\")\n",
    "    if (parts[-1] if len(parts)>1 else \"\") in {\"zip\",\"mov\",\"rest\",\"xyz\",\"top\",\"mom\",\"cam\",\"bar\",\"quest\"}:\n",
    "        res[\"risk_factors\"][\"suspicious_tld\"]=parts[-1]; res[\"confidence\"]=max(res[\"confidence\"],0.50); reasons.append(f\"ç–‘ã‚ã—ã„TLD({parts[-1]})\")\n",
    "    res[\"is_phishing\"]=res[\"confidence\"]>=0.50; res[\"reasoning\"]=\" / \".join(reasons)[:1500] if reasons else \"no-issues\"\n",
    "    return res\n",
    "'''.strip()\n",
    "\n",
    "RUN_ID, HANDOFF_DIR, RESULTS_DIR = resolve_artifact_dirs(os.environ.get(\"RUN_ID\"))\n",
    "pkl = HANDOFF_DIR / \"04-3_llm_tools_setup_with_tools.pkl\"\n",
    "\n",
    "state = {}\n",
    "if pkl.exists() and pkl.stat().st_size > 0:\n",
    "    try:\n",
    "        with open(pkl, \"rb\") as f:\n",
    "            state = pickle.load(f) or {}\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"[04-3] read handoff failed, recreate empty: {e}\")\n",
    "        state = {}\n",
    "\n",
    "tools_code = state.get(\"tools_code\", {})\n",
    "tools_code[\"short_domain_analysis\"] = SOURCE_short\n",
    "state[\"tools_code\"] = tools_code\n",
    "\n",
    "state.setdefault(\"present_top\", [])\n",
    "if \"short_domain_analysis\" not in state[\"present_top\"]:\n",
    "    state[\"present_top\"].append(\"short_domain_analysis\")\n",
    "\n",
    "state.setdefault(\"top_level_keys\", [])\n",
    "for k in (\"tools_code\", \"present_top\"):\n",
    "    if k not in state[\"top_level_keys\"]:\n",
    "        state[\"top_level_keys\"].append(k)\n",
    "\n",
    "state[\"timestamp\"] = _dt.datetime.now().isoformat()\n",
    "\n",
    "with open(pkl, \"wb\") as f:\n",
    "    pickle.dump(state, f)\n",
    "\n",
    "print(\"[C-05-REBASE] RUN_ID      =\", RUN_ID)\n",
    "print(\"[C-05-REBASE] HANDOFF_DIR =\", HANDOFF_DIR)\n",
    "print(\"[C-05-REBASE] exported    ->\", pkl)\n",
    "print(\"[C-05-REBASE] keys        ->\", sorted(state.keys()))\n",
    "print(\"[C-05-REBASE] tools_code  ->\", sorted(state.get(\"tools_code\",{}).keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd722dbe-dabc-47f5-8416-73f850c6ace8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
