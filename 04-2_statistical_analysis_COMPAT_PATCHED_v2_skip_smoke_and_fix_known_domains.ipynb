{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f06a045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NX] RUN_ID = 2026-01-10_140940 | paths.RUN_ID = 2026-01-10_140940\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0 (02ä»¥é™ å…±é€š): ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰è§£æ±ºã—ã¦ paths ã‚’èª­ã‚€ ===\n",
    "import run_id_registry as runreg\n",
    "rid = runreg.bootstrap()  # envâ†’ãƒ•ã‚¡ã‚¤ãƒ«(artifacts/_current/run_id.txt)â†’Part3â†’latestâ†’æ–°è¦ ã®é †ã§è§£æ±º\n",
    "\n",
    "import importlib\n",
    "import _compat.paths as paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "print(\"[NX] RUN_ID =\", rid, \"| paths.RUN_ID =\", paths.RUN_ID)\n",
    "paths.ensure_roots()  # artifacts/{RUN_ID}/... ã‚’å¿…ãšä½œæˆ\n",
    "BASE_DIRS = paths.compat_base_dirs  # {'raw','data','models','results','handoff','logs','traces'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85575615",
   "metadata": {},
   "source": [
    "# 04-2_statistical_analysis (RUN_ID/lineageå¯¾å¿œ, joblibå„ªå…ˆãƒ­ãƒ¼ãƒ€)\n",
    "\n",
    "ãƒãƒ¼ã‚¸ãƒ§ãƒ³: 2025-10-20 rebuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6cef0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COMPAT] RUN_ID = 2026-01-10_140940\n",
      "[COMPAT] Using cfg from: _compat/config.json\n",
      "[COMPAT] handoff dir: artifacts/2026-01-10_140940/handoff\n"
     ]
    }
   ],
   "source": [
    "# === COMPAT BOOTSTRAP (04-2): use _compat/paths.py & _compat/config.json ===\n",
    "import sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "if \"_compat\" not in sys.path:\n",
    "    sys.path.append(\"_compat\")\n",
    "\n",
    "from paths import ensure_roots, compat_base_dirs, load_config as _load_config  # _compat/paths.py\n",
    "ensure_roots()\n",
    "RUN_ID = paths.RUN_ID  # [PATCH: unify RUN_ID]\n",
    "\n",
    "CFG_PATH = Path(\"_compat/config.json\")\n",
    "if not CFG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Required config not found: {CFG_PATH}\")\n",
    "cfg = json.loads(CFG_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# output_dirs ã‚’ compat_base_dirs ã‹ã‚‰æ§‹æˆï¼ˆæ—¢å­˜ã® output_dirs ãŒã‚ã‚‹å ´åˆã¯å°Šé‡ï¼‰\n",
    "if 'output_dirs' not in globals():\n",
    "    output_dirs = {\n",
    "        'raw': compat_base_dirs['raw'],\n",
    "        'data': compat_base_dirs['data'],\n",
    "        'models': compat_base_dirs['models'],\n",
    "        'results': compat_base_dirs['results'],\n",
    "        'handoff': compat_base_dirs['handoff'],\n",
    "        'logs': compat_base_dirs['logs'],\n",
    "        'traces': compat_base_dirs['traces'],\n",
    "    }\n",
    "\n",
    "print(\"[COMPAT] RUN_ID =\", RUN_ID)\n",
    "print(\"[COMPAT] Using cfg from:\", CFG_PATH)\n",
    "print(\"[COMPAT] handoff dir:\", output_dirs['handoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dbecde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NX] RUN_ID = 2026-01-10_140940 | paths.RUN_ID = 2026-01-10_140940\n",
      "[NX] handoff dir: artifacts/2026-01-10_140940/handoff\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0 (02ä»¥é™ å…±é€š): ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰è§£æ±ºã—ã¦ _compat.paths ã‚’èª­ã‚€ ===\n",
    "import importlib, os, json\n",
    "from pathlib import Path\n",
    "\n",
    "import run_id_registry as runreg\n",
    "rid = runreg.bootstrap()  # envâ†’ãƒ•ã‚¡ã‚¤ãƒ«(artifacts/_current/run_id.txt)â†’Part3â†’latestâ†’æ–°è¦ ã®é †ã§è§£æ±º\n",
    "\n",
    "import _compat.paths as paths\n",
    "importlib.reload(paths)\n",
    "importlib.reload(paths)\n",
    "\n",
    "paths.ensure_roots()\n",
    "RUN_ID = paths.RUN_ID  # [PATCH: unify RUN_ID]\n",
    "output_dirs = paths.compat_base_dirs\n",
    "\n",
    "print(\"[NX] RUN_ID =\", rid, \"| paths.RUN_ID =\", RUN_ID)\n",
    "print(\"[NX] handoff dir:\", output_dirs.get(\"handoff\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ed36f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded _compat/config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 1: Config loader ===\n",
    "cfg = {}\n",
    "cfg_json = Path('_compat/config.json')\n",
    "if cfg_json.exists():\n",
    "    with open(cfg_json, 'r', encoding='utf-8') as f:\n",
    "        cfg = json.load(f)\n",
    "        print(\"âœ… Loaded _compat/config.json\")\n",
    "else:\n",
    "    try:\n",
    "        cfg = paths.load_config()\n",
    "        print(\"âš ï¸ _compat/config.json ãŒãªã„ãŸã‚ paths.load_config() ã‚’ä½¿ç”¨\")\n",
    "    except Exception as e:\n",
    "        cfg = {}\n",
    "        print(\"âš ï¸ load_config() ä¸åœ¨ã€‚ç©ºè¨­å®šã§ç¶™ç¶š:\", e)\n",
    "\n",
    "SESSION_ID = cfg.get(\"session_id\") or os.getenv(\"SESSION_ID\") or RUN_ID\n",
    "cfg.setdefault(\"run_id\", RUN_ID)\n",
    "cfg.setdefault(\"session_id\", SESSION_ID)\n",
    "cfg.setdefault(\"output_dirs\", output_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9add8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ DB_CONFIG ready: {'dbname': 'rapids_data', 'host': 'localhost', 'port': '5432', 'user': 'postgres'}\n",
      "cfg keys: ['DB_CONFIG', 'analysis', 'brand_keywords', 'db', 'engine', 'free_ca_list', 'full_processing', 'handoff', 'llm', 'model', 'output_dirs', 'paths', 'run_id', 'session_id', 'system', 'tld_analysis', 'visualization'] ...\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Apply config / paths / DB guard ===\n",
    "def _coerce_db_config(d: dict) -> dict:\n",
    "    import os\n",
    "    d = d or {}\n",
    "    aliases = {\n",
    "        \"dbname\": [\"dbname\", \"database\", \"db\", \"name\"],\n",
    "        \"user\": [\"user\", \"username\"],\n",
    "        \"password\": [\"password\", \"pass\", \"pwd\"],\n",
    "        \"host\": [\"host\", \"hostname\"],\n",
    "        \"port\": [\"port\"],\n",
    "    }\n",
    "    out = {}\n",
    "    for k, ks in aliases.items():\n",
    "        for kk in ks:\n",
    "            if kk in d and d[kk]:\n",
    "                out[k] = d[kk]\n",
    "                break\n",
    "    # env ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "    env_default = {\n",
    "        \"dbname\": os.getenv(\"PGDATABASE\", \"rapids_data\"),\n",
    "        \"user\": os.getenv(\"PGUSER\", \"postgres\"),\n",
    "        \"password\": os.getenv(\"PGPASSWORD\", \"postgres\"),\n",
    "        \"host\": os.getenv(\"PGHOST\", \"127.0.0.1\"),\n",
    "        \"port\": os.getenv(\"PGPORT\", \"5432\"),\n",
    "    }\n",
    "    for k, v in env_default.items():\n",
    "        out.setdefault(k, v)\n",
    "    return out\n",
    "\n",
    "DB_CONFIG = _coerce_db_config(cfg.get(\"DB_CONFIG\", {}) if isinstance(cfg, dict) else {})\n",
    "if not any(DB_CONFIG.values()):\n",
    "    print(\"âš ï¸ DB_CONFIG ãŒæœªè¨­å®šã§ã™ï¼ˆ04-2ã§ã¯è‡´å‘½çš„ã§ã¯ãªã„ãŸã‚ç¶™ç¶šï¼‰ã€‚\")\n",
    "\n",
    "print(\"ğŸ”§ DB_CONFIG ready:\", {k: DB_CONFIG.get(k, \"-\") for k in [\"dbname\",\"host\",\"port\",\"user\"]})\n",
    "print(\"cfg keys:\", sorted(list(cfg.keys()))[:20], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44b235fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resolver] handoff_in=artifacts/2026-01-10_140940/handoff/04-1_config_and_data_preparation.pkl (source=strict)\n",
      "[handoff] false_negatives_df=55524 rows | brand_keywords=109 | cert_map=55524\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2.3: handoff resolverï¼ˆå…¥åŠ›: 04-1 ã®æˆæœã‚’èª­ã‚€ï¼‰ ===\n",
    "import glob, time, pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def _read_json(path: Path):\n",
    "    try:\n",
    "        if path.exists():\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ JSON read failed: {path} | {e}\")\n",
    "    return None\n",
    "\n",
    "def _same_run_id(p: Path) -> bool:\n",
    "    try:\n",
    "        return str(p).split(\"artifacts/\")[1].split(\"/\")[0] == RUN_ID\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _joblib_first_load(p: Path):\n",
    "    try:\n",
    "        import joblib\n",
    "        return joblib.load(p)\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            with open(p, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f\"Failed to load handoff via joblib and pickle: {e1} | {e2}\")\n",
    "\n",
    "def resolve_handoff_04_1():\n",
    "    # (1) ENV\n",
    "    env_p = os.getenv(\"HANDOFF_IN_PATH\")\n",
    "    if env_p:\n",
    "        ep = Path(env_p)\n",
    "        if ep.exists() and _same_run_id(ep):\n",
    "            return ep, \"env\"\n",
    "\n",
    "    # (2) cfg.handoff[\"04-1\"][\"path\"]\n",
    "    try:\n",
    "        cfg_p = cfg.get(\"handoff\", {}).get(\"04-1\", {}).get(\"path\")\n",
    "        if cfg_p:\n",
    "            cp = Path(cfg_p)\n",
    "            if cp.exists() and _same_run_id(cp):\n",
    "                return cp, \"cfg\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # (3) pointers\n",
    "    for lp in [Path(\"artifacts/_current_lineage.json\"),\n",
    "               Path(output_dirs[\"handoff\"]) / \"_lineage.json\"]:\n",
    "        j = _read_json(lp)\n",
    "        if not j:\n",
    "            continue\n",
    "        items = []\n",
    "        if isinstance(j, list):\n",
    "            items = j\n",
    "        elif isinstance(j, dict):\n",
    "            items = j.get(\"items\", [])\n",
    "            last = j.get(\"last\")\n",
    "            if isinstance(last, dict) and str(last.get(\"type\",\"\")).startswith(\"04-1\"):\n",
    "                p = Path(last.get(\"path\",\"\"))\n",
    "                if p.exists() and _same_run_id(p):\n",
    "                    return p, \"pointer(last)\"\n",
    "        for it in reversed(items):\n",
    "            try:\n",
    "                if str(it.get(\"type\",\"\")).startswith(\"04-1\"):\n",
    "                    p = Path(it.get(\"path\",\"\"))\n",
    "                    if p.exists() and _same_run_id(p):\n",
    "                        return p, \"pointer(items)\"\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # (4) default strict name\n",
    "    strict = Path(output_dirs[\"handoff\"]) / \"04-1_config_and_data_preparation.pkl\"\n",
    "    if strict.exists() and _same_run_id(strict):\n",
    "        return strict, \"strict\"\n",
    "\n",
    "    # (5) wildcard newest\n",
    "    for pat in [str(Path(output_dirs[\"handoff\"]) / \"04-1_*.pkl\"),\n",
    "                str(Path(output_dirs[\"handoff\"]) / \"04-1*.pkl\")]:\n",
    "        files = sorted(glob.glob(pat), key=lambda p: Path(p).stat().st_mtime, reverse=True)\n",
    "        for f in files:\n",
    "            fp = Path(f)\n",
    "            if fp.exists() and _same_run_id(fp):\n",
    "                return fp, \"wildcard\"\n",
    "\n",
    "    all04 = sorted(Path(output_dirs[\"handoff\"]).glob(\"04-1*.pkl\"))\n",
    "    same_run = [str(p) for p in all04 if _same_run_id(p)]\n",
    "    raise FileNotFoundError(f\"[resolver] 04-1 handoff not found under same RUN_ID. candidates(same-run)={same_run}\")\n",
    "\n",
    "handoff_in_path, handoff_source = None, None\n",
    "try:\n",
    "    handoff_in_path, handoff_source = resolve_handoff_04_1()\n",
    "    print(f\"[resolver] handoff_in={handoff_in_path} (source={handoff_source})\")\n",
    "    handoff_in = _joblib_first_load(handoff_in_path)\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ resolver/load failed:\", e)\n",
    "    handoff_in = {}\n",
    "\n",
    "for k in [\"cfg\", \"RUN_ID\", \"output_dirs\"]:\n",
    "    if not isinstance(handoff_in, dict) or k not in handoff_in:\n",
    "        print(f\"âš ï¸ [handoff] missing key: {k}\")\n",
    "\n",
    "import pandas as pd\n",
    "false_negatives_df = handoff_in.get(\"false_negatives_df\", pd.DataFrame())\n",
    "brand_keywords = handoff_in.get(\"brand_keywords\", [])\n",
    "cert_full_info_map = handoff_in.get(\"cert_full_info_map\", {})\n",
    "print(f\"[handoff] false_negatives_df={len(false_negatives_df)} rows | brand_keywords={len(brand_keywords)} | cert_map={len(cert_full_info_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f70a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ HIGH_RISK_WORDS derived from brand_keywords (n=109)\n",
      "ğŸ”§ suspicious_words_stats derived (unique=55892)\n",
      "ğŸ”§ TLD_STATS derived (unique=504)\n",
      "ğŸ”§ KNOWN_DOMAINS derived from false_negatives_df (n=55524)\n",
      "âœ… stats resolver completed.\n"
     ]
    }
   ],
   "source": [
    "# === analysis stats resolver (auto-added) ===\n",
    "# ç›®çš„: HIGH_RISK_WORDS, suspicious_words_stats, TLD_STATS, KNOWN_DOMAINS ã‚’å¯èƒ½ãªç¯„å›²ã§åˆæœŸåŒ–\n",
    "import pandas as pd\n",
    "\n",
    "# HIGH_RISK_WORDS: brand_keywords ã‚’å„ªå…ˆ\n",
    "if 'HIGH_RISK_WORDS' not in globals():\n",
    "    if 'brand_keywords' in globals() and isinstance(brand_keywords, (list, set, dict)):\n",
    "        if isinstance(brand_keywords, dict):\n",
    "            HIGH_RISK_WORDS = sorted(set(sum(([k] + (v if isinstance(v, list) else []) for k,v in brand_keywords.items()), [])))\n",
    "        else:\n",
    "            HIGH_RISK_WORDS = sorted(list(set(brand_keywords)))\n",
    "        print(f\"ğŸ”§ HIGH_RISK_WORDS derived from brand_keywords (n={len(HIGH_RISK_WORDS)})\")\n",
    "    else:\n",
    "        HIGH_RISK_WORDS = []\n",
    "        print(\"âš ï¸ HIGH_RISK_WORDS fallback: empty list\")\n",
    "\n",
    "# suspicious_words_stats: false_negatives_df ãŒã‚ã‚Œã°ç°¡æ˜“é »åº¦é›†è¨ˆï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã¯æœ€å°é™ï¼‰\n",
    "if 'suspicious_words_stats' not in globals():\n",
    "    try:\n",
    "        if 'false_negatives_df' in globals() and hasattr(false_negatives_df, 'assign'):\n",
    "            df = false_negatives_df.copy()\n",
    "            # æœ€å°é™ï¼šãƒ‰ãƒ¡ã‚¤ãƒ³ã® '-' '_' '.' ã§åˆ†å‰²ã—ã¦å˜èªé »åº¦\n",
    "            def split_words(s):\n",
    "                try:\n",
    "                    for ch in ['-', '_', '.']:\n",
    "                        s = s.replace(ch, ' ')\n",
    "                    return [w for w in s.split() if w]\n",
    "                except Exception:\n",
    "                    return []\n",
    "            words = df['domain'].astype(str).map(split_words)\n",
    "            from collections import Counter\n",
    "            cnt = Counter([w for lst in words for w in lst])\n",
    "            suspicious_words_stats = pd.DataFrame(cnt.items(), columns=['word','count']).sort_values('count', ascending=False)\n",
    "            print(f\"ğŸ”§ suspicious_words_stats derived (unique={len(suspicious_words_stats)})\")\n",
    "        else:\n",
    "            suspicious_words_stats = pd.DataFrame(columns=['word','count'])\n",
    "            print(\"âš ï¸ suspicious_words_stats fallback: empty frame\")\n",
    "    except Exception as e:\n",
    "        suspicious_words_stats = pd.DataFrame(columns=['word','count'])\n",
    "        print(\"âš ï¸ suspicious_words_stats error -> empty:\", e)\n",
    "\n",
    "# TLD_STATS: false_negatives_df ãŒã‚ã‚Œã°ç°¡æ˜“é›†è¨ˆ\n",
    "if 'TLD_STATS' not in globals():\n",
    "    try:\n",
    "        if 'false_negatives_df' in globals() and 'domain' in false_negatives_df.columns:\n",
    "            def get_tld(d):\n",
    "                try:\n",
    "                    parts = str(d).split('.')\n",
    "                    return parts[-1] if len(parts)>1 else ''\n",
    "                except Exception:\n",
    "                    return ''\n",
    "            tlds = false_negatives_df['domain'].map(get_tld)\n",
    "            TLD_STATS = tlds.value_counts().rename_axis('tld').reset_index(name='count')\n",
    "            print(f\"ğŸ”§ TLD_STATS derived (unique={len(TLD_STATS)})\")\n",
    "        else:\n",
    "            import pandas as pd\n",
    "            TLD_STATS = pd.DataFrame(columns=['tld','count'])\n",
    "            print(\"âš ï¸ TLD_STATS fallback: empty frame\")\n",
    "    except Exception as e:\n",
    "        import pandas as pd\n",
    "        TLD_STATS = pd.DataFrame(columns=['tld','count'])\n",
    "        print(\"âš ï¸ TLD_STATS error -> empty:\", e)\n",
    "\n",
    "# KNOWN_DOMAINS: false_negatives_df or cert_full_info_map ã‹ã‚‰æ¨å®š\n",
    "if 'KNOWN_DOMAINS' not in globals():\n",
    "    try:\n",
    "        if 'false_negatives_df' in globals() and 'domain' in false_negatives_df.columns:\n",
    "            KNOWN_DOMAINS = set(false_negatives_df['domain'].dropna().astype(str).tolist())\n",
    "            print(f\"ğŸ”§ KNOWN_DOMAINS derived from false_negatives_df (n={len(KNOWN_DOMAINS)})\")\n",
    "        elif 'cert_full_info_map' in globals() and isinstance(cert_full_info_map, dict):\n",
    "            KNOWN_DOMAINS = set(cert_full_info_map.keys())\n",
    "            print(f\"ğŸ”§ KNOWN_DOMAINS derived from cert_full_info_map (n={len(KNOWN_DOMAINS)})\")\n",
    "        else:\n",
    "            KNOWN_DOMAINS = set()\n",
    "            print(\"âš ï¸ KNOWN_DOMAINS fallback: empty set\")\n",
    "    except Exception as e:\n",
    "        KNOWN_DOMAINS = set()\n",
    "        print(\"âš ï¸ KNOWN_DOMAINS error -> empty:\", e)\n",
    "\n",
    "print(\"âœ… stats resolver completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3beeecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… stats normalized: KNOWN_DOMAINS=dict HIGH_RISK_WORDS=109\n"
     ]
    }
   ],
   "source": [
    "# === stats normalize (auto-added) ===\n",
    "# ç›®çš„: ä¸‹æµã‚»ãƒ«ãŒ dict.keys() å‰æã§ã‚‚è½ã¡ãªã„ã‚ˆã†ã€å‹ã‚’æ­£è¦åŒ–\n",
    "def _to_domain_dict(obj):\n",
    "    if obj is None:\n",
    "        return {}\n",
    "    if isinstance(obj, dict):\n",
    "        # ã™ã§ã« dict ã®å ´åˆã¯ã‚­ãƒ¼ã‚’ str ã«å¯„ã›ã‚‹\n",
    "        return {str(k): v for k, v in obj.items()}\n",
    "    if isinstance(obj, (set, list, tuple)):\n",
    "        return {str(x): True for x in obj}\n",
    "    # æƒ³å®šå¤–ã¯ç©ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "    return {}\n",
    "\n",
    "# KNOWN_DOMAINS ã‚’ dict ã«æ­£è¦åŒ–ï¼ˆ.keys() ã‚’å®‰å…¨ã«å‘¼ã¹ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰\n",
    "if 'KNOWN_DOMAINS' in globals():\n",
    "    KNOWN_DOMAINS = _to_domain_dict(KNOWN_DOMAINS)\n",
    "\n",
    "# HIGH_RISK_WORDS ã¯ list ã¸å¯„ã›ã¦é‡è¤‡æ’é™¤\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    if isinstance(HIGH_RISK_WORDS, dict):\n",
    "        HIGH_RISK_WORDS = list(HIGH_RISK_WORDS.keys())\n",
    "    elif isinstance(HIGH_RISK_WORDS, (set, tuple)):\n",
    "        HIGH_RISK_WORDS = list(HIGH_RISK_WORDS)\n",
    "    HIGH_RISK_WORDS = sorted(list(set(map(str, HIGH_RISK_WORDS))))\n",
    "\n",
    "print(\"âœ… stats normalized:\",\n",
    "      \"KNOWN_DOMAINS=dict\" if isinstance(globals().get('KNOWN_DOMAINS', None), dict) else type(globals().get('KNOWN_DOMAINS', None)),\n",
    "      f\"HIGH_RISK_WORDS={len(globals().get('HIGH_RISK_WORDS', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f3583e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === brand_keywords resolver (auto-added) ===\n",
    "# å„ªå…ˆ: handoff -> artifacts/{RUN_ID}/models/brand_keywords.json -> models/*/brand_keywords.json\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "def _load_brand_keywords_candidates():\n",
    "    cands = []\n",
    "    # 1) handoff\n",
    "    if 'handoff' in globals() and isinstance(handoff, dict) and 'brand_keywords' in handoff:\n",
    "        return handoff['brand_keywords']\n",
    "    # 2) artifacts/{RUN_ID}/models\n",
    "    try:\n",
    "        p = Path(MODELS_DIR) / \"brand_keywords.json\"\n",
    "        if p.exists():\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3) legacy models/*/brand_keywords.json (latest)\n",
    "    try:\n",
    "        latest = None\n",
    "        models_root = Path(\"models\")\n",
    "        if models_root.exists():\n",
    "            dirs = sorted([d for d in models_root.glob(\"*\") if d.is_dir()])\n",
    "            if dirs:\n",
    "                latest = dirs[-1]\n",
    "                p = latest / \"brand_keywords.json\"\n",
    "                if p.exists():\n",
    "                    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                        return json.load(f)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "if 'brand_keywords' not in globals() or brand_keywords in (None, [], {}):\n",
    "    _bk = _load_brand_keywords_candidates()\n",
    "    if _bk is not None:\n",
    "        brand_keywords = _bk\n",
    "        print(\"ğŸ”§ brand_keywords loaded\")\n",
    "    else:\n",
    "        print(\"âš ï¸ brand_keywords not found in handoff nor models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76839746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ DB_CONFIG ready: {'dbname': 'rapids_data', 'host': 'localhost', 'port': '5432', 'user': 'postgres'}\n",
      "â„¹ï¸ optional inputs missing at load-time: ['fn_features_df'] â€” trying to backfill (lineage â†’ 04-1 â†’ cfg).\n",
      "â„¹ï¸ keys to be produced in this module: ['DANGEROUS_TLDS', 'LEGITIMATE_TLDS', 'NEUTRAL_TLDS']\n",
      "âœ… handoff unpack (partial allowed)\n"
     ]
    }
   ],
   "source": [
    "# === DB_CONFIG guard (mirrors 01_data_preparation, refined) ===\n",
    "import os, json, warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# --- DB_CONFIG (same behavior) ---\n",
    "if 'DB_CONFIG' not in globals():\n",
    "    if 'handoff' in globals() and isinstance(handoff, dict) and 'DB_CONFIG' in handoff:\n",
    "        DB_CONFIG = handoff['DB_CONFIG']\n",
    "    else:\n",
    "        DB_CONFIG = {\n",
    "            'dbname': os.getenv('PGDATABASE','rapids_data'),\n",
    "            'user': os.getenv('PGUSER','postgres'),\n",
    "            'password': os.getenv('PGPASSWORD','asomura'),\n",
    "            'host': os.getenv('PGHOST','localhost'),\n",
    "            'port': os.getenv('PGPORT','5432'),\n",
    "        }\n",
    "print(\"ğŸ”§ DB_CONFIG ready:\", {k: DB_CONFIG[k] for k in ['dbname','host','port','user']})\n",
    "\n",
    "# --- handoff unpack (only for keys that should exist as inputs) ---\n",
    "optional_from_04_1 = [\n",
    "    'false_negatives_df',\n",
    "    'fn_features_df',\n",
    "    'brand_keywords',\n",
    "    'cert_full_info_map',\n",
    "]\n",
    "generated_in_04_2 = [\n",
    "    'DANGEROUS_TLDS',\n",
    "    'LEGITIMATE_TLDS',\n",
    "    'NEUTRAL_TLDS',\n",
    "]\n",
    "\n",
    "# Unpack what is present (inputs)\n",
    "if 'handoff' in globals() and isinstance(handoff, dict):\n",
    "    for k in optional_from_04_1:\n",
    "        if k in handoff and globals().get(k) is None:\n",
    "            globals()[k] = handoff[k]\n",
    "\n",
    "missing_input = [k for k in optional_from_04_1 if globals().get(k) is None]\n",
    "if missing_input:\n",
    "    print(f\"â„¹ï¸ optional inputs missing at load-time: {missing_input} â€” trying to backfill (lineage â†’ 04-1 â†’ cfg).\")\n",
    "    # --- Backfill from lineage â†’ 04-1 handoff ---\n",
    "    def _resolve_04_1_from_lineage() -> Optional[Path]:\n",
    "        ptrs = [Path('artifacts/_current_lineage.json'), Path(compat_base_dirs['handoff']) / '_lineage.json']\n",
    "        for ptr in ptrs:\n",
    "            try:\n",
    "                if ptr.exists():\n",
    "                    data = json.loads(ptr.read_text(encoding='utf-8'))\n",
    "                    # Handle common shapes: items[-1] or last\n",
    "                    candidates = []\n",
    "                    if isinstance(data, dict):\n",
    "                        if 'last' in data and isinstance(data['last'], dict) and 'path' in data['last']:\n",
    "                            candidates.append(Path(data['last']['path']).parent / '04-1_config_and_data_preparation.pkl')\n",
    "                        if 'items' in data:\n",
    "                            for it in data['items']:\n",
    "                                if isinstance(it, dict) and it.get('type') == '04-1_handoff' and 'path' in it:\n",
    "                                    candidates.append(Path(it['path']))\n",
    "                    for cp in candidates:\n",
    "                        if cp.exists():\n",
    "                            return cp\n",
    "            except Exception as e:\n",
    "                print(f\"[Auto] lineage scan error: {e}\")\n",
    "        return None\n",
    "\n",
    "    h1 = _resolve_04_1_from_lineage()\n",
    "    if h1 and h1.exists():\n",
    "        try:\n",
    "            import pickle\n",
    "            with open(h1, 'rb') as f:\n",
    "                h1_payload = pickle.load(f)\n",
    "            for k in missing_input:\n",
    "                if globals().get(k) is None and k in h1_payload:\n",
    "                    globals()[k] = h1_payload[k]\n",
    "            missing_input = [k for k in optional_from_04_1 if globals().get(k) is None]\n",
    "            if missing_input:\n",
    "                print(f\"â„¹ï¸ 04-1 handoff ã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„: {missing_input}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ 04-1 handoff load failed: {e}\")\n",
    "\n",
    "    # --- Backfill from cfg if available ---\n",
    "    if globals().get('cfg'):\n",
    "        if 'brand_keywords' in missing_input and 'brand_keywords' in cfg:\n",
    "            globals()['brand_keywords'] = cfg['brand_keywords']\n",
    "        if 'cert_full_info_map' in missing_input and 'cert_full_info_map' in cfg:\n",
    "            globals()['cert_full_info_map'] = cfg['cert_full_info_map']\n",
    "\n",
    "    missing_input = [k for k in optional_from_04_1 if globals().get(k) is None]\n",
    "    if missing_input:\n",
    "        warnings.warn(f\"optional inputs still missing after backfill: {missing_input} â€” pipeline will continue with safe defaults.\")\n",
    "\n",
    "print(\"â„¹ï¸ keys to be produced in this module:\", generated_in_04_2)\n",
    "print(\"âœ… handoff unpack (partial allowed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46339223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ åˆ†æçµæœã‚’æ´»ã‹ã—ãŸæ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ \n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š åˆ©ç”¨å¯èƒ½ãªçµ±è¨ˆãƒ‡ãƒ¼ã‚¿:\n",
      "----------------------------------------\n",
      "âœ… HIGH_RISK_WORDS: 109å€‹\n",
      "âœ… KNOWN_DOMAINS: 55524ç¨®é¡\n",
      "\n",
      "ğŸ§ª æ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ:\n",
      "----------------------------------------\n",
      "â„¹ï¸ Smoke tests skipped. Set RUN_SMOKE_TESTS=1 to run.\n",
      "\n",
      "ğŸ“Š æ”¹è‰¯ã®åŠ¹æœ:\n",
      "  - å…ƒã®é«˜ãƒªã‚¹ã‚¯å˜èª: 109å€‹\n",
      "  - å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–: 0å€‹\n",
      "  - ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’å„ªå…ˆ: å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯95ç‚¹ä»¥ä¸Š\n",
      "  - TLDçµ±è¨ˆã‚’æ´»ç”¨: 504ç¨®é¡\n",
      "\n",
      "================================================================================\n",
      "âœ… æ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ æ”¹è‰¯ã®ãƒã‚¤ãƒ³ãƒˆ:\n",
      "  1. å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ10,695ä»¶ï¼‰ã‚’æœ€å„ªå…ˆã§æ¤œå‡º\n",
      "  2. åŸ‹ã‚è¾¼ã¿å½è£…ï¼ˆ1,190ä»¶ï¼‰ã‚’ç¢ºå®Ÿã«æ¤œå‡º\n",
      "  3. 'jp'ã‚„'co'ã¯æ–‡è„ˆã‚’è¦‹ã¦åˆ¤æ–­\n",
      "  4. wwwã‚’å®Œå…¨ã«é™¤å¤–ï¼ˆ59,046ä»¶ã®èª¤æ¤œå‡ºã‚’é˜²ãï¼‰\n",
      "  5. IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ­£ã—ãèªè­˜\n",
      "  6. TLDçµ±è¨ˆï¼ˆ810ç¨®é¡ï¼‰ã‚’æ´»ç”¨\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 04-IMPROVED\n",
    "æ¦‚è¦: åˆ†æçµæœã‚’æ´»ã‹ã—ãŸæ”¹è‰¯ç‰ˆåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯\n",
    "å…¥åŠ›: HIGH_RISK_WORDS, TLD_STATS, KNOWN_DOMAINSï¼ˆã‚»ãƒ«04ã®å®Ÿè¡Œçµæœï¼‰\n",
    "å‡ºåŠ›: æ”¹è‰¯ç‰ˆã®åˆ¤å®šé–¢æ•°ç¾¤\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import math\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸš€ åˆ†æçµæœã‚’æ´»ã‹ã—ãŸæ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ \")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== 1. åˆ†æçµæœã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\nğŸ“Š åˆ©ç”¨å¯èƒ½ãªçµ±è¨ˆãƒ‡ãƒ¼ã‚¿:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# å¿…è¦ãªå¤‰æ•°ã®ç¢ºèª\n",
    "required_vars = {\n",
    "    'HIGH_RISK_WORDS': 'é«˜ãƒªã‚¹ã‚¯å˜èªãƒªã‚¹ãƒˆ',\n",
    "    'suspicious_words_stats': 'å˜èªåˆ¥çµ±è¨ˆ',\n",
    "    'TLD_STATS': 'TLDåˆ¥çµ±è¨ˆ',\n",
    "    'KNOWN_DOMAINS': 'åŸ‹ã‚è¾¼ã¿ãƒ‰ãƒ¡ã‚¤ãƒ³çµ±è¨ˆ'\n",
    "}\n",
    "\n",
    "available_vars = {}\n",
    "for var_name, description in required_vars.items():\n",
    "    if var_name in globals():\n",
    "        available_vars[var_name] = globals()[var_name]\n",
    "        if isinstance(globals()[var_name], list):\n",
    "            print(f\"âœ… {var_name}: {len(globals()[var_name])}å€‹\")\n",
    "        elif isinstance(globals()[var_name], dict):\n",
    "            print(f\"âœ… {var_name}: {len(globals()[var_name])}ç¨®é¡\")\n",
    "    else:\n",
    "        print(f\"âŒ {var_name}: æœªå®šç¾© - {description}\")\n",
    "\n",
    "if len(available_vars) < 3:\n",
    "    print(\"\\nâš ï¸ ã‚»ãƒ«04ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 2. æ”¹è‰¯ç‰ˆã®ç‰¹å¾´æŠ½å‡ºé–¢æ•° ==========\n",
    "\n",
    "def extract_features_improved(domain: str) -> Dict:\n",
    "    \"\"\"\n",
    "    åˆ†æçµæœã‚’æ´»ã‹ã—ãŸæ”¹è‰¯ç‰ˆç‰¹å¾´æŠ½å‡º\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'clean_words': [],      # é€šå¸¸ã®å˜èªï¼ˆå•é¡Œã®ã‚ã‚‹ã‚‚ã®ã‚’é™¤å¤–ï¼‰\n",
    "            'risk_patterns': [],    # æ¤œå‡ºã•ã‚ŒãŸãƒªã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "            'risk_scores': {},      # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢\n",
    "            'tld_info': {},        # TLDæƒ…å ±\n",
    "            'structure': {}        # æ§‹é€ æƒ…å ±\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {\n",
    "        'clean_words': [],\n",
    "        'risk_patterns': [],\n",
    "        'risk_scores': {},\n",
    "        'tld_info': {},\n",
    "        'structure': {}\n",
    "    }\n",
    "    \n",
    "    if not domain:\n",
    "        return features\n",
    "    \n",
    "    domain_lower = domain.lower().strip()\n",
    "    \n",
    "    # ========== IPã‚¢ãƒ‰ãƒ¬ã‚¹ãƒã‚§ãƒƒã‚¯ ==========\n",
    "    # æ•°å­—ã®TLDã‚’é˜²ã\n",
    "    if re.match(r'^[\\d\\.]+$', domain_lower):\n",
    "        features['risk_patterns'].append('ip_address')\n",
    "        features['risk_scores']['ip_address'] = 30\n",
    "        return features\n",
    "    \n",
    "    # ========== å‰å‡¦ç† ==========\n",
    "    # www.ã‚’é™¤å»ï¼ˆ59,046ä»¶ã®èª¤æ¤œå‡ºã‚’é˜²ãï¼‰\n",
    "    if domain_lower.startswith('www.'):\n",
    "        domain_lower = domain_lower[4:]\n",
    "        features['structure']['has_www'] = True\n",
    "    \n",
    "    # ========== TLDæŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ==========\n",
    "    if 'extract_real_tld' in globals():\n",
    "        tld_info = extract_real_tld(domain_lower)\n",
    "        features['tld_info'] = tld_info\n",
    "        domain_body = tld_info.get('domain_body', domain_lower)\n",
    "    else:\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "        parts = domain_lower.split('.')\n",
    "        if len(parts) >= 2:\n",
    "            domain_body = '.'.join(parts[:-1])\n",
    "            features['tld_info'] = {'tld': parts[-1], 'domain_body': domain_body}\n",
    "        else:\n",
    "            domain_body = domain_lower\n",
    "            features['tld_info'] = {'tld': '', 'domain_body': domain_body}\n",
    "    \n",
    "    # ========== æ§‹é€ åˆ†æ ==========\n",
    "    features['structure']['depth'] = domain_lower.count('.') + 1\n",
    "    features['structure']['length'] = len(domain_lower)\n",
    "    features['structure']['hyphen_count'] = domain_lower.count('-')\n",
    "    \n",
    "    # ========== ãƒªã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆå„ªå…ˆåº¦é †ï¼‰ ==========\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç¢ºå®Ÿãªå½è£…ï¼ˆæœ€å„ªå…ˆï¼‰\n",
    "    if '.co.jp.' in domain_lower and not domain_lower.endswith('.co.jp'):\n",
    "        features['risk_patterns'].append('fake_cojp_absolute')\n",
    "        features['risk_scores']['fake_cojp_absolute'] = 95\n",
    "    \n",
    "    if '.com.' in domain_lower and not domain_lower.endswith('.com'):\n",
    "        features['risk_patterns'].append('fake_com_absolute')\n",
    "        features['risk_scores']['fake_com_absolute'] = 90\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ—¢çŸ¥ãƒ‰ãƒ¡ã‚¤ãƒ³ã®åŸ‹ã‚è¾¼ã¿ï¼ˆé«˜å„ªå…ˆï¼‰\n",
    "    if 'KNOWN_DOMAINS' in globals():\n",
    "        for known_domain in KNOWN_DOMAINS.keys():\n",
    "            if known_domain in domain_lower and not domain_lower.endswith(known_domain):\n",
    "                pattern_name = f'embedded_{known_domain.replace(\".\", \"_\")}'\n",
    "                features['risk_patterns'].append(pattern_name)\n",
    "                # åŸ‹ã‚è¾¼ã¿å›æ•°ã«åŸºã¥ãã‚¹ã‚³ã‚¢ï¼ˆKNOWN_DOMAINS ã®å‹æºã‚Œã«è€ãˆã‚‹ï¼‰\n",
    "                # CHANGELOG: 2025-12-28 bool is not subscriptable å¯¾ç­–ï¼ˆstats dict ä»¥å¤–ã¯ 0æ‰±ã„ï¼‰\n",
    "                embed_stats = KNOWN_DOMAINS.get(known_domain, {})\n",
    "                phish_embedded = embed_stats.get('phishing_embedded', 0) if isinstance(embed_stats, dict) else 0\n",
    "                if phish_embedded > 1000:\n",
    "                    features['risk_scores'][pattern_name] = 90\n",
    "                elif phish_embedded > 500:\n",
    "                    features['risk_scores'][pattern_name] = 85\n",
    "                elif phish_embedded > 100:\n",
    "                    features['risk_scores'][pattern_name] = 80\n",
    "                else:\n",
    "                    features['risk_scores'][pattern_name] = 70\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒã‚¤ãƒ•ãƒ³å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "    jp_hyphen_patterns = ['-jp.', '.jp-', '-jp-', 'jp-']\n",
    "    co_hyphen_patterns = ['-co.', '.co-', '-co-', 'co-']\n",
    "    \n",
    "    for pattern in jp_hyphen_patterns:\n",
    "        if pattern in domain_body:\n",
    "            features['risk_patterns'].append('jp_hyphen_variant')\n",
    "            features['risk_scores']['jp_hyphen_variant'] = 60\n",
    "            break\n",
    "    \n",
    "    for pattern in co_hyphen_patterns:\n",
    "        if pattern in domain_body:\n",
    "            features['risk_patterns'].append('co_hyphen_variant')\n",
    "            features['risk_scores']['co_hyphen_variant'] = 55\n",
    "            break\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: DuckDNSï¼ˆæœ€å±é™ºï¼‰\n",
    "    if 'duckdns' in domain_lower:\n",
    "        features['risk_patterns'].append('duckdns')\n",
    "        features['risk_scores']['duckdns'] = 100\n",
    "    \n",
    "    # ========== å˜èªæŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ==========\n",
    "    words_raw = re.findall(r'[a-z]+', domain_body)\n",
    "    \n",
    "    # æ¡ä»¶ä»˜ããƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "    for word in words_raw:\n",
    "        # å˜ç‹¬ã®'jp'ã‚„'co'ã®æ‰±ã„\n",
    "        if word == 'jp':\n",
    "            # å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆã®ã¿ãƒªã‚¹ã‚¯ã¨ã—ã¦æ‰±ã†\n",
    "            if any(p in features['risk_patterns'] for p in \n",
    "                   ['fake_cojp_absolute', 'jp_hyphen_variant']):\n",
    "                features['risk_patterns'].append('jp_in_suspicious_context')\n",
    "                features['risk_scores']['jp_in_suspicious_context'] = 40\n",
    "            # ãã‚Œä»¥å¤–ã¯ç„¡è¦–ï¼ˆæ­£å¸¸ãªä½¿ç”¨ã®å¯èƒ½æ€§ï¼‰\n",
    "            \n",
    "        elif word == 'co':\n",
    "            # å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆã®ã¿ãƒªã‚¹ã‚¯ã¨ã—ã¦æ‰±ã†\n",
    "            if any(p in features['risk_patterns'] for p in \n",
    "                   ['fake_cojp_absolute', 'co_hyphen_variant']):\n",
    "                features['risk_patterns'].append('co_in_suspicious_context')\n",
    "                features['risk_scores']['co_in_suspicious_context'] = 35\n",
    "            # ãã‚Œä»¥å¤–ã¯ç„¡è¦–\n",
    "            \n",
    "        elif word == 'www':\n",
    "            # wwwã¯å®Œå…¨ã«ç„¡è¦–ï¼ˆæ—¢ã«é™¤å»æ¸ˆã¿ï¼‰\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "            # é€šå¸¸ã®å˜èª\n",
    "            if 2 <= len(word) <= 15:\n",
    "                features['clean_words'].append(word)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# ========== 3. ãƒªã‚¹ã‚¯è©•ä¾¡é–¢æ•°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ==========\n",
    "\n",
    "def calculate_risk_score_improved(domain: str) -> Dict:\n",
    "    \"\"\"\n",
    "    åˆ†æçµæœã‚’æ´»ã‹ã—ãŸç·åˆãƒªã‚¹ã‚¯è©•ä¾¡\n",
    "    \"\"\"\n",
    "    \n",
    "    features = extract_features_improved(domain)\n",
    "    \n",
    "    result = {\n",
    "        'domain': domain,\n",
    "        'total_risk': 0,\n",
    "        'confidence': 0,\n",
    "        'risk_breakdown': {},\n",
    "        'verdict': 'unknown',\n",
    "        'explanation': []\n",
    "    }\n",
    "    \n",
    "    # ========== ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¹ã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰ ==========\n",
    "    pattern_risk = 0\n",
    "    if features['risk_patterns']:\n",
    "        # æœ€ã‚‚é«˜ã„ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã‚’æ¡ç”¨\n",
    "        pattern_risk = max(features['risk_scores'].values())\n",
    "        result['risk_breakdown']['pattern'] = pattern_risk\n",
    "        \n",
    "        # èª¬æ˜ã‚’è¿½åŠ \n",
    "        for pattern in features['risk_patterns']:\n",
    "            if pattern == 'fake_cojp_absolute':\n",
    "                result['explanation'].append(\"ç¢ºå®Ÿãª.co.jpå½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³\")\n",
    "            elif pattern.startswith('embedded_'):\n",
    "                domain_name = pattern.replace('embedded_', '').replace('_', '.')\n",
    "                result['explanation'].append(f\"{domain_name}ã®åŸ‹ã‚è¾¼ã¿å½è£…\")\n",
    "            elif pattern == 'duckdns':\n",
    "                result['explanation'].append(\"DuckDNSï¼ˆãƒ•ãƒªãƒ¼DNSï¼‰ä½¿ç”¨\")\n",
    "    \n",
    "    # ========== TLDãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¹ã‚¯ ==========\n",
    "    tld_risk = 0\n",
    "    if 'TLD_STATS' in globals() and features['tld_info'].get('tld'):\n",
    "        tld = features['tld_info']['tld']\n",
    "        if tld in TLD_STATS:\n",
    "            stats = TLD_STATS[tld]\n",
    "            odds_ratio = stats['odds_ratio']\n",
    "            \n",
    "            if odds_ratio == float('inf') or odds_ratio > 100:\n",
    "                tld_risk = 40\n",
    "                result['explanation'].append(f\"å±é™ºTLD: .{tld}\")\n",
    "            elif odds_ratio > 10:\n",
    "                tld_risk = 25\n",
    "                result['explanation'].append(f\"è¦æ³¨æ„TLD: .{tld}\")\n",
    "            elif odds_ratio < 0.1:\n",
    "                tld_risk = -20  # å®‰å…¨ãªTLDã¯ãƒªã‚¹ã‚¯ã‚’ä¸‹ã’ã‚‹\n",
    "                result['explanation'].append(f\"ä¿¡é ¼TLD: .{tld}\")\n",
    "    \n",
    "    result['risk_breakdown']['tld'] = tld_risk\n",
    "    \n",
    "    # ========== å˜èªãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¹ã‚¯ï¼ˆè£œåŠ©çš„ï¼‰ ==========\n",
    "    word_risk = 0\n",
    "    if 'suspicious_words_stats' in globals() and features['clean_words']:\n",
    "        risk_words = []\n",
    "        for word in features['clean_words']:\n",
    "            if word in suspicious_words_stats:\n",
    "                stats = suspicious_words_stats[word]\n",
    "                if stats['odds_ratio'] > 50:\n",
    "                    risk_words.append(word)\n",
    "                    word_risk += 5\n",
    "        \n",
    "        if risk_words:\n",
    "            word_risk = min(word_risk, 30)  # æœ€å¤§30ç‚¹\n",
    "            result['risk_breakdown']['words'] = word_risk\n",
    "            result['explanation'].append(f\"å±é™ºå˜èª: {', '.join(risk_words[:3])}\")\n",
    "    \n",
    "    # ========== æ§‹é€ çš„ãƒªã‚¹ã‚¯ ==========\n",
    "    structure_risk = 0\n",
    "    if features['structure']['depth'] > 4:\n",
    "        structure_risk += 15\n",
    "        result['explanation'].append(f\"éšå±¤ãŒæ·±ã„: {features['structure']['depth']}å±¤\")\n",
    "    \n",
    "    if features['structure']['length'] > 50:\n",
    "        structure_risk += 10\n",
    "        result['explanation'].append(f\"ç•°å¸¸ã«é•·ã„: {features['structure']['length']}æ–‡å­—\")\n",
    "    \n",
    "    result['risk_breakdown']['structure'] = structure_risk\n",
    "    \n",
    "    # ========== ç·åˆè©•ä¾¡ ==========\n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ã‚¯ã‚’é‡è¦–\n",
    "    if pattern_risk >= 80:\n",
    "        # ç¢ºå®Ÿãªå½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        result['total_risk'] = pattern_risk\n",
    "        result['confidence'] = 0.95\n",
    "    elif pattern_risk >= 60:\n",
    "        # é«˜ãƒªã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ + ä»–ã®è¦å› \n",
    "        result['total_risk'] = pattern_risk + (tld_risk + word_risk + structure_risk) * 0.3\n",
    "        result['confidence'] = 0.80\n",
    "    else:\n",
    "        # é€šå¸¸ã®é‡ã¿ä»˜ã‘\n",
    "        result['total_risk'] = pattern_risk * 0.5 + tld_risk * 0.3 + word_risk * 0.15 + structure_risk * 0.05\n",
    "        result['confidence'] = min(0.7, result['total_risk'] / 100)\n",
    "    \n",
    "    # æ­£è¦åŒ–\n",
    "    result['total_risk'] = min(100, max(0, result['total_risk']))\n",
    "    \n",
    "    # åˆ¤å®š\n",
    "    if result['total_risk'] >= 60:\n",
    "        result['verdict'] = 'phishing'\n",
    "    elif result['total_risk'] <= 30:\n",
    "        result['verdict'] = 'legitimate'\n",
    "    else:\n",
    "        result['verdict'] = 'suspicious'\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ========== 4. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ==========\n",
    "\n",
    "print(\"\\nğŸ§ª æ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_cases = [\n",
    "    # ç¢ºå®Ÿãªãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°\n",
    "    'amazon.co.jp.suppor34.frhnkm.ph',\n",
    "    'amazon-co-jp.com',\n",
    "    'rakuten-jp.net',\n",
    "    \n",
    "    # æ­£è¦ã‚µã‚¤ãƒˆ\n",
    "    'kobe-denki.co.jp',\n",
    "    'amazon.co.jp',\n",
    "    'google.com',\n",
    "    \n",
    "    # ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³\n",
    "    'jp-bank.com',\n",
    "    'secure-payment.tk',\n",
    "    'duckdns.org'\n",
    "]\n",
    "\n",
    "\n",
    "# ========== Smoke test (optional) ==========\n",
    "# CHANGELOG: 2025-12-28 04-2 ã®ç”Ÿæˆç‰©ã«å¿…é ˆã§ã¯ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯å®Ÿè¡Œã—ãªã„\n",
    "import os as _os\n",
    "_run_smoke = str(_os.getenv(\"RUN_SMOKE_TESTS\", \"0\")).lower() in (\"1\", \"true\", \"yes\") or bool(globals().get(\"DEVELOPMENT_MODE\", False))\n",
    "if not _run_smoke:\n",
    "    print(\"â„¹ï¸ Smoke tests skipped. Set RUN_SMOKE_TESTS=1 to run.\")\n",
    "else:\n",
    "    for domain in test_cases:\n",
    "        result = calculate_risk_score_improved(domain)\n",
    "    \n",
    "        # åˆ¤å®šãƒãƒ¼ã‚¯\n",
    "        if result['verdict'] == 'phishing':\n",
    "            mark = 'ğŸš¨'\n",
    "        elif result['verdict'] == 'legitimate':\n",
    "            mark = 'âœ…'\n",
    "        else:\n",
    "            mark = 'âš ï¸'\n",
    "    \n",
    "        print(f\"\\n{domain}\")\n",
    "        print(f\"  {mark} åˆ¤å®š: {result['verdict']} (ãƒªã‚¹ã‚¯: {result['total_risk']:.1f}, ä¿¡é ¼åº¦: {result['confidence']:.2f})\")\n",
    "        if result['explanation']:\n",
    "            print(f\"  ç†ç”±: {'; '.join(result['explanation'][:3])}\")\n",
    "# ========== 5. çµ±è¨ˆã‚µãƒãƒªãƒ¼ ==========\n",
    "\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    # å•é¡Œã®ã‚ã‚‹å˜èªã®é™¤å¤–ç¢ºèª\n",
    "    problematic_words = ['jp', 'co', 'www']\n",
    "    clean_high_risk = [w for w in HIGH_RISK_WORDS if w not in problematic_words]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š æ”¹è‰¯ã®åŠ¹æœ:\")\n",
    "    print(f\"  - å…ƒã®é«˜ãƒªã‚¹ã‚¯å˜èª: {len(HIGH_RISK_WORDS)}å€‹\")\n",
    "    print(f\"  - å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–: {len(HIGH_RISK_WORDS) - len(clean_high_risk)}å€‹\")\n",
    "    print(f\"  - ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’å„ªå…ˆ: å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯95ç‚¹ä»¥ä¸Š\")\n",
    "    print(f\"  - TLDçµ±è¨ˆã‚’æ´»ç”¨: {len(TLD_STATS) if 'TLD_STATS' in globals() else 0}ç¨®é¡\")\n",
    "\n",
    "# ========== 6. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ ==========\n",
    "\n",
    "globals()['extract_features_improved'] = extract_features_improved\n",
    "globals()['calculate_risk_score_improved'] = calculate_risk_score_improved\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… æ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ’¡ æ”¹è‰¯ã®ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"  1. å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ10,695ä»¶ï¼‰ã‚’æœ€å„ªå…ˆã§æ¤œå‡º\")\n",
    "print(\"  2. åŸ‹ã‚è¾¼ã¿å½è£…ï¼ˆ1,190ä»¶ï¼‰ã‚’ç¢ºå®Ÿã«æ¤œå‡º\")\n",
    "print(\"  3. 'jp'ã‚„'co'ã¯æ–‡è„ˆã‚’è¦‹ã¦åˆ¤æ–­\")\n",
    "print(\"  4. wwwã‚’å®Œå…¨ã«é™¤å¤–ï¼ˆ59,046ä»¶ã®èª¤æ¤œå‡ºã‚’é˜²ãï¼‰\")\n",
    "print(\"  5. IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ­£ã—ãèªè­˜\")\n",
    "print(\"  6. TLDçµ±è¨ˆï¼ˆ810ç¨®é¡ï¼‰ã‚’æ´»ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8875305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: artifacts/2026-01-10_140940/handoff/04-2_statistical_analysis.pkl\n",
      "âœ… Pointers updated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1197400/1680315291.py:73: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"updated_at\": datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: handoff ä¿å­˜ & ãƒã‚¤ãƒ³ã‚¿æ›´æ–°ï¼ˆå‡ºåŠ›: 04-2ï¼‰ ===\n",
    "from pathlib import Path\n",
    "import cloudpickle\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "handoff_out = Path(output_dirs['handoff']) / '04-2_statistical_analysis.pkl'\n",
    "\n",
    "# pass-through (from 04-1) ãŒã‚ã‚Œã°ç¢ºå®Ÿã«å¼•ãç¶™ã\n",
    "_pt_brand_keywords = globals().get(\"brand_keywords\", [])\n",
    "_pt_cert_map = globals().get(\"cert_full_info_map\", {})\n",
    "_pt_fn_df = globals().get(\"false_negatives_df\", None)\n",
    "_pt_features_df = globals().get(\"fn_features_df\", None)  # ã‚ã‚Œã°åŒæ¢±\n",
    "\n",
    "handoff_04_2 = {\n",
    "    # --- meta ---\n",
    "    \"cfg\": cfg,\n",
    "    \"RUN_ID\": RUN_ID,\n",
    "    \"SESSION_ID\": SESSION_ID,\n",
    "    \"output_dirs\": output_dirs,\n",
    "    # --- pass-through from 04-1 ---\n",
    "    \"brand_keywords\": _pt_brand_keywords,\n",
    "    \"cert_full_info_map\": _pt_cert_map,\n",
    "    \"false_negatives_df\": _pt_fn_df,\n",
    "    \"fn_features_df\": _pt_features_df,\n",
    "    # --- 04-2 artifacts ---\n",
    "    \"HIGH_RISK_WORDS\": globals().get(\"HIGH_RISK_WORDS\"),\n",
    "    \"suspicious_words_stats\": globals().get(\"suspicious_words_stats\"),\n",
    "    \"TLD_STATS\": globals().get(\"TLD_STATS\"),\n",
    "    \"KNOWN_DOMAINS\": globals().get(\"KNOWN_DOMAINS\"),\n",
    "    \"DANGEROUS_TLDS\": globals().get(\"DANGEROUS_TLDS\"),\n",
    "    \"LEGITIMATE_TLDS\": globals().get(\"LEGITIMATE_TLDS\"),\n",
    "    \"NEUTRAL_TLDS\": globals().get(\"NEUTRAL_TLDS\"),\n",
    "    \"extract_features_improved\": globals().get(\"extract_features_improved\"),\n",
    "    \"calculate_risk_score_improved\": globals().get(\"calculate_risk_score_improved\"),\n",
    "}\n",
    "\n",
    "handoff_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(handoff_out, 'wb') as f:\n",
    "    cloudpickle.dump(handoff_04_2, f)\n",
    "\n",
    "print(f\"âœ… Saved: {handoff_out}\")\n",
    "\n",
    "def _append_lineage(json_path: Path, entry: dict):\n",
    "    data = None\n",
    "    try:\n",
    "        if json_path.exists():\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "    except Exception:\n",
    "        data = None\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        data.append(entry)\n",
    "    elif isinstance(data, dict):\n",
    "        items = data.get(\"items\", [])\n",
    "        if not isinstance(items, list):\n",
    "            items = []\n",
    "        items.append(entry)\n",
    "        data[\"items\"] = items\n",
    "        data[\"last\"] = {\"type\": entry.get(\"type\"), \"path\": entry.get(\"path\")}\n",
    "    else:\n",
    "        data = {\"items\": [entry], \"last\": {\"type\": entry.get(\"type\"), \"path\": entry.get(\"path\")}}\n",
    "\n",
    "    json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "entry = {\n",
    "    \"type\": \"04-2_handoff\",\n",
    "    \"path\": str(handoff_out),\n",
    "    \"RUN_ID\": RUN_ID,\n",
    "    \"updated_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "}\n",
    "_append_lineage(Path(\"artifacts/_current_lineage.json\"), entry)\n",
    "_append_lineage(Path(output_dirs['handoff']) / \"_lineage.json\", entry)\n",
    "print(\"âœ… Pointers updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e422c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_ID: 2026-01-10_140940\n",
      "SESSION_ID: 2026-01-10_140940\n",
      "handoff_in source: strict\n",
      "handoff_in path: artifacts/2026-01-10_140940/handoff/04-1_config_and_data_preparation.pkl\n",
      "handoff_out: artifacts/2026-01-10_140940/handoff/04-2_statistical_analysis.pkl\n",
      "cfg keys (head): ['DB_CONFIG', 'analysis', 'brand_keywords', 'db', 'engine', 'free_ca_list', 'full_processing', 'handoff', 'llm', 'model', 'output_dirs', 'paths', 'run_id', 'session_id', 'system', 'tld_analysis', 'visualization']\n",
      "âœ… Required artifacts present.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Summary ===\n",
    "from pathlib import Path\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"SESSION_ID:\", SESSION_ID)\n",
    "print(\"handoff_in source:\", globals().get(\"handoff_source\"))\n",
    "print(\"handoff_in path:\", globals().get(\"handoff_in_path\"))\n",
    "print(\"handoff_out:\", str(Path(output_dirs['handoff']) / '04-2_statistical_analysis.pkl'))\n",
    "print(\"cfg keys (head):\", sorted(list(cfg.keys()))[:20])\n",
    "missing = [k for k in [\"HIGH_RISK_WORDS\",\"TLD_STATS\",\"extract_features_improved\",\"calculate_risk_score_improved\"] if globals().get(k) is None]\n",
    "if missing:\n",
    "    print(\"âš ï¸ Missing generated keys:\", missing)\n",
    "else:\n",
    "    print(\"âœ… Required artifacts present.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
