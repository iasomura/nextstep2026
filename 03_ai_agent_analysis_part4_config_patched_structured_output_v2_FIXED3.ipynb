{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38643ada-3dc9-452b-826f-7be38bcb164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 0 (02ä»¥é™ å…±é€š): ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰è§£æ±ºã—ã¦ paths ã‚’èª­ã‚€ ===\n",
    "import run_id_registry as runreg\n",
    "rid = runreg.bootstrap()  # envâ†’ãƒ•ã‚¡ã‚¤ãƒ«(artifacts/_current/run_id.txt)â†’Part3â†’latestâ†’æ–°è¦ ã®é †ã§è§£æ±º\n",
    "\n",
    "import importlib\n",
    "import _compat.paths as paths\n",
    "importlib.reload(paths)\n",
    "importlib.reload(paths)\n",
    "print(\"[NX] RUN_ID =\", rid, \"| paths.RUN_ID =\", paths.RUN_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698767bb",
   "metadata": {
    "title": "Cell 1: Config loader (NEW)"
   },
   "outputs": [],
   "source": [
    "# === Cell 1: Config loader (NEW) ===\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict, Any\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import yaml\n",
    "except Exception:\n",
    "    yaml = None\n",
    "\n",
    "def _deep_update(a: dict, b: dict) -> dict:\n",
    "    a = dict(a or {})\n",
    "    for k, v in (b or {}).items():\n",
    "        if isinstance(v, dict) and isinstance(a.get(k), dict):\n",
    "            a[k] = _deep_update(a[k], v)\n",
    "        else:\n",
    "            a[k] = v\n",
    "    return a\n",
    "\n",
    "def load_configuration(config_path: Optional[str] = None,\n",
    "                       cfg_override: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Part4ç”¨ Configçµ±åˆã€‚defaults -> file -> env -> override ã®é †ã§ãƒãƒ¼ã‚¸ã—æ¤œè¨¼ã€‚\"\"\"\n",
    "    defaults = {\n",
    "        \"system\": {\n",
    "            \"cert_only_mode\": False,\n",
    "            \"development_mode\": False,\n",
    "            \"seed\": 42,\n",
    "        },\n",
    "        \"llm\": {\n",
    "            \"provider\": \"vllm\",\n",
    "            \"vllm_base_url\": \"http://192.168.100.71:30000/v1\",\n",
    "            \"ollama_base_url\": \"http://localhost:11434/v1\",\n",
    "            \"model\": \"Qwen/Qwen3-14B-FP8\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 2048,\n",
    "            \"timeout\": 60,\n",
    "            \"max_concurrent\": 10,\n",
    "            \"enabled\": True,\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"sample_size\": 50,\n",
    "            \"batch_size\": 10,\n",
    "            \"enable_brand_detection\": True,\n",
    "            \"enable_tld_analysis\": True,\n",
    "            \"enable_domain_analysis\": True,\n",
    "        },\n",
    "        \"db\": {\n",
    "            \"dbname\": \"rapids_data\",\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"asomura\",\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": \"5432\",\n",
    "            \"read_only\": True,\n",
    "        },\n",
    "        \"paths\": {\n",
    "            \"artifacts_base\": \"artifacts\",\n",
    "            \"handoff_dir\": \"handoff\",\n",
    "            \"results_dir\": \"results\",\n",
    "            \"logs_dir\": \"logs\",\n",
    "        }\n",
    "    }\n",
    "    cfg = dict(defaults)\n",
    "\n",
    "    def _read_file(p: str) -> dict:\n",
    "        if not p or not os.path.exists(p): return {}\n",
    "        try:\n",
    "            text = Path(p).read_text(encoding=\"utf-8\")\n",
    "            if p.endswith((\".yml\",\".yaml\")) and yaml:\n",
    "                return yaml.safe_load(text) or {}\n",
    "            return json.loads(text)\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    if config_path:\n",
    "        cfg = _deep_update(cfg, _read_file(config_path))\n",
    "\n",
    "    env_map = {\n",
    "        \"DEV_MODE\": (\"system\",\"development_mode\"),\n",
    "        \"LLM_PROVIDER\": (\"llm\",\"provider\"),\n",
    "        \"VLLM_BASE_URL\": (\"llm\",\"vllm_base_url\"),\n",
    "    }\n",
    "    for env, path in env_map.items():\n",
    "        if env in os.environ and os.environ[env] != \"\":\n",
    "            val = os.environ[env]\n",
    "            val = True if str(val).lower()==\"true\" else False if str(val).lower()==\"false\" else val\n",
    "            node = cfg\n",
    "            for k in path[:-1]:\n",
    "                node = node.setdefault(k, {})\n",
    "            node[path[-1]] = val\n",
    "\n",
    "    if cfg_override:\n",
    "        cfg = _deep_update(cfg, cfg_override)\n",
    "\n",
    "    if str(cfg[\"llm\"][\"provider\"]).lower() not in (\"vllm\",\"ollama\"):\n",
    "        cfg[\"llm\"][\"provider\"] = \"vllm\"\n",
    "    if not (cfg[\"analysis\"][\"enable_brand_detection\"] or cfg[\"analysis\"][\"enable_tld_analysis\"] or cfg[\"analysis\"][\"enable_domain_analysis\"]):\n",
    "        print(\"âš ï¸ ã™ã¹ã¦ã®åˆ†æãƒ•ãƒ©ã‚°ãŒFalseã§ã™ã€‚å°‘ãªãã¨ã‚‚1ã¤ã¯æœ‰åŠ¹åŒ–ã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "    globals()[\"cfg\"] = cfg\n",
    "    print(f\"âœ… Config loaded: provider={cfg['llm']['provider']}, dev={cfg['system']['development_mode']}, \"\n",
    "          f\"brand={cfg['analysis']['enable_brand_detection']}, tld={cfg['analysis']['enable_tld_analysis']}, \"\n",
    "          f\"domain={cfg['analysis']['enable_domain_analysis']}\")\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b829e0-7344-4fbf-a0bc-e9e27b890a32",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Config loaded: provider=vllm, dev=False, brand=True, tld=True, domain=True\n",
      "ğŸ”§ Development Mode: False\n",
      "ğŸ”§ Artifacts: artifacts / RUN_ID=(auto)\n",
      "ğŸ” detected part3.pkl: ./artifacts/2025-10-19_124653/handoff/03_ai_agent_analysis_part3.pkl\n",
      "ğŸ¯ RUN_ID = 2025-10-19_124653\n",
      "HANDOFF_DIR = artifacts/2025-10-19_124653/handoff\n",
      "RESULTS_DIR = artifacts/2025-10-19_124653/results\n",
      "MODELS_DIR  = artifacts/2025-10-19_124653/models\n",
      "âœ… handoff loaded with keys: ['DANGEROUS_TLDS', 'DB_CONFIG', 'LEGITIMATE_TLDS', 'NEUTRAL_TLDS', 'brand_keywords', 'cert_full_info_map', 'cfg', 'false_negatives_df', 'fn_features_df', 'phishing_tld_stats', 'trusted_tld_stats']\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Apply config (NEW) ===\n",
    "if 'cfg' not in globals():\n",
    "    cfg = load_configuration(os.getenv(\"CONFIG_PATH\") or None, None)\n",
    "\n",
    "DEV_MODE = bool(cfg[\"system\"][\"development_mode\"])\n",
    "print(f\"ğŸ”§ Development Mode: {DEV_MODE}\")\n",
    "print(f\"ğŸ”§ Artifacts: {cfg.get('paths',{}).get('artifacts_base','artifacts')} / RUN_ID={globals().get('RUN_ID','(auto)')}\")\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# === RUN_ID normalize (robust; computes p_part3/RUN_ID if missing) ===\n",
    "import os, glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "def _pick_latest(paths):\n",
    "    cand = [p for p in paths if os.path.exists(p)]\n",
    "    return max(cand, key=lambda p: os.path.getmtime(p)) if cand else None\n",
    "\n",
    "def _find_project_roots():\n",
    "    roots = set()\n",
    "    cwd = Path.cwd()\n",
    "    for parent in [cwd, *cwd.parents]:\n",
    "        if (parent / \"artifacts\").exists():\n",
    "            roots.add(str(parent))\n",
    "    common = \"/home/asomura/nextstep\"\n",
    "    if (Path(common) / \"artifacts\").exists():\n",
    "        roots.add(common)\n",
    "    for parent in cwd.parents:\n",
    "        if parent.name == \"nextstep\":\n",
    "            roots.add(str(parent)); break\n",
    "    roots.add(\".\")\n",
    "    return sorted(roots)\n",
    "\n",
    "def _find_part3(run_id=None):\n",
    "    roots = _find_project_roots()\n",
    "    if run_id:\n",
    "        for root in roots:\n",
    "            p = os.path.join(root, \"artifacts\", run_id, \"handoff\", \"03_ai_agent_analysis_part3.pkl\")\n",
    "            if os.path.exists(p):\n",
    "                return p, run_id\n",
    "    globbed = []\n",
    "    for root in roots:\n",
    "        globbed += glob.glob(os.path.join(root, \"artifacts\", \"*\", \"handoff\", \"03_ai_agent_analysis_part3.pkl\"))\n",
    "    latest = _pick_latest(globbed)\n",
    "    resolved = None\n",
    "    if latest:\n",
    "        parts = Path(latest).parts\n",
    "        if \"artifacts\" in parts:\n",
    "            i = parts.index(\"artifacts\")\n",
    "            if i+1 < len(parts): resolved = parts[i+1]\n",
    "    return latest, resolved\n",
    "\n",
    "# 0) Adopt existing globals if present; otherwise compute\n",
    "RUN_ID = (globals().get(\"RUN_ID\") or os.environ.get(\"RUN_ID\") or None)\n",
    "p_part3 = globals().get(\"p_part3\") or None\n",
    "\n",
    "if not p_part3 or not Path(p_part3).exists():\n",
    "    p_part3, resolved = _find_part3(RUN_ID)\n",
    "    if not RUN_ID:\n",
    "        RUN_ID = resolved\n",
    "\n",
    "if not p_part3 or not Path(p_part3).exists():\n",
    "    raise FileNotFoundError(\"part3.pkl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚03-3 ã‚’å…ˆã«å®Ÿè¡Œã™ã‚‹ã‹ã€RUN_ID/ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "if not RUN_ID:\n",
    "    raise AssertionError(\"RUN_ID could not be resolved. æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "print(\"ğŸ” detected part3.pkl:\", p_part3)\n",
    "print(\"ğŸ¯ RUN_ID =\", RUN_ID)\n",
    "\n",
    "# 1) Derive directories from p_part3/RUN_ID\n",
    "parts = Path(p_part3).parts\n",
    "i = parts.index('artifacts')\n",
    "PROJECT_ROOT = str(Path(*parts[:i])) if i > 0 else '.'\n",
    "ARTIFACTS_DIR = Path(PROJECT_ROOT) / 'artifacts' / RUN_ID\n",
    "\n",
    "HANDOFF_DIR = str(ARTIFACTS_DIR / 'handoff')\n",
    "RESULTS_DIR = str(ARTIFACTS_DIR / 'results')\n",
    "MODELS_DIR  = str(ARTIFACTS_DIR / 'models')\n",
    "\n",
    "os.makedirs(HANDOFF_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"HANDOFF_DIR =\", HANDOFF_DIR)\n",
    "print(\"RESULTS_DIR =\", RESULTS_DIR)\n",
    "print(\"MODELS_DIR  =\", MODELS_DIR)\n",
    "\n",
    "# 2) Load handoff if not loaded yet\n",
    "if 'handoff' not in globals():\n",
    "    import joblib\n",
    "    handoff = joblib.load(p_part3)\n",
    "    print(\"âœ… handoff loaded with keys:\", sorted(list(handoff.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830363ce-b95b-4573-8d40-b490eea5ca4e",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… IO guard ready -> artifacts/2025-10-19_124653\n"
     ]
    }
   ],
   "source": [
    "# In[2]:\n",
    "\n",
    "\n",
    "# === IO PATHS (auto-added guard) ===\n",
    "# ã“ã®ã‚»ãƒ«ã¯è‡ªå‹•è¿½åŠ /è£œå¼·ç”¨ã§ã™ã€‚I/O ä»¥å¤–ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯ä¸€åˆ‡å¤‰æ›´ã—ã¾ã›ã‚“ã€‚\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "if 'RUN_ID' not in globals():\n",
    "    RUN_ID = os.environ.get(\"RUN_ID\") or datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "ARTIFACTS = Path(\"artifacts\") / RUN_ID\n",
    "\n",
    "RAW = ARTIFACTS / \"raw\"\n",
    "PROCESSED = ARTIFACTS / \"processed\"\n",
    "MODELS = ARTIFACTS / \"models\"\n",
    "RESULTS = ARTIFACTS / \"results\"\n",
    "HANDOFF = ARTIFACTS / \"handoff\"\n",
    "LOGS = ARTIFACTS / \"logs\"\n",
    "TRACES = ARTIFACTS / \"traces\"\n",
    "\n",
    "for _p in [RAW, PROCESSED, MODELS, RESULTS, HANDOFF, LOGS, TRACES]:\n",
    "    _p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# String shortcuts for os.path.join äº’æ›\n",
    "RAW_DIR = str(RAW); PROCESSED_DIR = str(PROCESSED); MODELS_DIR = str(MODELS)\n",
    "RESULTS_DIR = str(RESULTS); HANDOFF_DIR = str(HANDOFF); LOGS_DIR = str(LOGS); TRACES_DIR = str(TRACES)\n",
    "\n",
    "base_dirs = {\n",
    "    'raw': RAW_DIR,\n",
    "    'data': PROCESSED_DIR,\n",
    "    'models': MODELS_DIR,\n",
    "    'results': RESULTS_DIR,\n",
    "    'handoff': HANDOFF_DIR,\n",
    "    'logs': LOGS_DIR,\n",
    "    'traces': TRACES_DIR,\n",
    "}\n",
    "def resolve(p):\n",
    "    p = Path(p); p.mkdir(parents=True, exist_ok=True); return str(p)\n",
    "def ensure_roots(): pass\n",
    "def load_config(): return {\"root\": str(ARTIFACTS), \"run_id\": RUN_ID}\n",
    "\n",
    "print(f\"âœ… IO guard ready -> artifacts/{RUN_ID}\")\n",
    "\n",
    "\n",
    "# ## Part 4 â€” LLMè¨­å®šãƒ»ãƒ„ãƒ¼ãƒ«å®šç¾©ãƒ»AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰ã¨è©•ä¾¡ï¼ˆã‚»ãƒ«03ã€œ06ç¾¤ï¼‰\n",
    "# \n",
    "# â€»æœ¬ãƒ‘ãƒ¼ãƒˆã¯åŸæ–‡ã®ã‚»ãƒ«ã‚’**ä¸€å­—ä¸€å¥å¤‰æ›´ã›ãš**åéŒ²ã—ã¦ã„ã¾ã™ã€‚è¿½åŠ ã¯ã“ã®è¦‹å‡ºã—ã¨å…¥å‡ºåŠ›ã‚¹ãƒ­ãƒƒãƒˆã®ã¿ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7445e4-a36c-4239-bd8d-9566f83b79c3",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c91b3c-21bd-4fbc-9be1-e6ff4ec07164",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… handoff loaded: artifacts/2025-10-19_124653/handoff/03_ai_agent_analysis_part3.pkl\n"
     ]
    }
   ],
   "source": [
    "# === robust handoff loader (auto-added) ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "os.makedirs(HANDOFF_DIR, exist_ok=True)\n",
    "\n",
    "primary = os.path.join(HANDOFF_DIR, \"03_ai_agent_analysis_part3.pkl\")\n",
    "legacy  = os.path.join(\"handoff\", \"03_ai_agent_analysis_part3.pkl\")\n",
    "\n",
    "_candidates = []\n",
    "if os.path.exists(primary): _candidates.append(primary)\n",
    "if os.path.exists(legacy):  _candidates.append(legacy)\n",
    "\n",
    "if not _candidates:\n",
    "    arts = Path(\"artifacts\")\n",
    "    if arts.exists():\n",
    "        found = sorted(arts.glob(\"*/handoff/03_ai_agent_analysis_part3.pkl\"))\n",
    "        _candidates.extend(str(p) for p in found)\n",
    "\n",
    "if not _candidates:\n",
    "    raise FileNotFoundError(\n",
    "        \"handoff file not found.\\n\"\n",
    "        f\" - tried: {primary}\\n\"\n",
    "        f\" - tried: {legacy}\\n\"\n",
    "        \" - tried: artifacts/*/handoff/03_ai_agent_analysis_part3.pkl\\n\"\n",
    "        \"å¯¾å‡¦: ç›´å‰ãƒ‘ãƒ¼ãƒˆã‚’å…ˆã«å®Ÿè¡Œã™ã‚‹ã‹ã€RUN_ID ã‚’å›ºå®šã—ã¦åŒä¸€ RUN_ID ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\"\n",
    "    )\n",
    "\n",
    "_handoff_path = _candidates[0]\n",
    "handoff = joblib.load(_handoff_path)\n",
    "print(f\"âœ… handoff loaded: {_handoff_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256f4721",
   "metadata": {
    "tags": [
     "structured-output",
     "schema",
     "cell-5.5"
    ]
   },
   "outputs": [],
   "source": [
    "# === Cell 5.5: Structured Output Schema Definition (Pydantic v2) ===\\n# Purpose: Define Pydantic models to structure ALL agent I/O (LLM outputs & tool returns)\\n#          with type-safety and rich validation. This cell is intentionally standalone.\\n#\\n# Notes:\\n# - Field descriptions are in English for better LLM alignment; Japanese comments are for maintainers.\\n# - Backward compatibility: each model exposes `.to_dict()` (alias of `model_dump()`).\\n# - Pydantic v2 API (`field_validator`, `model_validator`) only; v1 is not required in this repo/runtime.\\n#\\n# Ref: https://docs.pydantic.dev/latest/ (v2)\\nfrom __future__ import annotations\\n\\nfrom typing import Optional, List, Dict, Any, Literal\\nfrom datetime import datetime\\n\\nfrom pydantic import BaseModel, Field, field_validator, model_validator, ValidationInfo\\n\\n# -------- Shared literals / limits --------\\nRiskLevelLiteral = Literal[\"low\", \"medium\", \"high\", \"critical\"]\\nActionLiteral    = Literal[\"block\", \"warn\", \"allow\", \"review\"]\\nStepLiteral      = Literal[\"initial\", \"brand_check\", \"cert_check\", \"domain_check\", \"decision\", \"complete\"]\\n\\n_MAX_REASON_LEN = 4096\\n_MAX_STR_LEN    = 256\\n_MAX_LIST_EVIDENCE = 32\\n_MAX_LIST_PATTERNS = 64\\n\\ndef _coerce_float_01(x: Any, default: float = 0.0) -> float:\\n    \"\"\"Coerce to float in [0,1]. If out of range or invalid, clamp to [0,1].\"\"\"\\n    try:\\n        f = float(x)\\n    except Exception:\\n        return default\\n    if f < 0.0:\\n        return 0.0\\n    if f > 1.0:\\n        return 1.0\\n    return f\\n\\nclass _ModelMixin:\\n    \"\"\"Backward-compat mixin to expose dict-like outputs expected by legacy code.\"\"\"\\n    def to_dict(self) -> Dict[str, Any]:\\n        return self.model_dump()\\n\\n    @classmethod\\n    def from_dict(cls, data: Dict[str, Any]):\\n        return cls(**data)\\n\\n# -------- 1) BrandAnalysisResult --------\\nclass BrandAnalysisResult(_ModelMixin, BaseModel):\\n    \"\"\"Structured result of brand impersonation analysis.\"\"\"\\n    is_impersonation: bool = Field(..., description=\"True if the domain likely impersonates a known brand.\")\\n    detected_brand: Optional[str] = Field(\\n        None, description=\"Canonical name of the impersonated brand if detected (normalized), else null.\",\\n        max_length=_MAX_STR_LEN\\n    )\\n    confidence: float = Field(0.0, description=\"Confidence score in [0.0, 1.0] for the impersonation judgment.\")\\n    evidence: List[str] = Field(default_factory=list, description=\"List of short evidence snippets supporting the judgment.\")\\n    risk_level: RiskLevelLiteral = Field(\"low\", description=\"Risk level based on brand context and patterns.\")\\n\\n    # Validators\\n    @field_validator(\"confidence\", mode=\"before\")\\n    @classmethod\\n    def _v_confidence(cls, v):\\n        return _coerce_float_01(v, default=0.0)\\n\\n    @field_validator(\"evidence\", mode=\"before\")\\n    @classmethod\\n    def _v_evidence_len(cls, v):\\n        v = v or []\\n        if isinstance(v, (list, tuple)):\\n            v = list(v)[:_MAX_LIST_EVIDENCE]\\n        else:\\n            v = [str(v)][:1]\\n        return [str(x)[:_MAX_STR_LEN] for x in v]\\n\\n# -------- 2) CertificateAnalysisResult --------\\nclass CertificateAnalysisResult(_ModelMixin, BaseModel):\\n    \"\"\"Security-focused summary of the target's TLS certificate properties.\"\"\"\\n    has_certificate: bool = Field(..., description=\"True if any valid certificate was found for the domain.\")\\n    is_valid: bool = Field(False, description=\"True if the certificate is currently valid (date range, parsing succeeded).\")\\n    age_days: int = Field(-1, description=\"Certificate age in days (from NotBefore to now); -1 if unknown/unavailable.\")\\n    issuer: Optional[str] = Field(None, description=\"Certificate issuer organization (normalized), if available.\", max_length=_MAX_STR_LEN)\\n    is_free_ca: bool = Field(False, description=\"True if a free CA (e.g., Let's Encrypt) issued the certificate.\")\\n    is_wildcard: bool = Field(False, description=\"True if a wildcard certificate (*.example.com).\")\\n    san_count: int = Field(0, description=\"Number of SAN entries on the certificate.\")\\n    risk_factors: List[str] = Field(default_factory=list, description=\"List of risk factors derived from certificate fields.\")\\n    confidence: float = Field(0.0, description=\"Confidence score in [0.0, 1.0] for the certificate-related assessment.\")\\n\\n    @field_validator(\"san_count\", \"age_days\", mode=\"before\")\\n    @classmethod\\n    def _v_non_negative_or_minus1(cls, v, info: ValidationInfo):\\n        # san_count: >=0 ; age_days: -1 or >=0\\n        try:\\n            iv = int(v)\\n        except Exception:\\n            iv = -1 if info.field_name == \"age_days\" else 0\\n        if info.field_name == \"age_days\":\\n            return iv if iv >= -1 else -1\\n        return max(0, iv)\\n\\n    @field_validator(\"confidence\", mode=\"before\")\\n    @classmethod\\n    def _v_confidence(cls, v):\\n        return _coerce_float_01(v, default=0.0)\\n\\n    @field_validator(\"risk_factors\", mode=\"before\")\\n    @classmethod\\n    def _v_risk_factors(cls, v):\\n        v = v or []\\n        if isinstance(v, (list, tuple)):\\n            v = list(v)[:_MAX_LIST_EVIDENCE]\\n        else:\\n            v = [str(v)][:1]\\n        return [str(x)[:_MAX_STR_LEN] for x in v]\\n\\n    @model_validator(mode=\"after\")\\n    def _v_is_valid_consistency(self):\\n        # has_certificate=False => is_valid must be False, san_count=0\\n        if not self.has_certificate:\\n            self.is_valid = False\\n            self.san_count = 0\\n        return self\\n\\n# -------- 3) DomainAnalysisResult --------\\nclass DomainAnalysisResult(_ModelMixin, BaseModel):\\n    \"\"\"Structured summary of domain structure risks and lexical signals.\"\"\"\\n    is_suspicious: bool = Field(..., description=\"True if domain structure suggests phishing or abuse.\")\\n    domain_length: int = Field(0, description=\"Length of the full domain string (host only, no scheme).\")\\n    has_suspicious_tld: bool = Field(False, description=\"True if the TLD is in a known dangerous list.\")\\n    tld: str = Field(..., description=\"Top-level domain (without leading dot), e.g., 'com', 'co.jp'.\")\\n    typosquatting_candidate: Optional[str] = Field(None, description=\"Closest brand or domain candidate (if any).\", max_length=_MAX_STR_LEN)\\n    levenshtein_distance: Optional[int] = Field(None, description=\"Edit distance to the candidate when relevant (non-negative).\")\\n    risk_factors: List[str] = Field(default_factory=list, description=\"List of risk factor codes or short explanations.\")\\n    confidence: float = Field(0.0, description=\"Confidence score in [0.0, 1.0] for the domain structural assessment.\")\\n\\n    @field_validator(\"domain_length\", mode=\"before\")\\n    @classmethod\\n    def _v_len(cls, v):\\n        try:\\n            iv = int(v)\\n        except Exception:\\n            iv = 0\\n        return max(0, iv)\\n\\n    @field_validator(\"tld\", mode=\"before\")\\n    @classmethod\\n    def _v_tld(cls, v):\\n        v = (v or \"\").strip()\\n        if v.startswith(\".\"):\\n            v = v[1:]\\n        return v[:_MAX_STR_LEN]\\n\\n    @field_validator(\"levenshtein_distance\", mode=\"before\")\\n    @classmethod\\n    def _v_lev(cls, v):\\n        if v is None:\\n            return None\\n        try:\\n            iv = int(v)\\n        except Exception:\\n            iv = 0\\n        return max(0, iv)\\n\\n    @field_validator(\"risk_factors\", mode=\"before\")\\n    @classmethod\\n    def _v_rf(cls, v):\\n        v = v or []\\n        if isinstance(v, (list, tuple)):\\n            v = list(v)[:_MAX_LIST_EVIDENCE]\\n        else:\\n            v = [str(v)][:1]\\n        return [str(x)[:_MAX_STR_LEN] for x in v]\\n\\n    @field_validator(\"confidence\", mode=\"before\")\\n    @classmethod\\n    def _v_confidence(cls, v):\\n        return _coerce_float_01(v, default=0.0)\\n\\n# -------- 4) PhishingDetectionResult (main output) --------\\nclass PhishingDetectionResult(_ModelMixin, BaseModel):\\n    \"\"\"Final phishing verdict with integrated signals from ML, certificate, domain & brand checks.\"\"\"\\n    domain: str = Field(..., description=\"Target domain under evaluation (host only).\", min_length=1, max_length=_MAX_STR_LEN)\\n    is_phishing: bool = Field(..., description=\"True if the final decision is phishing.\")\\n    confidence: float = Field(..., description=\"Overall confidence in [0.0, 1.0] for the final decision.\")\\n    risk_level: RiskLevelLiteral = Field(..., description=\"Normalized risk level summarizing overall severity.\")\\n    ml_probability: float = Field(..., description=\"Raw ML model probability (0.0 to 1.0).\")\\n    brand_analysis: Optional[BrandAnalysisResult] = Field(None, description=\"Nested result of brand impersonation analysis.\")\\n    certificate_analysis: Optional[CertificateAnalysisResult] = Field(None, description=\"Nested result of certificate analysis.\")\\n    domain_analysis: Optional[DomainAnalysisResult] = Field(None, description=\"Nested result of domain structure analysis.\")\\n    reasoning: str = Field(..., description=\"Concise explanation describing why the decision was made.\", max_length=_MAX_REASON_LEN)\\n    detected_patterns: List[str] = Field(default_factory=list, description=\"Patterns detected during analysis (codes or short strings).\")\\n    recommended_action: ActionLiteral = Field(\"review\", description=\"Recommended action for downstream systems or analysts.\")\\n\\n    @field_validator(\"confidence\", \"ml_probability\", mode=\"before\")\\n    @classmethod\\n    def _v_range_01(cls, v):\\n        return _coerce_float_01(v, default=0.0)\\n\\n    @field_validator(\"detected_patterns\", mode=\"before\")\\n    @classmethod\\n    def _v_patterns(cls, v):\\n        v = v or []\\n        if isinstance(v, (list, tuple)):\\n            v = list(v)[:_MAX_LIST_PATTERNS]\\n        else:\\n            v = [str(v)][:1]\\n        return [str(x)[:_MAX_STR_LEN] for x in v]\\n\\n    @model_validator(mode=\"after\")\\n    def _v_action_coherence(self):\\n        # If risk is high/critical, never return 'allow'\\n        if self.risk_level in (\"high\", \"critical\") and self.recommended_action == \"allow\":\\n            self.recommended_action = \"warn\"\\n        return self\\n\\n# -------- 5) AgentState (LangGraph state) --------\\nclass AgentState(_ModelMixin, BaseModel):\\n    \"\"\"Internal state for LangGraph-based agent orchestration.\"\"\"\\n    current_step: StepLiteral = Field(\"initial\", description=\"Current step in the agent pipeline.\")\\n    domain: Optional[str] = Field(None, description=\"Domain being processed, if any.\", max_length=_MAX_STR_LEN)\\n    ml_probability: Optional[float] = Field(None, description=\"ML probability available at state time; [0.0, 1.0].\")\\n    intermediate_results: Dict[str, Any] = Field(default_factory=dict, description=\"Arbitrary map to store tool and node results.\")\\n    final_result: Optional[PhishingDetectionResult] = Field(None, description=\"Finalized detection result if complete.\")\\n    error: Optional[str] = Field(None, description=\"Latest error message, if any.\", max_length=1024)\\n    retry_count: int = Field(0, description=\"How many times the agent retried recoverable failures.\")\\n\\n    @field_validator(\"retry_count\", mode=\"before\")\\n    @classmethod\\n    def _v_retry_non_negative(cls, v):\\n        try:\\n            iv = int(v)\\n        except Exception:\\n            iv = 0\\n        return max(0, iv)\\n\\n    @field_validator(\"ml_probability\", mode=\"before\")\\n    @classmethod\\n    def _v_mlprob_range(cls, v):\\n        if v is None:\\n            return None\\n        return _coerce_float_01(v, default=0.0)\\n\\n    @model_validator(mode=\"after\")\\n    def _v_consistency(self):\\n        # When complete, final_result should exist; if not, write error\\n        if self.current_step == \"complete\" and self.final_result is None:\\n            self.error = self.error or \"State is 'complete' but final_result is None.\"\\n        return self\\n\\n# ---- Public exports ----\\n__all__ = [\\n    \"BrandAnalysisResult\",\\n    \"CertificateAnalysisResult\",\\n    \"DomainAnalysisResult\",\\n    \"PhishingDetectionResult\",\\n    \"AgentState\",\\n    \"RiskLevelLiteral\",\\n    \"ActionLiteral\",\\n    \"StepLiteral\",\\n]\\n\\n# -------------------------\\n# Usage examples (commented)\\n# -------------------------\\n# result = PhishingDetectionResult(\\n#     domain=\"example.com\",\\n#     is_phishing=True,\\n#     confidence=0.85,\\n#     risk_level=\"high\",\\n#     ml_probability=0.30,\\n#     reasoning=\"High similarity to brand login + dangerous TLD + very new cert.\"\\n# )\\n#\\n# # LLM structured output (LangChain-style):\\n# #   llm_with_structured_output = llm.with_structured_output(PhishingDetectionResult)\\n# #   response = llm_with_structured_output.invoke(\"Please analyze this domain: example.com\")\\n# #\\n# # Backward compatibility:\\n# #   payload = result.to_dict()\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "411581c4-657b-4551-a3e7-34d9a64b0f85",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[4]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe03df6f-bea6-43d2-a3df-8613105afb21",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ DB_CONFIG ready: {'dbname': 'rapids_data', 'host': 'localhost', 'port': '5432', 'user': 'postgres'}\n",
      "âœ… handoff unpack (partial allowed)\n"
     ]
    }
   ],
   "source": [
    "# === DB_CONFIG guard (mirrors 01_data_preparation) ===\n",
    "import os\n",
    "if 'DB_CONFIG' not in globals():\n",
    "    if 'handoff' in globals() and isinstance(handoff, dict) and 'DB_CONFIG' in handoff:\n",
    "        DB_CONFIG = handoff['DB_CONFIG']\n",
    "    else:\n",
    "        DB_CONFIG = {\n",
    "            'dbname': os.getenv('PGDATABASE','rapids_data'),\n",
    "            'user': os.getenv('PGUSER','postgres'),\n",
    "            'password': os.getenv('PGPASSWORD','asomura'),\n",
    "            'host': os.getenv('PGHOST','localhost'),\n",
    "            'port': os.getenv('PGPORT','5432'),\n",
    "        }\n",
    "print(\"ğŸ”§ DB_CONFIG ready:\", {k: DB_CONFIG[k] for k in ['dbname','host','port','user']})\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# === handoff unpack (auto-added) ===\n",
    "# 03-4 ã§æœ€ä½é™å¿…è¦ãªã‚­ãƒ¼ã‚’å±•é–‹\n",
    "required_keys = ['false_negatives_df', 'brand_keywords', 'cert_full_info_map', 'fn_features_df',\n",
    "                 'DANGEROUS_TLDS', 'LEGITIMATE_TLDS', 'NEUTRAL_TLDS']\n",
    "missing = [k for k in required_keys if not ('handoff' in globals() and isinstance(handoff, dict) and k in handoff)]\n",
    "if missing:\n",
    "    print(\"âš ï¸ handoff ã‹ã‚‰è¦‹ã¤ã‹ã‚‰ãªã„ã‚­ãƒ¼ãŒã‚ã‚Šã¾ã™:\", missing)\n",
    "\n",
    "# ã‚ã‚‹ã‚‚ã®ã ã‘å±•é–‹ï¼ˆç„¡ã„ã‚‚ã®ã¯å¾Œæ®µã® resolver ã§è£œã†ï¼‰\n",
    "globals().update({k: handoff[k] for k in ['false_negatives_df','brand_keywords','cert_full_info_map','fn_features_df'] if 'handoff' in globals() and isinstance(handoff, dict) and k in handoff})\n",
    "for k in ['DANGEROUS_TLDS','LEGITIMATE_TLDS','NEUTRAL_TLDS']:\n",
    "    if 'handoff' in globals() and isinstance(handoff, dict) and k in handoff:\n",
    "        globals()[k] = handoff[k]\n",
    "print(\"âœ… handoff unpack (partial allowed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e04a6f4-14f2-462e-84a9-e0fc75a4fc51",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496d106c-800d-4ad7-baf7-29022619a72e",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ HIGH_RISK_WORDS derived from brand_keywords (n=100)\n",
      "ğŸ”§ suspicious_words_stats derived (unique=4644)\n",
      "ğŸ”§ TLD_STATS derived (unique=178)\n",
      "ğŸ”§ KNOWN_DOMAINS derived from false_negatives_df (n=4132)\n",
      "âœ… stats resolver completed.\n"
     ]
    }
   ],
   "source": [
    "# === analysis stats resolver (auto-added) ===\n",
    "# ç›®çš„: HIGH_RISK_WORDS, suspicious_words_stats, TLD_STATS, KNOWN_DOMAINS ã‚’å¯èƒ½ãªç¯„å›²ã§åˆæœŸåŒ–\n",
    "import pandas as pd\n",
    "\n",
    "# HIGH_RISK_WORDS: brand_keywords ã‚’å„ªå…ˆ\n",
    "if 'HIGH_RISK_WORDS' not in globals():\n",
    "    if 'brand_keywords' in globals() and isinstance(brand_keywords, (list, set, dict)):\n",
    "        if isinstance(brand_keywords, dict):\n",
    "            HIGH_RISK_WORDS = sorted(set(sum(([k] + (v if isinstance(v, list) else []) for k,v in brand_keywords.items()), [])))\n",
    "        else:\n",
    "            HIGH_RISK_WORDS = sorted(list(set(brand_keywords)))\n",
    "        print(f\"ğŸ”§ HIGH_RISK_WORDS derived from brand_keywords (n={len(HIGH_RISK_WORDS)})\")\n",
    "    else:\n",
    "        HIGH_RISK_WORDS = []\n",
    "        print(\"âš ï¸ HIGH_RISK_WORDS fallback: empty list\")\n",
    "\n",
    "# suspicious_words_stats: false_negatives_df ãŒã‚ã‚Œã°ç°¡æ˜“é »åº¦é›†è¨ˆï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã¯æœ€å°é™ï¼‰\n",
    "if 'suspicious_words_stats' not in globals():\n",
    "    try:\n",
    "        if 'false_negatives_df' in globals() and hasattr(false_negatives_df, 'assign'):\n",
    "            df = false_negatives_df.copy()\n",
    "            # æœ€å°é™ï¼šãƒ‰ãƒ¡ã‚¤ãƒ³ã® '-' '_' '.' ã§åˆ†å‰²ã—ã¦å˜èªé »åº¦\n",
    "            def split_words(s):\n",
    "                try:\n",
    "                    for ch in ['-', '_', '.']:\n",
    "                        s = s.replace(ch, ' ')\n",
    "                    return [w for w in s.split() if w]\n",
    "                except Exception:\n",
    "                    return []\n",
    "            words = df['domain'].astype(str).map(split_words)\n",
    "            from collections import Counter\n",
    "            cnt = Counter([w for lst in words for w in lst])\n",
    "            suspicious_words_stats = pd.DataFrame(cnt.items(), columns=['word','count']).sort_values('count', ascending=False)\n",
    "            print(f\"ğŸ”§ suspicious_words_stats derived (unique={len(suspicious_words_stats)})\")\n",
    "        else:\n",
    "            suspicious_words_stats = pd.DataFrame(columns=['word','count'])\n",
    "            print(\"âš ï¸ suspicious_words_stats fallback: empty frame\")\n",
    "    except Exception as e:\n",
    "        suspicious_words_stats = pd.DataFrame(columns=['word','count'])\n",
    "        print(\"âš ï¸ suspicious_words_stats error -> empty:\", e)\n",
    "\n",
    "# TLD_STATS: false_negatives_df ãŒã‚ã‚Œã°ç°¡æ˜“é›†è¨ˆ\n",
    "if 'TLD_STATS' not in globals():\n",
    "    try:\n",
    "        if 'false_negatives_df' in globals() and 'domain' in false_negatives_df.columns:\n",
    "            def get_tld(d):\n",
    "                try:\n",
    "                    parts = str(d).split('.')\n",
    "                    return parts[-1] if len(parts)>1 else ''\n",
    "                except Exception:\n",
    "                    return ''\n",
    "            tlds = false_negatives_df['domain'].map(get_tld)\n",
    "            TLD_STATS = tlds.value_counts().rename_axis('tld').reset_index(name='count')\n",
    "            print(f\"ğŸ”§ TLD_STATS derived (unique={len(TLD_STATS)})\")\n",
    "        else:\n",
    "            import pandas as pd\n",
    "            TLD_STATS = pd.DataFrame(columns=['tld','count'])\n",
    "            print(\"âš ï¸ TLD_STATS fallback: empty frame\")\n",
    "    except Exception as e:\n",
    "        import pandas as pd\n",
    "        TLD_STATS = pd.DataFrame(columns=['tld','count'])\n",
    "        print(\"âš ï¸ TLD_STATS error -> empty:\", e)\n",
    "\n",
    "# KNOWN_DOMAINS: false_negatives_df or cert_full_info_map ã‹ã‚‰æ¨å®š\n",
    "if 'KNOWN_DOMAINS' not in globals():\n",
    "    try:\n",
    "        if 'false_negatives_df' in globals() and 'domain' in false_negatives_df.columns:\n",
    "            KNOWN_DOMAINS = set(false_negatives_df['domain'].dropna().astype(str).tolist())\n",
    "            print(f\"ğŸ”§ KNOWN_DOMAINS derived from false_negatives_df (n={len(KNOWN_DOMAINS)})\")\n",
    "        elif 'cert_full_info_map' in globals() and isinstance(cert_full_info_map, dict):\n",
    "            KNOWN_DOMAINS = set(cert_full_info_map.keys())\n",
    "            print(f\"ğŸ”§ KNOWN_DOMAINS derived from cert_full_info_map (n={len(KNOWN_DOMAINS)})\")\n",
    "        else:\n",
    "            KNOWN_DOMAINS = set()\n",
    "            print(\"âš ï¸ KNOWN_DOMAINS fallback: empty set\")\n",
    "    except Exception as e:\n",
    "        KNOWN_DOMAINS = set()\n",
    "        print(\"âš ï¸ KNOWN_DOMAINS error -> empty:\", e)\n",
    "\n",
    "print(\"âœ… stats resolver completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0ca6d82-bdf4-4115-8531-76eaabd1117f",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d98d8004-2111-4d47-b1b1-2a2c556570a8",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… stats normalized: KNOWN_DOMAINS=dict HIGH_RISK_WORDS=100\n"
     ]
    }
   ],
   "source": [
    "# === stats normalize (auto-added) ===\n",
    "# ç›®çš„: ä¸‹æµã‚»ãƒ«ãŒ dict.keys() å‰æã§ã‚‚è½ã¡ãªã„ã‚ˆã†ã€å‹ã‚’æ­£è¦åŒ–\n",
    "def _to_domain_dict(obj):\n",
    "    if obj is None:\n",
    "        return {}\n",
    "    if isinstance(obj, dict):\n",
    "        # ã™ã§ã« dict ã®å ´åˆã¯ã‚­ãƒ¼ã‚’ str ã«å¯„ã›ã‚‹\n",
    "        return {str(k): v for k, v in obj.items()}\n",
    "    if isinstance(obj, (set, list, tuple)):\n",
    "        return {str(x): True for x in obj}\n",
    "    # æƒ³å®šå¤–ã¯ç©ºã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "    return {}\n",
    "\n",
    "# KNOWN_DOMAINS ã‚’ dict ã«æ­£è¦åŒ–ï¼ˆ.keys() ã‚’å®‰å…¨ã«å‘¼ã¹ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰\n",
    "if 'KNOWN_DOMAINS' in globals():\n",
    "    KNOWN_DOMAINS = _to_domain_dict(KNOWN_DOMAINS)\n",
    "\n",
    "# HIGH_RISK_WORDS ã¯ list ã¸å¯„ã›ã¦é‡è¤‡æ’é™¤\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    if isinstance(HIGH_RISK_WORDS, dict):\n",
    "        HIGH_RISK_WORDS = list(HIGH_RISK_WORDS.keys())\n",
    "    elif isinstance(HIGH_RISK_WORDS, (set, tuple)):\n",
    "        HIGH_RISK_WORDS = list(HIGH_RISK_WORDS)\n",
    "    HIGH_RISK_WORDS = sorted(list(set(map(str, HIGH_RISK_WORDS))))\n",
    "\n",
    "print(\"âœ… stats normalized:\",\n",
    "      \"KNOWN_DOMAINS=dict\" if isinstance(globals().get('KNOWN_DOMAINS', None), dict) else type(globals().get('KNOWN_DOMAINS', None)),\n",
    "      f\"HIGH_RISK_WORDS={len(globals().get('HIGH_RISK_WORDS', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e6232e7-f5b6-420b-b2b2-1036942bcb5a",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[8]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc50df98-a629-4672-af94-b16858bd8524",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# === brand_keywords resolver (auto-added) ===\n",
    "# å„ªå…ˆ: handoff -> artifacts/{RUN_ID}/models/brand_keywords.json -> models/*/brand_keywords.json\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "def _load_brand_keywords_candidates():\n",
    "    cands = []\n",
    "    # 1) handoff\n",
    "    if 'handoff' in globals() and isinstance(handoff, dict) and 'brand_keywords' in handoff:\n",
    "        return handoff['brand_keywords']\n",
    "    # 2) artifacts/{RUN_ID}/models\n",
    "    try:\n",
    "        p = Path(MODELS_DIR) / \"brand_keywords.json\"\n",
    "        if p.exists():\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3) legacy models/*/brand_keywords.json (latest)\n",
    "    try:\n",
    "        latest = None\n",
    "        models_root = Path(\"models\")\n",
    "        if models_root.exists():\n",
    "            dirs = sorted([d for d in models_root.glob(\"*\") if d.is_dir()])\n",
    "            if dirs:\n",
    "                latest = dirs[-1]\n",
    "                p = latest / \"brand_keywords.json\"\n",
    "                if p.exists():\n",
    "                    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                        return json.load(f)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "if 'brand_keywords' not in globals() or brand_keywords in (None, [], {}):\n",
    "    _bk = _load_brand_keywords_candidates()\n",
    "    if _bk is not None:\n",
    "        brand_keywords = _bk\n",
    "        print(\"ğŸ”§ brand_keywords loaded\")\n",
    "    else:\n",
    "        print(\"âš ï¸ brand_keywords not found in handoff nor models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b44dbe32-3646-4fb5-83e0-79909b5254b9",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[9]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a67e1d1c-0cec-4297-8d55-3762e88c5a92",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TLD sets resolver completed. DANGEROUS=22 LEGITIMATE=8 NEUTRAL=1\n"
     ]
    }
   ],
   "source": [
    "# === TLD sets resolver (auto-added) ===\n",
    "# ç›®çš„: DANGEROUS_TLDS, LEGITIMATE_TLDS, NEUTRAL_TLDS ã‚’æœ€ä½é™åˆæœŸåŒ–\n",
    "# å„ªå…ˆ: handoff å±•é–‹æ¸ˆã¿ -> false_negatives_df ã‹ã‚‰çµ±è¨ˆã§æ¨å®šï¼ˆç°¡æ˜“ï¼‰\n",
    "import pandas as pd\n",
    "\n",
    "def _derive_tld_stats_from_df(df):\n",
    "    if df is None or not hasattr(df, 'columns') or 'domain' not in df.columns:\n",
    "        return pd.DataFrame(columns=['tld','count'])\n",
    "    def get_tld(d):\n",
    "        try:\n",
    "            parts = str(d).split('.')\n",
    "            return parts[-1] if len(parts)>1 else ''\n",
    "        except Exception:\n",
    "            return ''\n",
    "    tlds = df['domain'].map(get_tld)\n",
    "    return tlds.value_counts().rename_axis('tld').reset_index(name='count')\n",
    "\n",
    "if 'DANGEROUS_TLDS' not in globals() or DANGEROUS_TLDS in (None, [], {}):\n",
    "    # ç°¡æ˜“æ¨å®š: å½é™°æ€§ã§å¤šã„ TLD ä¸Šä½ã‚’å±é™ºã¨ã—ã¦æ‰±ã†ï¼ˆé–¾å€¤: ä¸Šä½20 or å‡ºç¾>=2ï¼‰\n",
    "    stats = _derive_tld_stats_from_df(globals().get('false_negatives_df', None))\n",
    "    if len(stats):\n",
    "        topN = min(20, len(stats))\n",
    "        DANGEROUS_TLDS = stats.head(topN)['tld'].astype(str).tolist()\n",
    "        print(f\"ğŸ”§ DANGEROUS_TLDS derived (n={len(DANGEROUS_TLDS)})\")\n",
    "    else:\n",
    "        DANGEROUS_TLDS = []\n",
    "        print(\"âš ï¸ DANGEROUS_TLDS fallback: empty list\")\n",
    "\n",
    "if 'LEGITIMATE_TLDS' not in globals() or LEGITIMATE_TLDS in (None, [], {}):\n",
    "    LEGITIMATE_TLDS = []\n",
    "if 'NEUTRAL_TLDS' not in globals() or NEUTRAL_TLDS in (None, [], {}):\n",
    "    NEUTRAL_TLDS = []\n",
    "\n",
    "# æ—§ã‚³ãƒ¼ãƒ‰ãŒæœŸå¾…ã™ã‚‹æ´¾ç”Ÿçµ±è¨ˆï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã°ç©ºã§ç”¨æ„ï¼‰\n",
    "if 'phishing_tld_stats' not in globals():\n",
    "    phishing_tld_stats = _derive_tld_stats_from_df(globals().get('false_negatives_df', None))\n",
    "if 'trusted_tld_stats' not in globals():\n",
    "    trusted_tld_stats = pd.DataFrame(columns=['tld','count'])\n",
    "\n",
    "print(\"âœ… TLD sets resolver completed.\",\n",
    "      f\"DANGEROUS={len(DANGEROUS_TLDS)} LEGITIMATE={len(LEGITIMATE_TLDS)} NEUTRAL={len(NEUTRAL_TLDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "717062a3-d65d-4f22-a313-eaa422ed6b68",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[10]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18fc20fd-fe43-46a1-8354-75685b700802",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ¤– LLMã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹\n",
      "  - ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: vllm\n",
      "  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: phishing_agent_1760861689\n",
      "======================================================================\n",
      "ğŸ“¡ vLLMã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šä¸­: http://192.168.100.71:30000/v1\n",
      "âœ… vLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†\n",
      "  - ãƒ¢ãƒ‡ãƒ«: Qwen/Qwen3-14B-FP8\n",
      "  - æœ€å¤§ä¸¦åˆ—æ•°: 10\n",
      "âœ… LLMæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ\n",
      "  - å¿œç­”: <think>\n",
      "Okay, the user said \"Hello\"....\n",
      "âœ… éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ\n",
      "  - å¿œç­”: <think>\n",
      "Okay, the user said \"Hello async...\n",
      "\n",
      "ğŸ“Š LLMè¨­å®šã‚µãƒãƒªãƒ¼:\n",
      "  - ã‚¿ã‚¤ãƒ—: vllm\n",
      "  - ãƒ¢ãƒ‡ãƒ«: Qwen/Qwen3-14B-FP8\n",
      "  - æ¸©åº¦: 0.1\n",
      "  - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³: 2048\n",
      "  - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 60ç§’\n",
      "  - æœ€å¤§ä¸¦åˆ—æ•°: 10\n",
      "\n",
      "âœ… ã‚»ãƒ«03å®Ÿè¡Œå®Œäº†: LLMè¨­å®šï¼ˆä¸¦åˆ—å‡¦ç†å¯¾å¿œç‰ˆï¼‰\n",
      "  - åŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: åˆæœŸåŒ–æ¸ˆã¿\n",
      "  - éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: åˆæœŸåŒ–æ¸ˆã¿\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆGPT-4o miniï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 03\n",
    "æ¦‚è¦: LLMè¨­å®šï¼ˆvLLM/Ollamaå¯¾å¿œï¼‰- ä¸¦åˆ—å‡¦ç†å¯¾å¿œç‰ˆ\n",
    "å…¥åŠ›: ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã€è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "å‡ºåŠ›: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã€ä¸¦åˆ—å‡¦ç†è¨­å®š\n",
    "\n",
    "å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:\n",
    "- pip install openai>=1.0.0\n",
    "- pip install nest_asyncio\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Jupyterç’°å¢ƒã§ã®asyncioå¯¾å¿œ\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ nest_asyncioãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"  ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ãŒåˆ¶é™ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
    "    print(\"  ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install nest_asyncio\")\n",
    "\n",
    "# LLMè¨­å®š\n",
    "LLM_TYPE = \"vllm\"  # \"vllm\" or \"ollama\"\n",
    "VLLM_BASE_URL = \"http://192.168.100.71:30000/v1\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "\n",
    "# ä¸¦åˆ—å‡¦ç†è¨­å®š\n",
    "VLLM_MAX_CONCURRENT = 10  # RTX 5000 Adaã§ã®æœ€é©å€¤\n",
    "\n",
    "# ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆå†å®Ÿè¡Œæ™‚ã‚‚ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ï¼‰\n",
    "SESSION_ID = f\"phishing_agent_{int(time.time())}\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"ğŸ¤– LLMã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹\")\n",
    "print(f\"  - ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {LLM_TYPE}\")\n",
    "print(f\"  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {SESSION_ID}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# åŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆæ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ï¼‰\n",
    "if LLM_TYPE == \"vllm\":\n",
    "    print(f\"ğŸ“¡ vLLMã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šä¸­: {VLLM_BASE_URL}\")\n",
    "    \n",
    "    # åŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\n",
    "    client = OpenAI(\n",
    "        base_url=VLLM_BASE_URL,\n",
    "        api_key=\"EMPTY\"  # vLLMã¯èªè¨¼ä¸è¦\n",
    "    )\n",
    "    \n",
    "    # éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰\n",
    "    async_client = AsyncOpenAI(\n",
    "        base_url=VLLM_BASE_URL,\n",
    "        api_key=\"EMPTY\"\n",
    "    )\n",
    "    \n",
    "    DEFAULT_MODEL = \"Qwen/Qwen3-14B-FP8\"\n",
    "    print(f\"âœ… vLLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†\")\n",
    "    print(f\"  - ãƒ¢ãƒ‡ãƒ«: {DEFAULT_MODEL}\")\n",
    "    print(f\"  - æœ€å¤§ä¸¦åˆ—æ•°: {VLLM_MAX_CONCURRENT}\")\n",
    "    \n",
    "elif LLM_TYPE == \"ollama\":\n",
    "    print(f\"ğŸ“¡ Ollamaã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šä¸­: {OLLAMA_BASE_URL}\")\n",
    "    \n",
    "    # åŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\n",
    "    client = OpenAI(\n",
    "        base_url=OLLAMA_BASE_URL,\n",
    "        api_key=\"ollama\"  # Ollamaã¯ä»»æ„ã®å€¤\n",
    "    )\n",
    "    \n",
    "    # Ollamaã¯éåŒæœŸã«å¯¾å¿œã—ã¦ã„ãªã„ãŸã‚ã€åŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨\n",
    "    async_client = None\n",
    "    \n",
    "    DEFAULT_MODEL = \"qwen2.5:14b\"\n",
    "    print(f\"âœ… Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†\")\n",
    "    print(f\"  - ãƒ¢ãƒ‡ãƒ«: {DEFAULT_MODEL}\")\n",
    "    print(f\"  - æ³¨æ„: Ollamaã¯ä¸¦åˆ—å‡¦ç†éå¯¾å¿œ\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"ä¸æ˜ãªLLMã‚¿ã‚¤ãƒ—: {LLM_TYPE}\")\n",
    "\n",
    "# æ¥ç¶šãƒ†ã‚¹ãƒˆ\n",
    "try:\n",
    "    test_response = client.chat.completions.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "        max_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "    print(f\"âœ… LLMæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ\")\n",
    "    print(f\"  - å¿œç­”: {test_response.choices[0].message.content[:50]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ LLMæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—: {e}\")\n",
    "    print(f\"  - ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "    raise\n",
    "\n",
    "# éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆï¼ˆvLLMã®ã¿ï¼‰\n",
    "if LLM_TYPE == \"vllm\" and async_client:\n",
    "    async def test_async_connection():\n",
    "        \"\"\"éåŒæœŸæ¥ç¶šã®ãƒ†ã‚¹ãƒˆ\"\"\"\n",
    "        try:\n",
    "            response = await async_client.chat.completions.create(\n",
    "                model=DEFAULT_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello async\"}],\n",
    "                max_tokens=10,\n",
    "                temperature=0\n",
    "            )\n",
    "            print(f\"âœ… éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ\")\n",
    "            print(f\"  - å¿œç­”: {response.choices[0].message.content[:50]}...\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Jupyterç’°å¢ƒå¯¾å¿œ: nest_asyncioã‚’ä½¿ç”¨\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        asyncio.run(test_async_connection())\n",
    "    except ImportError:\n",
    "        # nest_asyncioãŒãªã„å ´åˆã¯ã€åŒæœŸçš„ã«ãƒ†ã‚¹ãƒˆ\n",
    "        print(\"âš ï¸ nest_asyncioãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        print(\"  éåŒæœŸãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
    "        print(\"  ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install nest_asyncio\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š\n",
    "LLM_CONFIG = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 2048,\n",
    "    \"timeout\": 60,  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’60ç§’ã«å»¶é•·ï¼ˆä¸¦åˆ—å‡¦ç†å¯¾å¿œï¼‰\n",
    "    \"model\": DEFAULT_MODEL,\n",
    "    \"max_concurrent\": VLLM_MAX_CONCURRENT if LLM_TYPE == \"vllm\" else 1\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“Š LLMè¨­å®šã‚µãƒãƒªãƒ¼:\")\n",
    "print(f\"  - ã‚¿ã‚¤ãƒ—: {LLM_TYPE}\")\n",
    "print(f\"  - ãƒ¢ãƒ‡ãƒ«: {LLM_CONFIG['model']}\")\n",
    "print(f\"  - æ¸©åº¦: {LLM_CONFIG['temperature']}\")\n",
    "print(f\"  - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³: {LLM_CONFIG['max_tokens']}\")\n",
    "print(f\"  - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {LLM_CONFIG['timeout']}ç§’\")\n",
    "print(f\"  - æœ€å¤§ä¸¦åˆ—æ•°: {LLM_CONFIG['max_concurrent']}\")\n",
    "\n",
    "print(\"\\nâœ… ã‚»ãƒ«03å®Ÿè¡Œå®Œäº†: LLMè¨­å®šï¼ˆä¸¦åˆ—å‡¦ç†å¯¾å¿œç‰ˆï¼‰\")\n",
    "print(\"  - åŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: åˆæœŸåŒ–æ¸ˆã¿\")\n",
    "if async_client:\n",
    "    print(\"  - éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: åˆæœŸåŒ–æ¸ˆã¿\")\n",
    "else:\n",
    "    print(\"  - éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ: æœªå¯¾å¿œï¼ˆOllamaï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "261762e8-9ff3-47bd-a2f6-a8e982494047",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[11]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23c5d09a-a021-4d0e-b83f-2b7f212f1075",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ åˆ†æçµæœã‚’æ´»ã‹ã—ãŸæ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ \n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š åˆ©ç”¨å¯èƒ½ãªçµ±è¨ˆãƒ‡ãƒ¼ã‚¿:\n",
      "----------------------------------------\n",
      "âœ… HIGH_RISK_WORDS: 100å€‹\n",
      "âœ… KNOWN_DOMAINS: 4132ç¨®é¡\n",
      "\n",
      "ğŸ§ª æ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ:\n",
      "----------------------------------------\n",
      "\n",
      "amazon.co.jp.suppor34.frhnkm.ph\n",
      "  ğŸš¨ åˆ¤å®š: phishing (ãƒªã‚¹ã‚¯: 95.0, ä¿¡é ¼åº¦: 0.95)\n",
      "  ç†ç”±: ç¢ºå®Ÿãª.co.jpå½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³; éšå±¤ãŒæ·±ã„: 6å±¤\n",
      "\n",
      "amazon-co-jp.com\n",
      "  âœ… åˆ¤å®š: legitimate (ãƒªã‚¹ã‚¯: 27.5, ä¿¡é ¼åº¦: 0.28)\n",
      "\n",
      "rakuten-jp.net\n",
      "  âœ… åˆ¤å®š: legitimate (ãƒªã‚¹ã‚¯: 0.0, ä¿¡é ¼åº¦: 0.00)\n",
      "\n",
      "kobe-denki.co.jp\n",
      "  âœ… åˆ¤å®š: legitimate (ãƒªã‚¹ã‚¯: 0.0, ä¿¡é ¼åº¦: 0.00)\n",
      "\n",
      "amazon.co.jp\n",
      "  âœ… åˆ¤å®š: legitimate (ãƒªã‚¹ã‚¯: 0.0, ä¿¡é ¼åº¦: 0.00)\n",
      "\n",
      "google.com\n",
      "  âœ… åˆ¤å®š: legitimate (ãƒªã‚¹ã‚¯: 0.0, ä¿¡é ¼åº¦: 0.00)\n",
      "\n",
      "jp-bank.com\n",
      "  ğŸš¨ åˆ¤å®š: phishing (ãƒªã‚¹ã‚¯: 60.0, ä¿¡é ¼åº¦: 0.80)\n",
      "\n",
      "secure-payment.tk\n",
      "  âœ… åˆ¤å®š: legitimate (ãƒªã‚¹ã‚¯: 0.0, ä¿¡é ¼åº¦: 0.00)\n",
      "\n",
      "duckdns.org\n",
      "  ğŸš¨ åˆ¤å®š: phishing (ãƒªã‚¹ã‚¯: 100.0, ä¿¡é ¼åº¦: 0.95)\n",
      "  ç†ç”±: DuckDNSï¼ˆãƒ•ãƒªãƒ¼DNSï¼‰ä½¿ç”¨\n",
      "\n",
      "ğŸ“Š æ”¹è‰¯ã®åŠ¹æœ:\n",
      "  - å…ƒã®é«˜ãƒªã‚¹ã‚¯å˜èª: 100å€‹\n",
      "  - å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–: 0å€‹\n",
      "  - ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’å„ªå…ˆ: å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯95ç‚¹ä»¥ä¸Š\n",
      "  - TLDçµ±è¨ˆã‚’æ´»ç”¨: 178ç¨®é¡\n",
      "\n",
      "================================================================================\n",
      "âœ… æ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ æ”¹è‰¯ã®ãƒã‚¤ãƒ³ãƒˆ:\n",
      "  1. å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ10,695ä»¶ï¼‰ã‚’æœ€å„ªå…ˆã§æ¤œå‡º\n",
      "  2. åŸ‹ã‚è¾¼ã¿å½è£…ï¼ˆ1,190ä»¶ï¼‰ã‚’ç¢ºå®Ÿã«æ¤œå‡º\n",
      "  3. 'jp'ã‚„'co'ã¯æ–‡è„ˆã‚’è¦‹ã¦åˆ¤æ–­\n",
      "  4. wwwã‚’å®Œå…¨ã«é™¤å¤–ï¼ˆ59,046ä»¶ã®èª¤æ¤œå‡ºã‚’é˜²ãï¼‰\n",
      "  5. IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ­£ã—ãèªè­˜\n",
      "  6. TLDçµ±è¨ˆï¼ˆ810ç¨®é¡ï¼‰ã‚’æ´»ç”¨\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 04-IMPROVED\n",
    "æ¦‚è¦: åˆ†æçµæœã‚’æ´»ã‹ã—ãŸæ”¹è‰¯ç‰ˆåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯\n",
    "å…¥åŠ›: HIGH_RISK_WORDS, TLD_STATS, KNOWN_DOMAINSï¼ˆã‚»ãƒ«04ã®å®Ÿè¡Œçµæœï¼‰\n",
    "å‡ºåŠ›: æ”¹è‰¯ç‰ˆã®åˆ¤å®šé–¢æ•°ç¾¤\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import math\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸš€ åˆ†æçµæœã‚’æ´»ã‹ã—ãŸæ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ \")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== 1. åˆ†æçµæœã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\nğŸ“Š åˆ©ç”¨å¯èƒ½ãªçµ±è¨ˆãƒ‡ãƒ¼ã‚¿:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# å¿…è¦ãªå¤‰æ•°ã®ç¢ºèª\n",
    "required_vars = {\n",
    "    'HIGH_RISK_WORDS': 'é«˜ãƒªã‚¹ã‚¯å˜èªãƒªã‚¹ãƒˆ',\n",
    "    'suspicious_words_stats': 'å˜èªåˆ¥çµ±è¨ˆ',\n",
    "    'TLD_STATS': 'TLDåˆ¥çµ±è¨ˆ',\n",
    "    'KNOWN_DOMAINS': 'åŸ‹ã‚è¾¼ã¿ãƒ‰ãƒ¡ã‚¤ãƒ³çµ±è¨ˆ'\n",
    "}\n",
    "\n",
    "available_vars = {}\n",
    "for var_name, description in required_vars.items():\n",
    "    if var_name in globals():\n",
    "        available_vars[var_name] = globals()[var_name]\n",
    "        if isinstance(globals()[var_name], list):\n",
    "            print(f\"âœ… {var_name}: {len(globals()[var_name])}å€‹\")\n",
    "        elif isinstance(globals()[var_name], dict):\n",
    "            print(f\"âœ… {var_name}: {len(globals()[var_name])}ç¨®é¡\")\n",
    "    else:\n",
    "        print(f\"âŒ {var_name}: æœªå®šç¾© - {description}\")\n",
    "\n",
    "if len(available_vars) < 3:\n",
    "    print(\"\\nâš ï¸ ã‚»ãƒ«04ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 2. æ”¹è‰¯ç‰ˆã®ç‰¹å¾´æŠ½å‡ºé–¢æ•° ==========\n",
    "\n",
    "def extract_features_improved(domain: str) -> Dict:\n",
    "    \"\"\"\n",
    "    åˆ†æçµæœã‚’æ´»ã‹ã—ãŸæ”¹è‰¯ç‰ˆç‰¹å¾´æŠ½å‡º\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            'clean_words': [],      # é€šå¸¸ã®å˜èªï¼ˆå•é¡Œã®ã‚ã‚‹ã‚‚ã®ã‚’é™¤å¤–ï¼‰\n",
    "            'risk_patterns': [],    # æ¤œå‡ºã•ã‚ŒãŸãƒªã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "            'risk_scores': {},      # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã®ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢\n",
    "            'tld_info': {},        # TLDæƒ…å ±\n",
    "            'structure': {}        # æ§‹é€ æƒ…å ±\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {\n",
    "        'clean_words': [],\n",
    "        'risk_patterns': [],\n",
    "        'risk_scores': {},\n",
    "        'tld_info': {},\n",
    "        'structure': {}\n",
    "    }\n",
    "    \n",
    "    if not domain:\n",
    "        return features\n",
    "    \n",
    "    domain_lower = domain.lower().strip()\n",
    "    \n",
    "    # ========== IPã‚¢ãƒ‰ãƒ¬ã‚¹ãƒã‚§ãƒƒã‚¯ ==========\n",
    "    # æ•°å­—ã®TLDã‚’é˜²ã\n",
    "    if re.match(r'^[\\d\\.]+$', domain_lower):\n",
    "        features['risk_patterns'].append('ip_address')\n",
    "        features['risk_scores']['ip_address'] = 30\n",
    "        return features\n",
    "    \n",
    "    # ========== å‰å‡¦ç† ==========\n",
    "    # www.ã‚’é™¤å»ï¼ˆ59,046ä»¶ã®èª¤æ¤œå‡ºã‚’é˜²ãï¼‰\n",
    "    if domain_lower.startswith('www.'):\n",
    "        domain_lower = domain_lower[4:]\n",
    "        features['structure']['has_www'] = True\n",
    "    \n",
    "    # ========== TLDæŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ==========\n",
    "    if 'extract_real_tld' in globals():\n",
    "        tld_info = extract_real_tld(domain_lower)\n",
    "        features['tld_info'] = tld_info\n",
    "        domain_body = tld_info.get('domain_body', domain_lower)\n",
    "    else:\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "        parts = domain_lower.split('.')\n",
    "        if len(parts) >= 2:\n",
    "            domain_body = '.'.join(parts[:-1])\n",
    "            features['tld_info'] = {'tld': parts[-1], 'domain_body': domain_body}\n",
    "        else:\n",
    "            domain_body = domain_lower\n",
    "            features['tld_info'] = {'tld': '', 'domain_body': domain_body}\n",
    "    \n",
    "    # ========== æ§‹é€ åˆ†æ ==========\n",
    "    features['structure']['depth'] = domain_lower.count('.') + 1\n",
    "    features['structure']['length'] = len(domain_lower)\n",
    "    features['structure']['hyphen_count'] = domain_lower.count('-')\n",
    "    \n",
    "    # ========== ãƒªã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºï¼ˆå„ªå…ˆåº¦é †ï¼‰ ==========\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ç¢ºå®Ÿãªå½è£…ï¼ˆæœ€å„ªå…ˆï¼‰\n",
    "    if '.co.jp.' in domain_lower and not domain_lower.endswith('.co.jp'):\n",
    "        features['risk_patterns'].append('fake_cojp_absolute')\n",
    "        features['risk_scores']['fake_cojp_absolute'] = 95\n",
    "    \n",
    "    if '.com.' in domain_lower and not domain_lower.endswith('.com'):\n",
    "        features['risk_patterns'].append('fake_com_absolute')\n",
    "        features['risk_scores']['fake_com_absolute'] = 90\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ—¢çŸ¥ãƒ‰ãƒ¡ã‚¤ãƒ³ã®åŸ‹ã‚è¾¼ã¿ï¼ˆé«˜å„ªå…ˆï¼‰\n",
    "    if 'KNOWN_DOMAINS' in globals():\n",
    "        for known_domain in KNOWN_DOMAINS.keys():\n",
    "            if known_domain in domain_lower and not domain_lower.endswith(known_domain):\n",
    "                pattern_name = f'embedded_{known_domain.replace(\".\", \"_\")}'\n",
    "                features['risk_patterns'].append(pattern_name)\n",
    "                # åŸ‹ã‚è¾¼ã¿å›æ•°ã«åŸºã¥ãã‚¹ã‚³ã‚¢\n",
    "                embed_stats = KNOWN_DOMAINS[known_domain]\n",
    "                if embed_stats['phishing_embedded'] > 1000:\n",
    "                    features['risk_scores'][pattern_name] = 90\n",
    "                elif embed_stats['phishing_embedded'] > 500:\n",
    "                    features['risk_scores'][pattern_name] = 85\n",
    "                elif embed_stats['phishing_embedded'] > 100:\n",
    "                    features['risk_scores'][pattern_name] = 80\n",
    "                else:\n",
    "                    features['risk_scores'][pattern_name] = 70\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒã‚¤ãƒ•ãƒ³å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "    jp_hyphen_patterns = ['-jp.', '.jp-', '-jp-', 'jp-']\n",
    "    co_hyphen_patterns = ['-co.', '.co-', '-co-', 'co-']\n",
    "    \n",
    "    for pattern in jp_hyphen_patterns:\n",
    "        if pattern in domain_body:\n",
    "            features['risk_patterns'].append('jp_hyphen_variant')\n",
    "            features['risk_scores']['jp_hyphen_variant'] = 60\n",
    "            break\n",
    "    \n",
    "    for pattern in co_hyphen_patterns:\n",
    "        if pattern in domain_body:\n",
    "            features['risk_patterns'].append('co_hyphen_variant')\n",
    "            features['risk_scores']['co_hyphen_variant'] = 55\n",
    "            break\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: DuckDNSï¼ˆæœ€å±é™ºï¼‰\n",
    "    if 'duckdns' in domain_lower:\n",
    "        features['risk_patterns'].append('duckdns')\n",
    "        features['risk_scores']['duckdns'] = 100\n",
    "    \n",
    "    # ========== å˜èªæŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ==========\n",
    "    words_raw = re.findall(r'[a-z]+', domain_body)\n",
    "    \n",
    "    # æ¡ä»¶ä»˜ããƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "    for word in words_raw:\n",
    "        # å˜ç‹¬ã®'jp'ã‚„'co'ã®æ‰±ã„\n",
    "        if word == 'jp':\n",
    "            # å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆã®ã¿ãƒªã‚¹ã‚¯ã¨ã—ã¦æ‰±ã†\n",
    "            if any(p in features['risk_patterns'] for p in \n",
    "                   ['fake_cojp_absolute', 'jp_hyphen_variant']):\n",
    "                features['risk_patterns'].append('jp_in_suspicious_context')\n",
    "                features['risk_scores']['jp_in_suspicious_context'] = 40\n",
    "            # ãã‚Œä»¥å¤–ã¯ç„¡è¦–ï¼ˆæ­£å¸¸ãªä½¿ç”¨ã®å¯èƒ½æ€§ï¼‰\n",
    "            \n",
    "        elif word == 'co':\n",
    "            # å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆã®ã¿ãƒªã‚¹ã‚¯ã¨ã—ã¦æ‰±ã†\n",
    "            if any(p in features['risk_patterns'] for p in \n",
    "                   ['fake_cojp_absolute', 'co_hyphen_variant']):\n",
    "                features['risk_patterns'].append('co_in_suspicious_context')\n",
    "                features['risk_scores']['co_in_suspicious_context'] = 35\n",
    "            # ãã‚Œä»¥å¤–ã¯ç„¡è¦–\n",
    "            \n",
    "        elif word == 'www':\n",
    "            # wwwã¯å®Œå…¨ã«ç„¡è¦–ï¼ˆæ—¢ã«é™¤å»æ¸ˆã¿ï¼‰\n",
    "            pass\n",
    "            \n",
    "        else:\n",
    "            # é€šå¸¸ã®å˜èª\n",
    "            if 2 <= len(word) <= 15:\n",
    "                features['clean_words'].append(word)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# ========== 3. ãƒªã‚¹ã‚¯è©•ä¾¡é–¢æ•°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ==========\n",
    "\n",
    "def calculate_risk_score_improved(domain: str) -> Dict:\n",
    "    \"\"\"\n",
    "    åˆ†æçµæœã‚’æ´»ã‹ã—ãŸç·åˆãƒªã‚¹ã‚¯è©•ä¾¡\n",
    "    \"\"\"\n",
    "    \n",
    "    features = extract_features_improved(domain)\n",
    "    \n",
    "    result = {\n",
    "        'domain': domain,\n",
    "        'total_risk': 0,\n",
    "        'confidence': 0,\n",
    "        'risk_breakdown': {},\n",
    "        'verdict': 'unknown',\n",
    "        'explanation': []\n",
    "    }\n",
    "    \n",
    "    # ========== ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¹ã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰ ==========\n",
    "    pattern_risk = 0\n",
    "    if features['risk_patterns']:\n",
    "        # æœ€ã‚‚é«˜ã„ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã‚’æ¡ç”¨\n",
    "        pattern_risk = max(features['risk_scores'].values())\n",
    "        result['risk_breakdown']['pattern'] = pattern_risk\n",
    "        \n",
    "        # èª¬æ˜ã‚’è¿½åŠ \n",
    "        for pattern in features['risk_patterns']:\n",
    "            if pattern == 'fake_cojp_absolute':\n",
    "                result['explanation'].append(\"ç¢ºå®Ÿãª.co.jpå½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³\")\n",
    "            elif pattern.startswith('embedded_'):\n",
    "                domain_name = pattern.replace('embedded_', '').replace('_', '.')\n",
    "                result['explanation'].append(f\"{domain_name}ã®åŸ‹ã‚è¾¼ã¿å½è£…\")\n",
    "            elif pattern == 'duckdns':\n",
    "                result['explanation'].append(\"DuckDNSï¼ˆãƒ•ãƒªãƒ¼DNSï¼‰ä½¿ç”¨\")\n",
    "    \n",
    "    # ========== TLDãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¹ã‚¯ ==========\n",
    "    tld_risk = 0\n",
    "    if 'TLD_STATS' in globals() and features['tld_info'].get('tld'):\n",
    "        tld = features['tld_info']['tld']\n",
    "        if tld in TLD_STATS:\n",
    "            stats = TLD_STATS[tld]\n",
    "            odds_ratio = stats['odds_ratio']\n",
    "            \n",
    "            if odds_ratio == float('inf') or odds_ratio > 100:\n",
    "                tld_risk = 40\n",
    "                result['explanation'].append(f\"å±é™ºTLD: .{tld}\")\n",
    "            elif odds_ratio > 10:\n",
    "                tld_risk = 25\n",
    "                result['explanation'].append(f\"è¦æ³¨æ„TLD: .{tld}\")\n",
    "            elif odds_ratio < 0.1:\n",
    "                tld_risk = -20  # å®‰å…¨ãªTLDã¯ãƒªã‚¹ã‚¯ã‚’ä¸‹ã’ã‚‹\n",
    "                result['explanation'].append(f\"ä¿¡é ¼TLD: .{tld}\")\n",
    "    \n",
    "    result['risk_breakdown']['tld'] = tld_risk\n",
    "    \n",
    "    # ========== å˜èªãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¹ã‚¯ï¼ˆè£œåŠ©çš„ï¼‰ ==========\n",
    "    word_risk = 0\n",
    "    if 'suspicious_words_stats' in globals() and features['clean_words']:\n",
    "        risk_words = []\n",
    "        for word in features['clean_words']:\n",
    "            if word in suspicious_words_stats:\n",
    "                stats = suspicious_words_stats[word]\n",
    "                if stats['odds_ratio'] > 50:\n",
    "                    risk_words.append(word)\n",
    "                    word_risk += 5\n",
    "        \n",
    "        if risk_words:\n",
    "            word_risk = min(word_risk, 30)  # æœ€å¤§30ç‚¹\n",
    "            result['risk_breakdown']['words'] = word_risk\n",
    "            result['explanation'].append(f\"å±é™ºå˜èª: {', '.join(risk_words[:3])}\")\n",
    "    \n",
    "    # ========== æ§‹é€ çš„ãƒªã‚¹ã‚¯ ==========\n",
    "    structure_risk = 0\n",
    "    if features['structure']['depth'] > 4:\n",
    "        structure_risk += 15\n",
    "        result['explanation'].append(f\"éšå±¤ãŒæ·±ã„: {features['structure']['depth']}å±¤\")\n",
    "    \n",
    "    if features['structure']['length'] > 50:\n",
    "        structure_risk += 10\n",
    "        result['explanation'].append(f\"ç•°å¸¸ã«é•·ã„: {features['structure']['length']}æ–‡å­—\")\n",
    "    \n",
    "    result['risk_breakdown']['structure'] = structure_risk\n",
    "    \n",
    "    # ========== ç·åˆè©•ä¾¡ ==========\n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒªã‚¹ã‚¯ã‚’é‡è¦–\n",
    "    if pattern_risk >= 80:\n",
    "        # ç¢ºå®Ÿãªå½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        result['total_risk'] = pattern_risk\n",
    "        result['confidence'] = 0.95\n",
    "    elif pattern_risk >= 60:\n",
    "        # é«˜ãƒªã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ + ä»–ã®è¦å› \n",
    "        result['total_risk'] = pattern_risk + (tld_risk + word_risk + structure_risk) * 0.3\n",
    "        result['confidence'] = 0.80\n",
    "    else:\n",
    "        # é€šå¸¸ã®é‡ã¿ä»˜ã‘\n",
    "        result['total_risk'] = pattern_risk * 0.5 + tld_risk * 0.3 + word_risk * 0.15 + structure_risk * 0.05\n",
    "        result['confidence'] = min(0.7, result['total_risk'] / 100)\n",
    "    \n",
    "    # æ­£è¦åŒ–\n",
    "    result['total_risk'] = min(100, max(0, result['total_risk']))\n",
    "    \n",
    "    # åˆ¤å®š\n",
    "    if result['total_risk'] >= 60:\n",
    "        result['verdict'] = 'phishing'\n",
    "    elif result['total_risk'] <= 30:\n",
    "        result['verdict'] = 'legitimate'\n",
    "    else:\n",
    "        result['verdict'] = 'suspicious'\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ========== 4. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ==========\n",
    "\n",
    "print(\"\\nğŸ§ª æ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_cases = [\n",
    "    # ç¢ºå®Ÿãªãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°\n",
    "    'amazon.co.jp.suppor34.frhnkm.ph',\n",
    "    'amazon-co-jp.com',\n",
    "    'rakuten-jp.net',\n",
    "    \n",
    "    # æ­£è¦ã‚µã‚¤ãƒˆ\n",
    "    'kobe-denki.co.jp',\n",
    "    'amazon.co.jp',\n",
    "    'google.com',\n",
    "    \n",
    "    # ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³\n",
    "    'jp-bank.com',\n",
    "    'secure-payment.tk',\n",
    "    'duckdns.org'\n",
    "]\n",
    "\n",
    "for domain in test_cases:\n",
    "    result = calculate_risk_score_improved(domain)\n",
    "    \n",
    "    # åˆ¤å®šãƒãƒ¼ã‚¯\n",
    "    if result['verdict'] == 'phishing':\n",
    "        mark = 'ğŸš¨'\n",
    "    elif result['verdict'] == 'legitimate':\n",
    "        mark = 'âœ…'\n",
    "    else:\n",
    "        mark = 'âš ï¸'\n",
    "    \n",
    "    print(f\"\\n{domain}\")\n",
    "    print(f\"  {mark} åˆ¤å®š: {result['verdict']} (ãƒªã‚¹ã‚¯: {result['total_risk']:.1f}, ä¿¡é ¼åº¦: {result['confidence']:.2f})\")\n",
    "    if result['explanation']:\n",
    "        print(f\"  ç†ç”±: {'; '.join(result['explanation'][:3])}\")\n",
    "\n",
    "# ========== 5. çµ±è¨ˆã‚µãƒãƒªãƒ¼ ==========\n",
    "\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    # å•é¡Œã®ã‚ã‚‹å˜èªã®é™¤å¤–ç¢ºèª\n",
    "    problematic_words = ['jp', 'co', 'www']\n",
    "    clean_high_risk = [w for w in HIGH_RISK_WORDS if w not in problematic_words]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š æ”¹è‰¯ã®åŠ¹æœ:\")\n",
    "    print(f\"  - å…ƒã®é«˜ãƒªã‚¹ã‚¯å˜èª: {len(HIGH_RISK_WORDS)}å€‹\")\n",
    "    print(f\"  - å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–: {len(HIGH_RISK_WORDS) - len(clean_high_risk)}å€‹\")\n",
    "    print(f\"  - ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚’å„ªå…ˆ: å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯95ç‚¹ä»¥ä¸Š\")\n",
    "    print(f\"  - TLDçµ±è¨ˆã‚’æ´»ç”¨: {len(TLD_STATS) if 'TLD_STATS' in globals() else 0}ç¨®é¡\")\n",
    "\n",
    "# ========== 6. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ ==========\n",
    "\n",
    "globals()['extract_features_improved'] = extract_features_improved\n",
    "globals()['calculate_risk_score_improved'] = calculate_risk_score_improved\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… æ”¹è‰¯ç‰ˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ’¡ æ”¹è‰¯ã®ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"  1. å½è£…ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ10,695ä»¶ï¼‰ã‚’æœ€å„ªå…ˆã§æ¤œå‡º\")\n",
    "print(\"  2. åŸ‹ã‚è¾¼ã¿å½è£…ï¼ˆ1,190ä»¶ï¼‰ã‚’ç¢ºå®Ÿã«æ¤œå‡º\")\n",
    "print(\"  3. 'jp'ã‚„'co'ã¯æ–‡è„ˆã‚’è¦‹ã¦åˆ¤æ–­\")\n",
    "print(\"  4. wwwã‚’å®Œå…¨ã«é™¤å¤–ï¼ˆ59,046ä»¶ã®èª¤æ¤œå‡ºã‚’é˜²ãï¼‰\")\n",
    "print(\"  5. IPã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æ­£ã—ãèªè­˜\")\n",
    "print(\"  6. TLDçµ±è¨ˆï¼ˆ810ç¨®é¡ï¼‰ã‚’æ´»ç”¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "233e8f19-3772-4df8-91f7-1aeb3f4d410f",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[12]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5938040-c56f-4ebb-9447-ce5602126cfb",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ã‚»ãƒ«05å®Ÿè¡Œå®Œäº†: 4ã¤ã®ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°æ¤œçŸ¥ãƒ„ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¾ã—ãŸ\n",
      "  - brand_impersonation_check: ãƒ–ãƒ©ãƒ³ãƒ‰å½è£…ã¨ã‚¿ã‚¤ãƒã‚¹ã‚¯ãƒ¯ãƒƒãƒ†ã‚£ãƒ³ã‚°æ¤œå‡º\n",
      "  - certificate_analysis: è¨¼æ˜æ›¸åˆ†æï¼ˆç„¡æ–™CAãƒ»çµ„ç¹”åé‡è¦–ï¼‰\n",
      "  - short_domain_analysis: çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨æ§‹é€ åˆ†æ\n",
      "  - contextual_risk_assessment: æ–‡è„ˆçš„ãƒªã‚¹ã‚¯è©•ä¾¡\n",
      "\n",
      "ç‰¹å¾´:\n",
      "  - ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ï¼ˆURLã§ã¯ãªãï¼‰ã§å‹•ä½œ\n",
      "  - ä½MLç¢ºç‡(<0.2)ã‚’é«˜ãƒªã‚¹ã‚¯ã¨ã—ã¦æ‰±ã†é€†è»¢ã®ç™ºæƒ³\n",
      "  - å½é™°æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é‡ç‚¹æ¤œå‡º\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 05\n",
    "æ¦‚è¦: å½é™°æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã®ãŸã‚ã®4ã¤ã®ãƒ„ãƒ¼ãƒ«å®šç¾©ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ã€LLMçµ±åˆï¼‰\n",
    "å…¥åŠ›: brand_keywords, DANGEROUS_TLDS, HIGH_RISK_WORDS, cert_full_info_map\n",
    "å‡ºåŠ›: 4ã¤ã®æ¤œå‡ºãƒ„ãƒ¼ãƒ«é–¢æ•°ï¼ˆLangChainå½¢å¼ï¼‰\n",
    "\"\"\"\n",
    "\n",
    "from langchain.tools import tool\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# ãƒ„ãƒ¼ãƒ«1: ãƒ–ãƒ©ãƒ³ãƒ‰å½è£…ãƒã‚§ãƒƒã‚¯\n",
    "@tool\n",
    "def brand_impersonation_check(domain: str, ml_probability: float) -> Dict:\n",
    "    \"\"\"\n",
    "    ãƒ–ãƒ©ãƒ³ãƒ‰åã®å½è£…ã‚„é¡ä¼¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æ¤œå‡ºï¼ˆ64ãƒ–ãƒ©ãƒ³ãƒ‰å¯¾å¿œï¼‰\n",
    "    å½é™°æ€§ã®58.4%ãŒ.comãƒ‰ãƒ¡ã‚¤ãƒ³ã§ç™ºç”Ÿã—ã¦ã„ã‚‹ã“ã¨ã‚’è€ƒæ…®\n",
    "    \n",
    "    Args:\n",
    "        domain: æ¤œæŸ»å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆä¾‹: example.comï¼‰\n",
    "        ml_probability: MLãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡\n",
    "    \"\"\"\n",
    "    domain_lower = domain.lower()\n",
    "    domain_parts = domain_lower.replace('-', '.').replace('_', '.').split('.')\n",
    "    \n",
    "    result = {\n",
    "        'is_impersonation': False,\n",
    "        'confidence': 0.0,\n",
    "        'detected_brands': [],\n",
    "        'reasons': [],\n",
    "        'risk_factors': {}\n",
    "    }\n",
    "    \n",
    "    # ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œå‡ºã¨ã‚¿ã‚¤ãƒã‚¹ã‚¯ãƒ¯ãƒƒãƒ†ã‚£ãƒ³ã‚°è©•ä¾¡\n",
    "    for brand in brand_keywords:\n",
    "        brand_lower = brand.lower()\n",
    "        \n",
    "        # å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯\n",
    "        if brand_lower in domain_lower:\n",
    "            result['detected_brands'].append(brand)\n",
    "            result['is_impersonation'] = True\n",
    "            \n",
    "            # TLDæŠ½å‡º\n",
    "            tld = domain_parts[-1] if len(domain_parts) > 1 else ''\n",
    "            \n",
    "            # å±é™ºTLDã¨ã®çµ„ã¿åˆã‚ã›ãƒã‚§ãƒƒã‚¯\n",
    "            if tld in DANGEROUS_TLDS:\n",
    "                result['risk_factors']['dangerous_tld_with_brand'] = True\n",
    "                result['confidence'] = max(result['confidence'], 0.95)\n",
    "                result['reasons'].append(f\"å±é™ºTLD .{tld} + ãƒ–ãƒ©ãƒ³ãƒ‰ '{brand}'\")\n",
    "            \n",
    "            # .com + ä½MLç¢ºç‡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå½é™°æ€§ã®58.4%ï¼‰\n",
    "            elif tld == 'com' and ml_probability < 0.2:\n",
    "                result['risk_factors']['com_low_ml_with_brand'] = True\n",
    "                result['confidence'] = max(result['confidence'], 0.90)\n",
    "                result['reasons'].append(f\".com + ä½MLç¢ºç‡({ml_probability:.2f}) + ãƒ–ãƒ©ãƒ³ãƒ‰ '{brand}'\")\n",
    "        \n",
    "        # ãƒ¬ãƒ¼ãƒ™ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³è·é›¢ã«ã‚ˆã‚‹ã‚¿ã‚¤ãƒã‚¹ã‚¯ãƒ¯ãƒƒãƒ†ã‚£ãƒ³ã‚°æ¤œå‡º\n",
    "        for part in domain_parts[:-1]:  # TLDä»¥å¤–ã®éƒ¨åˆ†\n",
    "            if len(part) > 3 and len(brand_lower) > 3:\n",
    "                lev_dist = levenshtein_distance(part, brand_lower)\n",
    "                similarity = 1 - (lev_dist / max(len(part), len(brand_lower)))\n",
    "                \n",
    "                if 0.7 < similarity < 1.0:  # 70-99%ã®é¡ä¼¼åº¦\n",
    "                    result['is_impersonation'] = True\n",
    "                    result['risk_factors']['typosquatting'] = True\n",
    "                    result['confidence'] = max(result['confidence'], 0.75)\n",
    "                    result['reasons'].append(f\"ã‚¿ã‚¤ãƒã‚¹ã‚¯ãƒ¯ãƒƒãƒ†ã‚£ãƒ³ã‚°: '{part}' â‰ˆ '{brand}' (é¡ä¼¼åº¦: {similarity:.2%})\")\n",
    "                    result['detected_brands'].append(f\"{brand}(typo)\")\n",
    "    \n",
    "    # MLç¢ºç‡ãŒä½ã„å ´åˆã®è¿½åŠ è©•ä¾¡\n",
    "    if result['detected_brands'] and ml_probability < 0.2:\n",
    "        result['risk_factors']['brand_with_low_ml'] = True\n",
    "        result['confidence'] = max(result['confidence'], 0.85)\n",
    "        result['reasons'].append(f\"ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œå‡º + æ¥µä½MLç¢ºç‡({ml_probability:.2f})\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ãƒ„ãƒ¼ãƒ«2: è¨¼æ˜æ›¸åˆ†æ\n",
    "@tool\n",
    "def certificate_analysis(domain: str, ml_probability: float) -> Dict:\n",
    "    \"\"\"\n",
    "    è¨¼æ˜æ›¸æƒ…å ±ã‚’åˆ†æï¼ˆå½é™°æ€§ã®100%ãŒè¨¼æ˜æ›¸æŒã¡ã€80.1%ãŒç„¡æ–™CAï¼‰\n",
    "    \n",
    "    Args:\n",
    "        domain: æ¤œæŸ»å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
    "        ml_probability: MLãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'is_suspicious': False,\n",
    "        'confidence': 0.0,\n",
    "        'risk_score': 0.0,\n",
    "        'reasons': [],\n",
    "        'risk_factors': {},\n",
    "        'is_free_ca': False,\n",
    "        'certificate_age_days': None\n",
    "    }\n",
    "    \n",
    "    # cert_full_info_mapã‹ã‚‰è¨¼æ˜æ›¸æƒ…å ±ã‚’å–å¾—\n",
    "    cert_info = cert_full_info_map.get(domain, {}) if 'cert_full_info_map' in globals() else {}\n",
    "    \n",
    "    if not cert_info:\n",
    "        # è¨¼æ˜æ›¸ãªã—ï¼ˆå½é™°æ€§ã§ã¯0%ãªã®ã§ã€é€†ã«å®‰å…¨ã®å¯èƒ½æ€§ï¼‰\n",
    "        result['risk_factors']['no_certificate'] = True\n",
    "        return result\n",
    "    \n",
    "    # ç„¡æ–™CAã®ãƒªã‚¹ãƒˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰\n",
    "    free_cas = ['Let\\'s Encrypt', 'Cloudflare', 'cPanel', 'Sectigo', 'ZeroSSL', 'R3', 'R10', 'R11']\n",
    "    issuer = cert_info.get('issuer_o', '') or cert_info.get('issuer_cn', '')\n",
    "    \n",
    "    # ç„¡æ–™CAåˆ¤å®š\n",
    "    is_free_ca = any(ca.lower() in issuer.lower() for ca in free_cas)\n",
    "    result['is_free_ca'] = is_free_ca\n",
    "    \n",
    "    if is_free_ca:\n",
    "        result['risk_factors']['free_ca'] = True\n",
    "        result['risk_score'] = 0.6\n",
    "        \n",
    "        # ä½MLç¢ºç‡ + ç„¡æ–™CAï¼ˆå½é™°æ€§ã®80.1%ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰\n",
    "        if ml_probability < 0.2:\n",
    "            result['confidence'] = max(result['confidence'], 0.75)\n",
    "            result['risk_score'] = 0.8\n",
    "            result['reasons'].append(f\"ç„¡æ–™CA '{issuer}' + ä½MLç¢ºç‡({ml_probability:.2f})\")\n",
    "        elif ml_probability < 0.5:\n",
    "            result['confidence'] = max(result['confidence'], 0.60)\n",
    "            result['risk_score'] = 0.7\n",
    "            result['reasons'].append(f\"ç„¡æ–™CA '{issuer}' + ä¸­MLç¢ºç‡({ml_probability:.2f})\")\n",
    "    \n",
    "    # è¨¼æ˜æ›¸ã®å¹´é½¢åˆ†æ\n",
    "    days_remaining = cert_info.get('days_remaining')\n",
    "    if days_remaining is not None:\n",
    "        # è¨¼æ˜æ›¸ã®æ¨å®šå¹´é½¢ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“çš„ã«ï¼‰\n",
    "        cert_age_days = 365 - days_remaining if days_remaining < 365 else 365\n",
    "        result['certificate_age_days'] = cert_age_days\n",
    "        \n",
    "        # æ–°ã—ã™ãã‚‹è¨¼æ˜æ›¸ï¼ˆ30æ—¥ä»¥å†…ï¼‰\n",
    "        if cert_age_days < 30:\n",
    "            result['risk_factors']['very_new_cert'] = True\n",
    "            result['confidence'] = max(result['confidence'], 0.40)\n",
    "            result['reasons'].append(f\"éå¸¸ã«æ–°ã—ã„è¨¼æ˜æ›¸ï¼ˆ{cert_age_days}æ—¥ï¼‰\")\n",
    "        \n",
    "        # æœŸé™åˆ‡ã‚Œé–“è¿‘ï¼ˆ7æ—¥ä»¥å†…ï¼‰\n",
    "        if days_remaining < 7:\n",
    "            result['risk_factors']['expiring_soon'] = True\n",
    "            result['confidence'] = max(result['confidence'], 0.55)\n",
    "            result['reasons'].append(f\"è¨¼æ˜æ›¸æœŸé™åˆ‡ã‚Œé–“è¿‘ï¼ˆæ®‹{days_remaining}æ—¥ï¼‰\")\n",
    "    \n",
    "    # çµ„ç¹”åãªã—ï¼ˆãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã®99.7%ï¼‰\n",
    "    if not cert_info.get('subject_o'):\n",
    "        result['risk_factors']['no_organization'] = True\n",
    "        result['confidence'] = max(result['confidence'], 0.65)\n",
    "        result['reasons'].append(\"è¨¼æ˜æ›¸ã«çµ„ç¹”åãªã—\")\n",
    "    \n",
    "    result['is_suspicious'] = result['confidence'] > 0.5\n",
    "    return result\n",
    "\n",
    "# ãƒ„ãƒ¼ãƒ«3: ã‚·ãƒ§ãƒ¼ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³åˆ†æ\n",
    "@tool \n",
    "def short_domain_analysis(domain: str, ml_probability: float) -> Dict:\n",
    "    \"\"\"\n",
    "    çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨æ§‹é€ åˆ†æï¼ˆ10æ–‡å­—æœªæº€ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã§å½é™°æ€§ãŒå¤šã„ï¼‰\n",
    "    \n",
    "    Args:\n",
    "        domain: æ¤œæŸ»å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
    "        ml_probability: MLãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡\n",
    "    \"\"\"\n",
    "    # TLDé™¤å»\n",
    "    domain_parts = domain.lower().split('.')\n",
    "    domain_without_tld = '.'.join(domain_parts[:-1]) if len(domain_parts) > 1 else domain\n",
    "    tld = domain_parts[-1] if len(domain_parts) > 1 else ''\n",
    "    \n",
    "    result = {\n",
    "        'is_suspicious': False,\n",
    "        'confidence': 0.0,\n",
    "        'reasons': [],\n",
    "        'risk_factors': {},\n",
    "        'entropy': 0.0,\n",
    "        'domain_length': len(domain_without_tld)\n",
    "    }\n",
    "    \n",
    "    # çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆ10æ–‡å­—æœªæº€ï¼‰ã®ç‰¹åˆ¥å‡¦ç†\n",
    "    if len(domain_without_tld) < 10:\n",
    "        result['risk_factors']['short_domain'] = True\n",
    "        \n",
    "        # çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ + ä½MLç¢ºç‡\n",
    "        if ml_probability < 0.3:\n",
    "            result['confidence'] = max(result['confidence'], 0.65)\n",
    "            result['reasons'].append(f\"çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³({len(domain_without_tld)}æ–‡å­—) + ä½MLç¢ºç‡\")\n",
    "    \n",
    "    # å±é™ºTLDåˆ¤å®š\n",
    "    if tld in DANGEROUS_TLDS:\n",
    "        result['risk_factors']['dangerous_tld'] = True\n",
    "        result['confidence'] = max(result['confidence'], 0.80)\n",
    "        result['reasons'].append(f\"å±é™ºTLD: .{tld}\")\n",
    "    \n",
    "    # .comãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆå½é™°æ€§ã®58.4%ï¼‰\n",
    "    elif tld == 'com':\n",
    "        if ml_probability < 0.2:\n",
    "            result['risk_factors']['com_with_low_ml'] = True\n",
    "            result['confidence'] = max(result['confidence'], 0.70)\n",
    "            result['reasons'].append(f\".com + æ¥µä½MLç¢ºç‡({ml_probability:.2f})\")\n",
    "    \n",
    "    # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æ•°\n",
    "    subdomain_count = len(domain_parts) - 2  # ãƒ¡ã‚¤ãƒ³ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨TLDã‚’é™¤ã\n",
    "    if subdomain_count > 2:\n",
    "        result['risk_factors']['excessive_subdomains'] = True\n",
    "        result['confidence'] = max(result['confidence'], 0.60)\n",
    "        result['reasons'].append(f\"éå‰°ãªã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³: {subdomain_count}å€‹\")\n",
    "    \n",
    "    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—\n",
    "    def calculate_entropy(text):\n",
    "        if not text:\n",
    "            return 0\n",
    "        prob = [float(text.count(c)) / len(text) for c in set(text)]\n",
    "        return -sum(p * math.log(p, 2) for p in prob if p > 0)\n",
    "    \n",
    "    entropy = calculate_entropy(domain_without_tld)\n",
    "    result['entropy'] = entropy\n",
    "    \n",
    "    if entropy > 4.0:  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = ãƒ©ãƒ³ãƒ€ãƒ ã£ã½ã„\n",
    "        result['risk_factors']['high_entropy'] = True\n",
    "        result['confidence'] = max(result['confidence'], 0.55)\n",
    "        result['reasons'].append(f\"é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ‰ãƒ¡ã‚¤ãƒ³: {entropy:.2f}\")\n",
    "    \n",
    "    # ãƒã‚¤ãƒ•ãƒ³ã®éå‰°ä½¿ç”¨\n",
    "    hyphen_count = domain.count('-')\n",
    "    if hyphen_count > 2:\n",
    "        result['risk_factors']['excessive_hyphens'] = True\n",
    "        result['confidence'] = max(result['confidence'], 0.45)\n",
    "        result['reasons'].append(f\"éå‰°ãªãƒã‚¤ãƒ•ãƒ³: {hyphen_count}å€‹\")\n",
    "    \n",
    "    # æ•°å­—ã®å‰²åˆ\n",
    "    digit_ratio = sum(c.isdigit() for c in domain_without_tld) / len(domain_without_tld) if domain_without_tld else 0\n",
    "    if digit_ratio > 0.3:\n",
    "        result['risk_factors']['high_digit_ratio'] = True\n",
    "        result['confidence'] = max(result['confidence'], 0.50)\n",
    "        result['reasons'].append(f\"é«˜ã„æ•°å­—æ¯”ç‡: {digit_ratio:.1%}\")\n",
    "    \n",
    "    result['is_suspicious'] = result['confidence'] > 0.5\n",
    "    return result\n",
    "\n",
    "# ãƒ„ãƒ¼ãƒ«4: æ–‡è„ˆçš„ãƒªã‚¹ã‚¯è©•ä¾¡\n",
    "@tool\n",
    "def contextual_risk_assessment(\n",
    "    domain: str,\n",
    "    ml_probability: float,\n",
    "    brand_result: Optional[Dict] = None,\n",
    "    cert_result: Optional[Dict] = None,\n",
    "    domain_result: Optional[Dict] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    å…¨ä½“çš„ãªæ–‡è„ˆã‚’è€ƒæ…®ã—ãŸãƒªã‚¹ã‚¯è©•ä¾¡\n",
    "    ä½MLç¢ºç‡ï¼ˆ<0.2ï¼‰ã‚’é«˜ãƒªã‚¹ã‚¯ã¨ã—ã¦ç‰¹åˆ¥æ‰±ã„ï¼ˆå½é™°æ€§ã®52.1%ï¼‰\n",
    "    \n",
    "    Args:\n",
    "        domain: æ¤œæŸ»å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
    "        ml_probability: MLãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡\n",
    "        brand_result: ãƒ–ãƒ©ãƒ³ãƒ‰å½è£…ãƒã‚§ãƒƒã‚¯ã®çµæœ\n",
    "        cert_result: è¨¼æ˜æ›¸åˆ†æã®çµæœ\n",
    "        domain_result: ãƒ‰ãƒ¡ã‚¤ãƒ³æ§‹é€ åˆ†æã®çµæœ\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'risk_level': 'low',\n",
    "        'confidence': 0.0,\n",
    "        'risk_score': 0.0,\n",
    "        'aggregated_reasons': [],\n",
    "        'key_findings': {},\n",
    "        'ml_paradox': False  # MLç¢ºç‡ãŒä½ã„ã®ã«å±é™ºãªå…†å€™ãŒã‚ã‚‹\n",
    "    }\n",
    "    \n",
    "    # HIGH_RISK_WORDSã‚’ä½¿ã£ãŸå˜èªãƒã‚§ãƒƒã‚¯\n",
    "    detected_risk_words = []\n",
    "    domain_lower = domain.lower()\n",
    "    \n",
    "    if 'HIGH_RISK_WORDS' in globals():\n",
    "        for word in HIGH_RISK_WORDS:\n",
    "            if word.lower() in domain_lower:\n",
    "                detected_risk_words.append(word)\n",
    "    \n",
    "    if detected_risk_words:\n",
    "        result['key_findings']['high_risk_words'] = detected_risk_words\n",
    "        result['aggregated_reasons'].append(f\"é«˜ãƒªã‚¹ã‚¯å˜èª: {', '.join(detected_risk_words[:3])}\")\n",
    "        result['confidence'] = max(result['confidence'], 0.60)\n",
    "    \n",
    "    # MLç¢ºç‡ã«ã‚ˆã‚‹åŸºæœ¬ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ï¼ˆé€†è»¢ã®ç™ºæƒ³ï¼‰\n",
    "    if ml_probability < 0.2:\n",
    "        # å½é™°æ€§ã®52.1%ãŒã“ã®é ˜åŸŸ\n",
    "        ml_risk_score = 0.7  # é«˜ãƒªã‚¹ã‚¯\n",
    "        result['key_findings']['low_ml_probability'] = True\n",
    "        result['ml_paradox'] = True\n",
    "        result['aggregated_reasons'].append(f\"MLç¢ºç‡æ¥µä½({ml_probability:.2f}) = å·§å¦™ãªå½è£…ã®å¯èƒ½æ€§å¤§\")\n",
    "    elif ml_probability < 0.3:\n",
    "        ml_risk_score = 0.5\n",
    "        result['ml_paradox'] = True\n",
    "    elif ml_probability < 0.5:\n",
    "        ml_risk_score = 0.3\n",
    "    else:\n",
    "        ml_risk_score = 0.2\n",
    "    \n",
    "    # å„ãƒ„ãƒ¼ãƒ«çµæœã®çµ±åˆ\n",
    "    tool_confidences = []\n",
    "    \n",
    "    if brand_result:\n",
    "        tool_confidences.append(brand_result.get('confidence', 0))\n",
    "        if brand_result.get('detected_brands'):\n",
    "            result['key_findings']['brands'] = brand_result['detected_brands']\n",
    "    \n",
    "    if cert_result:\n",
    "        tool_confidences.append(cert_result.get('confidence', 0))\n",
    "        if cert_result.get('is_free_ca'):\n",
    "            result['key_findings']['free_ca'] = True\n",
    "    \n",
    "    if domain_result:\n",
    "        tool_confidences.append(domain_result.get('confidence', 0))\n",
    "        if domain_result.get('risk_factors', {}).get('short_domain'):\n",
    "            result['key_findings']['short_domain'] = True\n",
    "    \n",
    "    # é‡ã¿ä»˜ãçµ±åˆã‚¹ã‚³ã‚¢\n",
    "    if tool_confidences:\n",
    "        avg_tool_confidence = np.mean(tool_confidences)\n",
    "        result['risk_score'] = (ml_risk_score * 0.4) + (avg_tool_confidence * 0.6)\n",
    "    else:\n",
    "        result['risk_score'] = ml_risk_score\n",
    "    \n",
    "    result['confidence'] = result['risk_score']\n",
    "    \n",
    "    # ç‰¹åˆ¥ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º\n",
    "    critical_patterns = []\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ä½ML + ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œå‡º\n",
    "    if ml_probability < 0.2 and brand_result and brand_result.get('detected_brands'):\n",
    "        critical_patterns.append(\"ä½ML+ãƒ–ãƒ©ãƒ³ãƒ‰\")\n",
    "        result['confidence'] = max(result['confidence'], 0.90)\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ä½ML + ç„¡æ–™CA\n",
    "    if ml_probability < 0.2 and cert_result and cert_result.get('is_free_ca'):\n",
    "        critical_patterns.append(\"ä½ML+ç„¡æ–™CA\")\n",
    "        result['confidence'] = max(result['confidence'], 0.85)\n",
    "    \n",
    "    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ + ä½ML\n",
    "    if ml_probability < 0.3 and domain_result and domain_result.get('risk_factors', {}).get('short_domain'):\n",
    "        critical_patterns.append(\"çŸ­ãƒ‰ãƒ¡ã‚¤ãƒ³+ä½ML\")\n",
    "        result['confidence'] = max(result['confidence'], 0.75)\n",
    "    \n",
    "    if critical_patterns:\n",
    "        result['key_findings']['critical_patterns'] = critical_patterns\n",
    "        result['aggregated_reasons'].append(f\"é‡å¤§ãƒ‘ã‚¿ãƒ¼ãƒ³: {', '.join(critical_patterns)}\")\n",
    "    \n",
    "    # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¤å®š\n",
    "    if result['confidence'] >= 0.7:\n",
    "        result['risk_level'] = 'high'\n",
    "    elif result['confidence'] >= 0.5:\n",
    "        result['risk_level'] = 'medium-high'\n",
    "    elif result['confidence'] >= 0.3:\n",
    "        result['risk_level'] = 'medium'\n",
    "    else:\n",
    "        result['risk_level'] = 'low'\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ãƒ„ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ä½¿ç”¨ï¼‰\n",
    "tools = [\n",
    "    brand_impersonation_check,\n",
    "    certificate_analysis,\n",
    "    short_domain_analysis,\n",
    "    contextual_risk_assessment\n",
    "]\n",
    "\n",
    "print(\"âœ… ã‚»ãƒ«05å®Ÿè¡Œå®Œäº†: 4ã¤ã®ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°æ¤œçŸ¥ãƒ„ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¾ã—ãŸ\")\n",
    "print(f\"  - brand_impersonation_check: ãƒ–ãƒ©ãƒ³ãƒ‰å½è£…ã¨ã‚¿ã‚¤ãƒã‚¹ã‚¯ãƒ¯ãƒƒãƒ†ã‚£ãƒ³ã‚°æ¤œå‡º\")\n",
    "print(f\"  - certificate_analysis: è¨¼æ˜æ›¸åˆ†æï¼ˆç„¡æ–™CAãƒ»çµ„ç¹”åé‡è¦–ï¼‰\")\n",
    "print(f\"  - short_domain_analysis: çŸ­ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨æ§‹é€ åˆ†æ\")\n",
    "print(f\"  - contextual_risk_assessment: æ–‡è„ˆçš„ãƒªã‚¹ã‚¯è©•ä¾¡\")\n",
    "print(f\"\\nç‰¹å¾´:\")\n",
    "print(f\"  - ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ï¼ˆURLã§ã¯ãªãï¼‰ã§å‹•ä½œ\")\n",
    "print(f\"  - ä½MLç¢ºç‡(<0.2)ã‚’é«˜ãƒªã‚¹ã‚¯ã¨ã—ã¦æ‰±ã†é€†è»¢ã®ç™ºæƒ³\")\n",
    "print(f\"  - å½é™°æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é‡ç‚¹æ¤œå‡º\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0827e67-550a-4440-8f40-19693629a311",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”§ LLM Client Initialization\n",
      "======================================================================\n",
      "âœ… LLM_TYPE: vllm\n",
      "âœ… VLLM_BASE_URL: http://192.168.100.71:30000/v1\n",
      "âš ï¸ VLLM_MODEL was undefined, set to: Qwen/Qwen3-14B-FP8\n",
      "âš ï¸ OLLAMA_MODEL was undefined, set to: qwen3:14b\n",
      "\n",
      "ğŸš€ Initializing LLM client...\n",
      "\n",
      "âœ… vLLM client initialized successfully\n",
      "   - Server: http://192.168.100.71:30000/v1\n",
      "   - Model: Qwen/Qwen3-14B-FP8\n",
      "\n",
      "ğŸ§ª Testing LLM client...\n",
      "âœ… LLM client is working properly\n",
      "âœ… tools: 4 tools available\n",
      "âœ… brand_keywords: 100 brands loaded\n",
      "\n",
      "======================================================================\n",
      "âœ… LLM Initialization Complete\n",
      "======================================================================\n",
      "\n",
      "You can now run the fixed Cell 06 code.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 06-å‰å‡¦ç†\n",
    "æ¦‚è¦: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆã‚»ãƒ«06å®Ÿè¡Œå‰ã«å¿…è¦ï¼‰\n",
    "å…¥åŠ›: LLMè¨­å®šå¤‰æ•°\n",
    "å‡ºåŠ›: llmï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼‰\n",
    "\"\"\"\n",
    "\n",
    "# ========== LLM Client Initialization Fix ==========\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”§ LLM Client Initialization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. å¿…è¦ãªå¤‰æ•°ã®ç¢ºèªã¨è¨­å®š\n",
    "if 'LLM_TYPE' not in globals():\n",
    "    # ã‚»ãƒ«01ã‹ã‚‰ã®å¤‰æ•°ã‚’ç¢ºèª\n",
    "    LLM_TYPE = \"vllm\"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n",
    "    print(f\"âš ï¸ LLM_TYPE was undefined, set to: {LLM_TYPE}\")\n",
    "else:\n",
    "    print(f\"âœ… LLM_TYPE: {LLM_TYPE}\")\n",
    "\n",
    "if 'VLLM_BASE_URL' not in globals():\n",
    "    VLLM_BASE_URL = \"http://192.168.100.71:30000/v1\"\n",
    "    print(f\"âš ï¸ VLLM_BASE_URL was undefined, set to: {VLLM_BASE_URL}\")\n",
    "else:\n",
    "    print(f\"âœ… VLLM_BASE_URL: {VLLM_BASE_URL}\")\n",
    "\n",
    "if 'VLLM_MODEL' not in globals():\n",
    "    VLLM_MODEL = \"Qwen/Qwen3-14B-FP8\"\n",
    "    print(f\"âš ï¸ VLLM_MODEL was undefined, set to: {VLLM_MODEL}\")\n",
    "else:\n",
    "    print(f\"âœ… VLLM_MODEL: {VLLM_MODEL}\")\n",
    "\n",
    "if 'OLLAMA_MODEL' not in globals():\n",
    "    OLLAMA_MODEL = 'qwen3:14b'\n",
    "    print(f\"âš ï¸ OLLAMA_MODEL was undefined, set to: {OLLAMA_MODEL}\")\n",
    "else:\n",
    "    print(f\"âœ… OLLAMA_MODEL: {OLLAMA_MODEL}\")\n",
    "\n",
    "# 2. LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–é–¢æ•°\n",
    "def initialize_llm_client():\n",
    "    \"\"\"Initialize LLM client with proper error handling\"\"\"\n",
    "    global llm\n",
    "    \n",
    "    try:\n",
    "        if LLM_TYPE == \"vllm\":\n",
    "            from langchain_openai import ChatOpenAI\n",
    "            \n",
    "            llm = ChatOpenAI(\n",
    "                model=VLLM_MODEL,\n",
    "                base_url=VLLM_BASE_URL,\n",
    "                api_key=\"EMPTY\",\n",
    "                temperature=0.1,\n",
    "                max_tokens=2048,\n",
    "                timeout=60,\n",
    "                max_retries=2\n",
    "            )\n",
    "            print(f\"\\nâœ… vLLM client initialized successfully\")\n",
    "            print(f\"   - Server: {VLLM_BASE_URL}\")\n",
    "            print(f\"   - Model: {VLLM_MODEL}\")\n",
    "            \n",
    "        else:  # Ollama\n",
    "            from langchain_ollama import ChatOllama\n",
    "            \n",
    "            llm = ChatOllama(\n",
    "                model=OLLAMA_MODEL,\n",
    "                temperature=0.1,\n",
    "                timeout=60\n",
    "            )\n",
    "            print(f\"\\nâœ… Ollama client initialized successfully\")\n",
    "            print(f\"   - Model: {OLLAMA_MODEL}\")\n",
    "        \n",
    "        return llm\n",
    "        \n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ Import error: {e}\")\n",
    "        print(\"   Please ensure langchain_openai or langchain_ollama is installed\")\n",
    "        raise\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LLM initialization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# 3. LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–\n",
    "print(\"\\nğŸš€ Initializing LLM client...\")\n",
    "llm = initialize_llm_client()\n",
    "\n",
    "# 4. åˆæœŸåŒ–ã®ç¢ºèªãƒ†ã‚¹ãƒˆ\n",
    "print(\"\\nğŸ§ª Testing LLM client...\")\n",
    "try:\n",
    "    # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "    test_response = llm.invoke(\"Say 'OK' if you are working\")\n",
    "    print(\"âœ… LLM client is working properly\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ LLM test failed: {e}\")\n",
    "    print(\"   The client is initialized but may have connection issues\")\n",
    "\n",
    "# 5. ä»–ã®å¿…è¦ãªå¤‰æ•°ã®ç¢ºèª\n",
    "required_vars = ['tools', 'brand_keywords']\n",
    "missing_vars = []\n",
    "\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        missing_vars.append(var)\n",
    "        print(f\"âš ï¸ {var} is not defined\")\n",
    "    else:\n",
    "        if var == 'tools':\n",
    "            print(f\"âœ… {var}: {len(globals()[var])} tools available\")\n",
    "        elif var == 'brand_keywords':\n",
    "            print(f\"âœ… {var}: {len(globals()[var])} brands loaded\")\n",
    "\n",
    "if missing_vars:\n",
    "    print(\"\\nâš ï¸ Warning: Some required variables are missing\")\n",
    "    print(\"   Please run the following cells before proceeding:\")\n",
    "    if 'tools' in missing_vars:\n",
    "        print(\"   - Cell 04 or 05: Tool definitions\")\n",
    "    if 'brand_keywords' in missing_vars:\n",
    "        print(\"   - Cell 01 or 02: Brand keywords loading\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… LLM Initialization Complete\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nYou can now run the fixed Cell 06 code.\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "globals()['llm'] = llm\n",
    "globals()['LLM_TYPE'] = LLM_TYPE\n",
    "globals()['VLLM_BASE_URL'] = VLLM_BASE_URL if LLM_TYPE == 'vllm' else None\n",
    "globals()['VLLM_MODEL'] = VLLM_MODEL if LLM_TYPE == 'vllm' else None\n",
    "globals()['OLLAMA_MODEL'] = OLLAMA_MODEL if LLM_TYPE == 'ollama' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55b2c8eb-3c8f-4d3c-aaa3-e52b97d29cff",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LangGraph-based PhishingDetectionAgent is defined and PHISHING_DETECTION_GRAPH is compiled.\n"
     ]
    }
   ],
   "source": [
    "# === ã‚»ãƒ«6ï¼ˆLangGraph-based Phishing Detection Agentï¼‰===\n",
    "# ç›®çš„: æ—¢å­˜ã® PhishingDetectionAgent ã‚¯ãƒ©ã‚¹ã‚’ LangGraph ã® StateGraph ãƒ™ãƒ¼ã‚¹å®Ÿè£…ã«ç½®ãæ›ãˆã‚‹\n",
    "# äº’æ›æ€§: æ—¢å­˜ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆevaluate / batch_evaluateï¼‰ã¨ evaluate_domain ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ç¶­æŒ\n",
    "# ä¾å­˜: ã‚»ãƒ«5.5ã® AgentState / Structured Output ãƒ¢ãƒ‡ãƒ«ç¾¤ï¼ˆBrandAnalysisResult, CertificateAnalysisResult, DomainAnalysisResult, PhishingDetectionResultï¼‰\n",
    "# å‚è€ƒ: ãƒãƒ¼ãƒ‰ã¨æ¡ä»¶ä»˜ãã‚¨ãƒƒã‚¸ã¯ä»•æ§˜æ›¸ã«æº–æ‹ ï¼ˆinitialize -> brand/cert -> domain -> decisionï¼‰\n",
    "# ãƒªãƒˆãƒ©ã‚¤/ãƒˆãƒ¬ãƒ¼ã‚¹: MAX_RETRIES=3ã€ENABLE_TRACING ã§è©³ç´°ãƒ­ã‚°\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import traceback\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "# --- LangGraph / LangChain imports (with fallbacks for compatibility) ---\n",
    "try:\n",
    "    from langgraph.graph import StateGraph, END, START\n",
    "except Exception as _e:\n",
    "    raise ImportError(\"LangGraph is required for this cell. Please install `langgraph`.\") from _e\n",
    "\n",
    "# BaseMessage compatibility (langchain v0.2+ uses langchain_core)\n",
    "try:\n",
    "    from langchain_core.messages import BaseMessage  # type: ignore\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.schema import BaseMessage  # type: ignore\n",
    "    except Exception:\n",
    "        class BaseMessage:  # minimal fallback\n",
    "            pass\n",
    "\n",
    "import json\n",
    "\n",
    "# --- Globals expected from previous cells ---\n",
    "# cfg: dict-like configuration\n",
    "# Tool functions (LangChain tools or plain functions):\n",
    "#   - brand_impersonation_check(domain, ml_probability, **kwargs) -> BrandAnalysisResult-like\n",
    "#   - certificate_analysis(domain, ml_probability, **kwargs) -> CertificateAnalysisResult-like\n",
    "#   - short_domain_analysis(domain, ml_probability, **kwargs) -> DomainAnalysisResult-like\n",
    "# Structured models from cell 5.5 (pydantic):\n",
    "#   - AgentState\n",
    "#   - BrandAnalysisResult, CertificateAnalysisResult, DomainAnalysisResult, PhishingDetectionResult\n",
    "# Dangerous TLD etc:\n",
    "#   - DANGEROUS_TLDS (list), HIGH_RISK_WORDS (list)  # optional\n",
    "#\n",
    "# ã“ã®ã‚»ãƒ«ã¯ã€ã“ã‚Œã‚‰ãŒæœªå®šç¾©ã§ã‚‚å‹•ä½œã‚’ç¶™ç¶šã§ãã‚‹ã‚ˆã†ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ã‚’æŒã¡ã¾ã™ï¼ˆæœ€å°é™ã®è¾æ›¸ã§ä»£æ›¿ï¼‰\n",
    "\n",
    "# -------- Safe getters / fallbacks --------\n",
    "def _get_cfg_thresholds():\n",
    "    low = 0.2\n",
    "    mid = 0.3\n",
    "    high = 0.7\n",
    "    try:\n",
    "        if 'cfg' in globals() and isinstance(cfg, dict):\n",
    "            low = cfg.get('analysis', {}).get('fn_threshold_low', low)\n",
    "            high = cfg.get('analysis', {}).get('fn_threshold_high', high)\n",
    "            # mid: ä»•æ§˜ä¸Š 0.3 ã‚’å›ºå®šçš„ã«ç”¨ã„ã‚‹\n",
    "    except Exception:\n",
    "        pass\n",
    "    return float(low), float(mid), float(high)\n",
    "\n",
    "def _structured(cls_name: str, data: Dict[str, Any]) -> Any:\n",
    "    \"\"\"Try to instantiate pydantic models if available; otherwise return dict.\"\"\"\n",
    "    g = globals()\n",
    "    model = g.get(cls_name)\n",
    "    if model is None:\n",
    "        return data\n",
    "    try:\n",
    "        # pydantic v2: model(**data) or model.model_validate\n",
    "        return model(**data)  # type: ignore\n",
    "    except Exception:\n",
    "        try:\n",
    "            return model.model_validate(data)  # type: ignore\n",
    "        except Exception:\n",
    "            return data\n",
    "\n",
    "def _to_dict(obj: Any) -> Dict[str, Any]:\n",
    "    if obj is None:\n",
    "        return {}\n",
    "    if isinstance(obj, dict):\n",
    "        return obj\n",
    "    # pydantic v2\n",
    "    for attr in (\"model_dump\", \"dict\", \"model_dump_json\"):\n",
    "        if hasattr(obj, attr):\n",
    "            try:\n",
    "                return getattr(obj, \"model_dump\")() if attr == \"model_dump\" else getattr(obj, \"dict\")()\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Dataclass or generic object\n",
    "    try:\n",
    "        import dataclasses\n",
    "        if dataclasses.is_dataclass(obj):\n",
    "            return dataclasses.asdict(obj)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Fallback: best-effort\n",
    "    try:\n",
    "        return json.loads(getattr(obj, \"json\")())\n",
    "    except Exception:\n",
    "        return {\"value\": repr(obj)}\n",
    "\n",
    "def _call_tool(tool_name: str, **kwargs) -> Dict[str, Any]:\n",
    "    fn = globals().get(tool_name)\n",
    "    if fn is None:\n",
    "        # Minimal default result when the tool is missing\n",
    "        return {\"success\": False, \"error\": f\"Tool `{tool_name}` not available\", \"reasons\": [f\"{tool_name} not defined\"]}\n",
    "    try:\n",
    "        res = fn(**kwargs)  # type: ignore\n",
    "        return _to_dict(res)\n",
    "    except Exception as e:\n",
    "        return {\"success\": False, \"error\": f\"{tool_name} failed: {e.__class__.__name__}: {e}\", \"traceback\": traceback.format_exc()}\n",
    "\n",
    "def _risk_level_to_score(level: str) -> int:\n",
    "    table = {\"critical\": 100, \"high\": 75, \"medium\": 50, \"low\": 25, \"none\": 0}\n",
    "    return table.get(str(level).lower(), 0)\n",
    "\n",
    "def _now_ms() -> int:\n",
    "    return int(time.time() * 1000)\n",
    "\n",
    "# --- Configurable constants ---\n",
    "MAX_RETRIES = 3\n",
    "ENABLE_TRACING = bool(globals().get(\"ENABLE_TRACING\", False))\n",
    "\n",
    "# --- Ensure AgentState exists (fallback minimal dict schema) ---\n",
    "try:\n",
    "    AgentState  # type: ignore\n",
    "except NameError:\n",
    "    from typing import TypedDict, Optional\n",
    "    class AgentState(TypedDict, total=False):\n",
    "        domain: str\n",
    "        ml_probability: float\n",
    "        messages: List[BaseMessage]\n",
    "        tools_output: Dict[str, Any]\n",
    "        should_continue: bool\n",
    "        current_step: str\n",
    "        intermediate_results: Dict[str, Any]\n",
    "        final_result: Any\n",
    "        retry_count: int\n",
    "        error: Optional[str]\n",
    "        trace: List[Dict[str, Any]]\n",
    "\n",
    "# --------------- Node implementations ---------------\n",
    "\n",
    "def initialize_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    t0 = _now_ms()\n",
    "    low, mid, high = _get_cfg_thresholds()\n",
    "    domain = state.get(\"domain\", \"\")\n",
    "    ml_p = float(state.get(\"ml_probability\", 0.0) or 0.0)\n",
    "\n",
    "    updates: Dict[str, Any] = {}\n",
    "    updates.setdefault(\"intermediate_results\", {})\n",
    "    updates.setdefault(\"tools_output\", {})\n",
    "    updates.setdefault(\"retry_count\", state.get(\"retry_count\", 0))\n",
    "\n",
    "    err = None\n",
    "    try:\n",
    "        # 1) ãƒ‰ãƒ¡ã‚¤ãƒ³åŸºæœ¬æ¤œè¨¼\n",
    "        if not isinstance(domain, str) or len(domain) < 3 or \".\" not in domain or \" \" in domain:\n",
    "            err = f\"invalid domain: {domain!r}\"\n",
    "            updates[\"error\"] = err\n",
    "            updates[\"should_continue\"] = False\n",
    "            updates[\"current_step\"] = \"error\"\n",
    "            return updates\n",
    "\n",
    "        # 2) æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ’ãƒ³ãƒˆï¼ˆè¨¼æ˜æ›¸ç­‰ï¼‰\n",
    "        cinfo = None\n",
    "        try:\n",
    "            cinfo = globals().get(\"cert_full_info_map\", {}).get(domain)\n",
    "        except Exception:\n",
    "            cinfo = None\n",
    "\n",
    "        # 3) æ–¹é‡æ±ºå®š\n",
    "        if ml_p < mid:\n",
    "            next_step = \"brand_check\"\n",
    "        elif ml_p >= high:\n",
    "            next_step = \"certificate_check\"\n",
    "        else:\n",
    "            next_step = \"brand_check\"\n",
    "\n",
    "        # 4) ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–\n",
    "        updates[\"current_step\"] = next_step\n",
    "        updates[\"should_continue\"] = True\n",
    "        updates[\"intermediate_results\"][\"init\"] = {\n",
    "            \"domain\": domain,\n",
    "            \"ml_probability\": ml_p,\n",
    "            \"has_certificate_info\": bool(cinfo),\n",
    "            \"policy_thresholds\": {\"low\": low, \"mid\": mid, \"high\": high},\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"initialize_node error: {e}\"\n",
    "        updates[\"error\"] = err\n",
    "        updates[\"should_continue\"] = False\n",
    "        updates[\"current_step\"] = \"error\"\n",
    "    finally:\n",
    "        if ENABLE_TRACING:\n",
    "            updates.setdefault(\"trace\", []).append({\n",
    "                \"node\": \"initialize_node\", \"ms\": _now_ms() - t0, \"input\": {\"domain\": domain, \"ml_probability\": ml_p},\n",
    "                \"output_keys\": list(updates.keys()), \"error\": err\n",
    "            })\n",
    "    return updates\n",
    "\n",
    "def brand_analysis_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    t0 = _now_ms()\n",
    "    domain = state.get(\"domain\")\n",
    "    ml_p = float(state.get(\"ml_probability\", 0.0) or 0.0)\n",
    "\n",
    "    updates: Dict[str, Any] = {}\n",
    "    updates.setdefault(\"intermediate_results\", {})\n",
    "    updates.setdefault(\"tools_output\", {})\n",
    "    updates.setdefault(\"retry_count\", state.get(\"retry_count\", 0))\n",
    "\n",
    "    err = None\n",
    "\n",
    "    # ãƒªãƒˆãƒ©ã‚¤åˆ¶å¾¡\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # 1) ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ\n",
    "            tool_out = _call_tool(\"brand_impersonation_check\", domain=domain, ml_probability=ml_p, cfg=globals().get(\"cfg\"))\n",
    "            updates[\"tools_output\"][\"brand_impersonation_check\"] = tool_out\n",
    "\n",
    "            # 2) æ§‹é€ åŒ–\n",
    "            result = {\n",
    "                \"is_impersonation\": bool(tool_out.get(\"is_impersonation\") or tool_out.get(\"is_suspicious\")),\n",
    "                \"confidence\": float(tool_out.get(\"confidence\", 0.0) or 0.0),\n",
    "                \"risk_level\": tool_out.get(\"risk_level\", \"low\"),\n",
    "                \"detected_brands\": tool_out.get(\"detected_brands\") or tool_out.get(\"brands\") or [],\n",
    "                \"risk_factors\": tool_out.get(\"risk_factors\", {}),\n",
    "                \"reasons\": tool_out.get(\"reasons\", []),\n",
    "                \"success\": tool_out.get(\"success\", True),\n",
    "                \"error\": tool_out.get(\"error\"),\n",
    "            }\n",
    "            updates[\"intermediate_results\"][\"brand\"] = _structured(\"BrandAnalysisResult\", result)\n",
    "\n",
    "            # 3) æ¬¡ã‚¹ãƒ†ãƒƒãƒ—\n",
    "            if result[\"is_impersonation\"] or _risk_level_to_score(result[\"risk_level\"]) >= 75:\n",
    "                updates[\"current_step\"] = \"certificate_check\"\n",
    "            else:\n",
    "                updates[\"current_step\"] = \"domain_check\"\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            err = f\"brand_analysis_node attempt#{attempt+1} error: {e}\"\n",
    "            updates[\"retry_count\"] = int(updates[\"retry_count\"]) + 1\n",
    "            if updates[\"retry_count\"] >= MAX_RETRIES:\n",
    "                updates[\"intermediate_results\"][\"brand\"] = _structured(\"BrandAnalysisResult\", {\n",
    "                    \"is_impersonation\": False, \"confidence\": 0.0, \"risk_level\": \"low\", \"detected_brands\": [],\n",
    "                    \"risk_factors\": {\"error\": str(e)}, \"reasons\": [\"brand tool failed\"], \"success\": False, \"error\": str(e)\n",
    "                })\n",
    "                updates[\"current_step\"] = \"certificate_check\"\n",
    "                break\n",
    "            time.sleep(0.1)  # small backoff\n",
    "    if ENABLE_TRACING:\n",
    "        updates.setdefault(\"trace\", []).append({\n",
    "            \"node\": \"brand_analysis_node\", \"ms\": _now_ms()-t0, \"error\": err,\n",
    "            \"output_keys\": list(updates.keys())\n",
    "        })\n",
    "    return updates\n",
    "\n",
    "def certificate_analysis_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    t0 = _now_ms()\n",
    "    domain = state.get(\"domain\")\n",
    "    ml_p = float(state.get(\"ml_probability\", 0.0) or 0.0)\n",
    "\n",
    "    updates: Dict[str, Any] = {}\n",
    "    updates.setdefault(\"intermediate_results\", {})\n",
    "    updates.setdefault(\"tools_output\", {})\n",
    "    updates.setdefault(\"retry_count\", state.get(\"retry_count\", 0))\n",
    "    err = None\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            # å¸¸ã«å®Ÿè¡Œ\n",
    "            tool_out = _call_tool(\"certificate_analysis\", domain=domain, ml_probability=ml_p, cfg=globals().get(\"cfg\"))\n",
    "            updates[\"tools_output\"][\"certificate_analysis\"] = tool_out\n",
    "\n",
    "            # æ§‹é€ åŒ–ã¨è¿½åŠ è©•ä¾¡\n",
    "            risk_level = tool_out.get(\"risk_level\", \"low\")\n",
    "            derived_score = _risk_level_to_score(risk_level)\n",
    "\n",
    "            # ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ã‚¯ã‚¹ï¼ˆç„¡æ–™CAã€æ–°è¦è¨¼æ˜æ›¸ã€ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ï¼‰\n",
    "            free_ca = bool(tool_out.get(\"risk_factors\", {}).get(\"free_ca\") or tool_out.get(\"is_free_ca\", False))\n",
    "            very_new = bool(tool_out.get(\"risk_factors\", {}).get(\"very_new_cert\") or tool_out.get(\"very_new\", False))\n",
    "            wildcard = bool(tool_out.get(\"risk_factors\", {}).get(\"wildcard\") or tool_out.get(\"is_wildcard\", False))\n",
    "            derived_score += 10*free_ca + 10*very_new + 5*wildcard\n",
    "\n",
    "            result = {\n",
    "                \"risk_level\": risk_level,\n",
    "                \"score\": derived_score,\n",
    "                \"free_ca\": free_ca,\n",
    "                \"very_new\": very_new,\n",
    "                \"wildcard\": wildcard,\n",
    "                \"reasons\": tool_out.get(\"reasons\", []),\n",
    "                \"risk_factors\": tool_out.get(\"risk_factors\", {}),\n",
    "                \"success\": tool_out.get(\"success\", True),\n",
    "                \"error\": tool_out.get(\"error\"),\n",
    "            }\n",
    "            updates[\"intermediate_results\"][\"certificate\"] = _structured(\"CertificateAnalysisResult\", result)\n",
    "\n",
    "            # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "            updates[\"current_step\"] = \"domain_check\"\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            err = f\"certificate_analysis_node attempt#{attempt+1} error: {e}\"\n",
    "            updates[\"retry_count\"] = int(updates[\"retry_count\"]) + 1\n",
    "            if updates[\"retry_count\"] >= MAX_RETRIES:\n",
    "                updates[\"intermediate_results\"][\"certificate\"] = _structured(\"CertificateAnalysisResult\", {\n",
    "                    \"risk_level\": \"low\", \"score\": 0, \"reasons\": [\"certificate tool failed\"],\n",
    "                    \"risk_factors\": {\"error\": str(e)}, \"success\": False, \"error\": str(e)\n",
    "                })\n",
    "                updates[\"current_step\"] = \"domain_check\"\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "    if ENABLE_TRACING:\n",
    "        updates.setdefault(\"trace\", []).append({\n",
    "            \"node\": \"certificate_analysis_node\", \"ms\": _now_ms()-t0, \"error\": err,\n",
    "            \"output_keys\": list(updates.keys())\n",
    "        })\n",
    "    return updates\n",
    "\n",
    "def domain_analysis_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    t0 = _now_ms()\n",
    "    domain = state.get(\"domain\")\n",
    "    ml_p = float(state.get(\"ml_probability\", 0.0) or 0.0)\n",
    "\n",
    "    updates: Dict[str, Any] = {}\n",
    "    updates.setdefault(\"intermediate_results\", {})\n",
    "    updates.setdefault(\"tools_output\", {})\n",
    "    updates.setdefault(\"retry_count\", state.get(\"retry_count\", 0))\n",
    "    err = None\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            tool_out = _call_tool(\"short_domain_analysis\", domain=domain, ml_probability=ml_p, cfg=globals().get(\"cfg\"),\n",
    "                                  DANGEROUS_TLDS=globals().get(\"DANGEROUS_TLDS\"))\n",
    "            updates[\"tools_output\"][\"short_domain_analysis\"] = tool_out\n",
    "\n",
    "            # å±é™ºTLD/ã‚¿ã‚¤ãƒã‚¹ã‚¯ãƒ¯ãƒƒãƒˆã®è©•ä¾¡\n",
    "            tld_risky = bool(tool_out.get(\"risk_factors\", {}).get(\"dangerous_tld\"))\n",
    "            typosquat = bool(tool_out.get(\"risk_factors\", {}).get(\"typosquatting\"))\n",
    "            entropy = float(tool_out.get(\"risk_factors\", {}).get(\"entropy\", 0.0) or tool_out.get(\"entropy\", 0.0))\n",
    "            length = int(tool_out.get(\"risk_factors\", {}).get(\"length\", 0) or tool_out.get(\"length\", 0))\n",
    "\n",
    "            # ç·åˆã‚¹ã‚³ã‚¢\n",
    "            score = _risk_level_to_score(tool_out.get(\"risk_level\", \"low\"))\n",
    "            score += 10 if tld_risky else 0\n",
    "            score += 10 if typosquat else 0\n",
    "            score += 5 if length <= 10 else 0\n",
    "            score += 5 if entropy >= 4.0 else 0\n",
    "\n",
    "            result = {\n",
    "                \"risk_level\": tool_out.get(\"risk_level\", \"low\"),\n",
    "                \"score\": score,\n",
    "                \"tld_risky\": tld_risky,\n",
    "                \"typosquatting\": typosquat,\n",
    "                \"length\": length,\n",
    "                \"entropy\": entropy,\n",
    "                \"reasons\": tool_out.get(\"reasons\", []),\n",
    "                \"risk_factors\": tool_out.get(\"risk_factors\", {}),\n",
    "                \"success\": tool_out.get(\"success\", True),\n",
    "                \"error\": tool_out.get(\"error\"),\n",
    "            }\n",
    "            updates[\"intermediate_results\"][\"domain\"] = _structured(\"DomainAnalysisResult\", result)\n",
    "            updates[\"current_step\"] = \"decision\"\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            err = f\"domain_analysis_node attempt#{attempt+1} error: {e}\"\n",
    "            updates[\"retry_count\"] = int(updates[\"retry_count\"]) + 1\n",
    "            if updates[\"retry_count\"] >= MAX_RETRIES:\n",
    "                updates[\"intermediate_results\"][\"domain\"] = _structured(\"DomainAnalysisResult\", {\n",
    "                    \"risk_level\": \"low\", \"score\": 0, \"reasons\": [\"domain tool failed\"],\n",
    "                    \"risk_factors\": {\"error\": str(e)}, \"success\": False, \"error\": str(e)\n",
    "                })\n",
    "                updates[\"current_step\"] = \"decision\"\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "    if ENABLE_TRACING:\n",
    "        updates.setdefault(\"trace\", []).append({\n",
    "            \"node\": \"domain_analysis_node\", \"ms\": _now_ms()-t0, \"error\": err,\n",
    "            \"output_keys\": list(updates.keys())\n",
    "        })\n",
    "    return updates\n",
    "\n",
    "def decision_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    t0 = _now_ms()\n",
    "    low, mid, high = _get_cfg_thresholds()\n",
    "    domain = state.get(\"domain\")\n",
    "    ml_p = float(state.get(\"ml_probability\", 0.0) or 0.0)\n",
    "\n",
    "    updates: Dict[str, Any] = {}\n",
    "    updates.setdefault(\"intermediate_results\", {})\n",
    "    updates.setdefault(\"retry_count\", state.get(\"retry_count\", 0))\n",
    "\n",
    "    err = None\n",
    "    try:\n",
    "        brand = _to_dict(state.get(\"intermediate_results\", {}).get(\"brand\"))\n",
    "        cert  = _to_dict(state.get(\"intermediate_results\", {}).get(\"certificate\"))\n",
    "        dres  = _to_dict(state.get(\"intermediate_results\", {}).get(\"domain\"))\n",
    "\n",
    "        brand_detected = bool(brand.get(\"is_impersonation\"))\n",
    "        brand_high = _risk_level_to_score(brand.get(\"risk_level\", \"low\")) >= 75\n",
    "        cert_score = int(cert.get(\"score\", 0))\n",
    "        domain_score = int(dres.get(\"score\", 0))\n",
    "\n",
    "        # 1) ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼ˆä»•æ§˜æº–æ‹ ï¼‰\n",
    "        is_phishing = False\n",
    "        reasons: List[str] = []\n",
    "\n",
    "        if ml_p < 0.2 and (brand_detected or brand_high):\n",
    "            is_phishing = True\n",
    "            reasons.append(\"ML<0.2 & ãƒ–ãƒ©ãƒ³ãƒ‰å½è£…/é«˜ãƒªã‚¹ã‚¯\")\n",
    "        if (cert_score >= 75 and domain_score >= 75):\n",
    "            is_phishing = True\n",
    "            reasons.append(\"è¨¼æ˜æ›¸ãƒªã‚¹ã‚¯é«˜ & ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ã‚¯é«˜\")\n",
    "\n",
    "        # 2) é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢\n",
    "        total_score = (1.0 - ml_p) * 100.0 + cert_score * 0.8 + domain_score * 0.8 + (_risk_level_to_score(brand.get(\"risk_level\",\"low\")) * 0.9)\n",
    "        threshold = 120.0\n",
    "        if total_score > threshold:\n",
    "            is_phishing = True\n",
    "            reasons.append(f\"ç·åˆã‚¹ã‚³ã‚¢ {total_score:.1f} > {threshold}\")\n",
    "\n",
    "        detected_brands = brand.get(\"detected_brands\", []) or []\n",
    "        risk_level = \"high\" if is_phishing else (\"medium\" if total_score > threshold*0.8 else \"low\")\n",
    "        confidence = min(1.0, max(0.0, (total_score/200.0)))\n",
    "\n",
    "        result = {\n",
    "            \"domain\": domain,\n",
    "            \"ml_probability\": ml_p,\n",
    "            \"is_phishing\": bool(is_phishing),\n",
    "            \"confidence\": float(confidence),\n",
    "            \"risk_level\": risk_level,\n",
    "            \"detected_brands\": detected_brands,\n",
    "            \"risk_factors\": {\n",
    "                \"brand\": brand, \"certificate\": cert, \"domain\": dres,\n",
    "                \"total_score\": total_score\n",
    "            },\n",
    "            \"reasons\": reasons,\n",
    "            \"success\": True\n",
    "        }\n",
    "        updates[\"final_result\"] = _structured(\"PhishingDetectionResult\", result)\n",
    "        updates[\"current_step\"] = \"complete\"\n",
    "        updates[\"should_continue\"] = False\n",
    "\n",
    "    except Exception as e:\n",
    "        err = f\"decision_node error: {e}\"\n",
    "        updates[\"final_result\"] = _structured(\"PhishingDetectionResult\", {\n",
    "            \"domain\": domain, \"ml_probability\": ml_p, \"is_phishing\": False, \"confidence\": 0.0,\n",
    "            \"risk_level\": \"low\", \"detected_brands\": [], \"risk_factors\": {\"error\": str(e)}, \"reasons\": [\"decision error\"], \"success\": False\n",
    "        })\n",
    "        updates[\"current_step\"] = \"complete\"\n",
    "        updates[\"should_continue\"] = False\n",
    "        updates[\"error\"] = err\n",
    "    finally:\n",
    "        if ENABLE_TRACING:\n",
    "            updates.setdefault(\"trace\", []).append({\n",
    "                \"node\": \"decision_node\", \"ms\": _now_ms()-t0, \"error\": err,\n",
    "                \"output_keys\": list(updates.keys())\n",
    "            })\n",
    "    return updates\n",
    "\n",
    "# --------------- Conditional routers ---------------\n",
    "\n",
    "def route_after_init(state: Dict[str, Any]) -> str:\n",
    "    \"\"\"Return label used to choose the next node.\"\"\"\n",
    "    _, mid, high = _get_cfg_thresholds()\n",
    "    ml_p = float(state.get(\"ml_probability\", 0.0) or 0.0)\n",
    "    if ml_p < mid:\n",
    "        return \"to_brand\"\n",
    "    elif ml_p >= high:\n",
    "        return \"to_cert\"\n",
    "    else:\n",
    "        return \"to_brand\"\n",
    "\n",
    "def route_after_brand(state: Dict[str, Any]) -> str:\n",
    "    brand = _to_dict(state.get(\"intermediate_results\", {}).get(\"brand\"))\n",
    "    detected = bool(brand.get(\"is_impersonation\"))\n",
    "    risk_high = _risk_level_to_score(brand.get(\"risk_level\", \"low\")) >= 75\n",
    "    if detected or risk_high:\n",
    "        return \"to_cert\"\n",
    "    # ä½ãƒªã‚¹ã‚¯â†’ãƒ‰ãƒ¡ã‚¤ãƒ³ã¸\n",
    "    if _risk_level_to_score(brand.get(\"risk_level\", \"low\")) <= 25:\n",
    "        return \"to_domain\"\n",
    "    return \"to_cert\"\n",
    "\n",
    "# --------------- Graph construction ---------------\n",
    "\n",
    "# Build the graph with the explicit state type (AgentState if available)\n",
    "_state_type = globals().get(\"AgentState\", dict)\n",
    "phishing_detection_graph = StateGraph(_state_type)\n",
    "\n",
    "# Nodes\n",
    "phishing_detection_graph.add_node(\"initialize_node\", initialize_node)\n",
    "phishing_detection_graph.add_node(\"brand_analysis_node\", brand_analysis_node)\n",
    "phishing_detection_graph.add_node(\"certificate_analysis_node\", certificate_analysis_node)\n",
    "phishing_detection_graph.add_node(\"domain_analysis_node\", domain_analysis_node)\n",
    "phishing_detection_graph.add_node(\"decision_node\", decision_node)\n",
    "\n",
    "# Entry\n",
    "phishing_detection_graph.set_entry_point(\"initialize_node\")\n",
    "\n",
    "# Conditional edges\n",
    "phishing_detection_graph.add_conditional_edges(\n",
    "    \"initialize_node\",\n",
    "    route_after_init,\n",
    "    {\n",
    "        \"to_brand\": \"brand_analysis_node\",\n",
    "        \"to_cert\": \"certificate_analysis_node\",\n",
    "    },\n",
    ")\n",
    "phishing_detection_graph.add_conditional_edges(\n",
    "    \"brand_analysis_node\",\n",
    "    route_after_brand,\n",
    "    {\n",
    "        \"to_cert\": \"certificate_analysis_node\",\n",
    "        \"to_domain\": \"domain_analysis_node\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Linear edges\n",
    "phishing_detection_graph.add_edge(\"certificate_analysis_node\", \"domain_analysis_node\")\n",
    "phishing_detection_graph.add_edge(\"domain_analysis_node\", \"decision_node\")\n",
    "phishing_detection_graph.add_edge(\"decision_node\", END)\n",
    "\n",
    "# Compile\n",
    "PHISHING_DETECTION_GRAPH = phishing_detection_graph.compile()\n",
    "\n",
    "# --------------- Compatibility wrapper ---------------\n",
    "\n",
    "class PhishingDetectionAgent:\n",
    "    \"\"\"Compatibility layer that preserves the existing interface while using the compiled LangGraph.\"\"\"\n",
    "\n",
    "    def __init__(self, graph=None):\n",
    "        self.graph = graph or PHISHING_DETECTION_GRAPH\n",
    "\n",
    "    def evaluate(self, domain: str, ml_probability: float, **kwargs) -> Any:\n",
    "        \"\"\"Run the graph for a single domain and return PhishingDetectionResult (structured or dict).\"\"\"\n",
    "        initial_state: Dict[str, Any] = {\n",
    "            \"domain\": domain,\n",
    "            \"ml_probability\": float(ml_probability),\n",
    "            \"messages\": [],\n",
    "            \"tools_output\": {},\n",
    "            \"should_continue\": True,\n",
    "            \"current_step\": \"initialize\",\n",
    "            \"intermediate_results\": {},\n",
    "            \"retry_count\": 0,\n",
    "        }\n",
    "        # Allow caller to pass additional initial fields (e.g., prior tool outputs)\n",
    "        initial_state.update({k: v for k, v in kwargs.items() if k not in initial_state})\n",
    "\n",
    "        # Execute\n",
    "        out_state = self.graph.invoke(initial_state)\n",
    "        # Return final_result if present; else a conservative dict\n",
    "        fr = out_state.get(\"final_result\")\n",
    "        if fr is None:\n",
    "            return _structured(\"PhishingDetectionResult\", {\n",
    "                \"domain\": domain, \"ml_probability\": float(ml_probability),\n",
    "                \"is_phishing\": False, \"confidence\": 0.0, \"risk_level\": \"low\",\n",
    "                \"detected_brands\": [], \"risk_factors\": {\"error\": \"no final_result\"},\n",
    "                \"reasons\": [\"graph terminated without decision\"], \"success\": False\n",
    "            })\n",
    "        return fr\n",
    "\n",
    "    def batch_evaluate(self, domains: List[str], ml_probabilities: List[float]) -> List[Any]:\n",
    "        results: List[Any] = []\n",
    "        for d, p in zip(domains, ml_probabilities):\n",
    "            try:\n",
    "                results.append(self.evaluate(d, p))\n",
    "            except Exception as e:\n",
    "                results.append(_structured(\"PhishingDetectionResult\", {\n",
    "                    \"domain\": d, \"ml_probability\": float(p), \"is_phishing\": False, \"confidence\": 0.0,\n",
    "                    \"risk_level\": \"low\", \"detected_brands\": [], \"risk_factors\": {\"error\": str(e)},\n",
    "                    \"reasons\": [\"batch evaluate error\"], \"success\": False\n",
    "                }))\n",
    "        return results\n",
    "\n",
    "# Backward-compatible alias function\n",
    "def evaluate_domain(domain: str, ml_probability: float, **kwargs) -> Any:\n",
    "    agent = PhishingDetectionAgent()\n",
    "    return agent.evaluate(domain, ml_probability, **kwargs)\n",
    "\n",
    "print(\"âœ… LangGraph-based PhishingDetectionAgent is defined and PHISHING_DETECTION_GRAPH is compiled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b54359a1-7b70-411d-a398-e91cc125be87",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”§ process_domains_batché–¢æ•°ã®å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ï¼‰\n",
      "======================================================================\n",
      "\n",
      "ğŸ” å¿…è¦ãªé–¢æ•°ã®ç¢ºèª:\n",
      "  âœ… evaluate_domain: å®šç¾©æ¸ˆã¿\n",
      "  âš ï¸ evaluate_domain_fixed: æœªå®šç¾© - ã‚»ãƒ«06ã§å®šç¾©ã•ã‚Œã‚‹ä¿®æ­£ç‰ˆè©•ä¾¡é–¢æ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
      "  âš ï¸ AI_AGENT: æœªå®šç¾© - ã‚»ãƒ«06ã§å®šç¾©ã•ã‚Œã‚‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\n",
      "\n",
      "âœ… process_domains_batché–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\n",
      "\n",
      "======================================================================\n",
      "âœ… ã‚»ãƒ«06-Då®Ÿè¡Œå®Œäº†\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ ä¿®æ­£å†…å®¹:\n",
      "  1. process_domains_batché–¢æ•°ã‚’å®šç¾©\n",
      "  2. run_async_testé–¢æ•°ã‚’ä¿®æ­£ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ ï¼‰\n",
      "  3. quick_test_process_domainsé–¢æ•°ã‚’è¿½åŠ ï¼ˆå‹•ä½œç¢ºèªç”¨ï¼‰\n",
      "\n",
      "ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
      "  1. ã‚»ãƒ«06ãŒå®Ÿè¡Œæ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
      "  2. run_async_test()ã‚’å®Ÿè¡Œã—ã¦ãƒ†ã‚¹ãƒˆ\n",
      "  3. ã‚¨ãƒ©ãƒ¼ãŒè§£æ±ºã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
      "\n",
      "âš ï¸ æ³¨æ„:\n",
      "  - ã‚»ãƒ«06ï¼ˆAI Agentä½œæˆï¼‰ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„å ´åˆã€\n",
      "    evaluate_domainé–¢æ•°ãŒæœªå®šç¾©ã®ãŸã‚å‹•ä½œã—ã¾ã›ã‚“\n",
      "  - ãã®å ´åˆã¯ã€ã‚»ãƒ«06ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\n"
     ]
    }
   ],
   "source": [
    "# In[18]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 06-Dï¼ˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ï¼‰\n",
    "æ¦‚è¦: æ¬ è½ã—ã¦ã„ãŸprocess_domains_batché–¢æ•°ã‚’å®šç¾©ã—ã€ãƒ†ã‚¹ãƒˆé–¢æ•°ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±º\n",
    "å…¥åŠ›: evaluate_domainé–¢æ•°ï¼ˆã‚»ãƒ«06ã§å®šç¾©æ¸ˆã¿ï¼‰\n",
    "å‡ºåŠ›: process_domains_batché–¢æ•°\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”§ process_domains_batché–¢æ•°ã®å®šç¾©ï¼ˆã‚¨ãƒ©ãƒ¼ä¿®æ­£ï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1. åŸºæœ¬ã®process_domains_batché–¢æ•°ã‚’å®šç¾© ==========\n",
    "\n",
    "async def process_domains_batch(domains_data: List[Tuple[str, float]], \n",
    "                               max_concurrent: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒãƒƒãƒå‡¦ç†ï¼ˆåŸºæœ¬ç‰ˆï¼‰\n",
    "    \n",
    "    Args:\n",
    "        domains_data: (domain, ml_probability)ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆ\n",
    "        max_concurrent: æœ€å¤§ä¸¦åˆ—æ•°\n",
    "    \n",
    "    Returns:\n",
    "        è©•ä¾¡çµæœã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    async def evaluate_async(domain: str, ml_prob: float, semaphore: asyncio.Semaphore) -> Dict:\n",
    "        \"\"\"éåŒæœŸã§ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è©•ä¾¡\"\"\"\n",
    "        async with semaphore:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "                # evaluate_domainé–¢æ•°ã‚’éåŒæœŸã§å®Ÿè¡Œ\n",
    "                result = await loop.run_in_executor(\n",
    "                    executor,\n",
    "                    evaluate_domain,  # ã‚»ãƒ«06ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹é–¢æ•°\n",
    "                    domain,\n",
    "                    ml_prob\n",
    "                )\n",
    "            return result\n",
    "    \n",
    "    # ä¸¦åˆ—æ•°åˆ¶é™ã®ãŸã‚ã®ã‚»ãƒãƒ•ã‚©\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "    tasks = []\n",
    "    \n",
    "    print(f\"\\nâš¡ ãƒãƒƒãƒå‡¦ç†é–‹å§‹:\")\n",
    "    print(f\"  - å‡¦ç†æ•°: {len(domains_data)}ä»¶\")\n",
    "    print(f\"  - ä¸¦åˆ—æ•°: {max_concurrent}\")\n",
    "    \n",
    "    # éåŒæœŸã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "    for domain, ml_prob in domains_data:\n",
    "        task = evaluate_async(domain, ml_prob, semaphore)\n",
    "        tasks.append(task)\n",
    "    \n",
    "    # å…¨ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ã‚‚å«ã‚ã¦çµæœã‚’å–å¾—ï¼‰\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n",
    "    processed_results = []\n",
    "    error_count = 0\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            domain, ml_prob = domains_data[i]\n",
    "            error_count += 1\n",
    "            \n",
    "            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã§ã‚‚çµæœã‚’è¨˜éŒ²\n",
    "            processed_results.append({\n",
    "                'domain': domain,\n",
    "                'ml_probability': ml_prob,\n",
    "                'ai_is_phishing': True,  # ä¿å®ˆçš„ã«ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã¨åˆ¤å®š\n",
    "                'ai_confidence': 0.5,\n",
    "                'ai_risk_level': 'medium',\n",
    "                'error': str(result),\n",
    "                'success': False\n",
    "            })\n",
    "            \n",
    "            print(f\"  âŒ ã‚¨ãƒ©ãƒ¼ [{domain}]: {str(result)[:100]}\")\n",
    "        else:\n",
    "            processed_results.append(result)\n",
    "    \n",
    "    # å‡¦ç†çµæœã®ã‚µãƒãƒªãƒ¼\n",
    "    success_count = len(processed_results) - error_count\n",
    "    print(f\"\\nğŸ“Š ãƒãƒƒãƒå‡¦ç†å®Œäº†:\")\n",
    "    print(f\"  - æˆåŠŸ: {success_count}/{len(domains_data)}\")\n",
    "    print(f\"  - ã‚¨ãƒ©ãƒ¼: {error_count}/{len(domains_data)}\")\n",
    "    \n",
    "    return processed_results\n",
    "\n",
    "# ========== 2. å¿…è¦ãªé–¢æ•°ã®å­˜åœ¨ç¢ºèª ==========\n",
    "\n",
    "print(\"\\nğŸ” å¿…è¦ãªé–¢æ•°ã®ç¢ºèª:\")\n",
    "\n",
    "required_functions = {\n",
    "    'evaluate_domain': 'ã‚»ãƒ«06ã§å®šç¾©ã•ã‚Œã‚‹åŸºæœ¬è©•ä¾¡é–¢æ•°',\n",
    "    'evaluate_domain_fixed': 'ã‚»ãƒ«06ã§å®šç¾©ã•ã‚Œã‚‹ä¿®æ­£ç‰ˆè©•ä¾¡é–¢æ•°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰',\n",
    "    'AI_AGENT': 'ã‚»ãƒ«06ã§å®šç¾©ã•ã‚Œã‚‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ'\n",
    "}\n",
    "\n",
    "for func_name, description in required_functions.items():\n",
    "    if func_name in globals():\n",
    "        print(f\"  âœ… {func_name}: å®šç¾©æ¸ˆã¿\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ {func_name}: æœªå®šç¾© - {description}\")\n",
    "\n",
    "# ========== 3. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ ==========\n",
    "\n",
    "globals()['process_domains_batch'] = process_domains_batch\n",
    "\n",
    "print(\"\\nâœ… process_domains_batché–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# ========== 4. å‹•ä½œç¢ºèªç”¨ã®ç°¡æ˜“ãƒ†ã‚¹ãƒˆ ==========\n",
    "\n",
    "def quick_test_process_domains():\n",
    "    \"\"\"process_domains_batché–¢æ•°ã®ç°¡æ˜“ãƒ†ã‚¹ãƒˆ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ§ª process_domains_batchç°¡æ˜“ãƒ†ã‚¹ãƒˆ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # evaluate_domainé–¢æ•°ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª\n",
    "    if 'evaluate_domain' not in globals():\n",
    "        print(\"âŒ evaluate_domainé–¢æ•°ãŒæœªå®šç¾©ã§ã™\")\n",
    "        print(\"  â†’ ã‚»ãƒ«06ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        return None\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆç”¨ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆå°‘æ•°ï¼‰\n",
    "    test_domains = [\n",
    "        (\"test-phishing.com\", 0.15),\n",
    "        (\"normal-site.org\", 0.85)\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³:\")\n",
    "    for domain, prob in test_domains:\n",
    "        print(f\"  - {domain} (MLç¢ºç‡: {prob:.2f})\")\n",
    "    \n",
    "    # éåŒæœŸå®Ÿè¡Œã®ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\n",
    "    async def run_test():\n",
    "        results = await process_domains_batch(test_domains, max_concurrent=10)\n",
    "        return results\n",
    "    \n",
    "    # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã§å®Ÿè¡Œ\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        loop = asyncio.get_event_loop()\n",
    "        results = loop.run_until_complete(run_test())\n",
    "        \n",
    "        print(f\"\\nâœ… ãƒ†ã‚¹ãƒˆæˆåŠŸï¼\")\n",
    "        print(f\"  å‡¦ç†çµæœ: {len(results)}ä»¶\")\n",
    "        \n",
    "        for result in results:\n",
    "            if result.get('success', True):\n",
    "                status = \"âœ…\"\n",
    "            else:\n",
    "                status = \"âŒ\"\n",
    "            print(f\"  {status} {result['domain']}: \"\n",
    "                  f\"ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°={'Yes' if result.get('ai_is_phishing') else 'No'}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "# ========== 5. ä¿®æ­£ç‰ˆrun_async_testï¼ˆã‚¨ãƒ©ãƒ¼å¯¾å‡¦æ¸ˆã¿ï¼‰ ==========\n",
    "\n",
    "def run_async_test():\n",
    "    \"\"\"éåŒæœŸãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆä¿®æ­£ç‰ˆï¼‰\"\"\"\n",
    "    \n",
    "    # evaluate_domainé–¢æ•°ã®å­˜åœ¨ç¢ºèª\n",
    "    if 'evaluate_domain' not in globals():\n",
    "        print(\"âŒ ã‚¨ãƒ©ãƒ¼: evaluate_domainé–¢æ•°ãŒæœªå®šç¾©ã§ã™\")\n",
    "        print(\"  è§£æ±ºæ–¹æ³•: ã‚»ãƒ«06ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        return None\n",
    "    \n",
    "    # test_async_functionsé–¢æ•°ã®å­˜åœ¨ç¢ºèª\n",
    "    if 'test_async_functions' not in globals():\n",
    "        print(\"âŒ ã‚¨ãƒ©ãƒ¼: test_async_functionsé–¢æ•°ãŒæœªå®šç¾©ã§ã™\")\n",
    "        print(\"  è§£æ±ºæ–¹æ³•: ã‚»ãƒ«06-Bã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        return None\n",
    "    \n",
    "    # nest_asyncioã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨é©ç”¨\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ nest_asyncioãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        print(\"  å®Ÿè¡Œ: pip install nest_asyncio\")\n",
    "        return None\n",
    "    \n",
    "    # æ–°ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    \n",
    "    try:\n",
    "        # test_async_functionsé–¢æ•°ã‚’å–å¾—\n",
    "        test_func = globals()['test_async_functions']\n",
    "        \n",
    "        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "        results = loop.run_until_complete(test_func())\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        loop.close()\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "globals()['run_async_test'] = run_async_test\n",
    "globals()['quick_test_process_domains'] = quick_test_process_domains\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… ã‚»ãƒ«06-Då®Ÿè¡Œå®Œäº†\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“‹ ä¿®æ­£å†…å®¹:\")\n",
    "print(\"  1. process_domains_batché–¢æ•°ã‚’å®šç¾©\")\n",
    "print(\"  2. run_async_testé–¢æ•°ã‚’ä¿®æ­£ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ ï¼‰\")\n",
    "print(\"  3. quick_test_process_domainsé–¢æ•°ã‚’è¿½åŠ ï¼ˆå‹•ä½œç¢ºèªç”¨ï¼‰\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"  1. ã‚»ãƒ«06ãŒå®Ÿè¡Œæ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\")\n",
    "print(\"  2. run_async_test()ã‚’å®Ÿè¡Œã—ã¦ãƒ†ã‚¹ãƒˆ\")\n",
    "print(\"  3. ã‚¨ãƒ©ãƒ¼ãŒè§£æ±ºã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\")\n",
    "\n",
    "print(\"\\nâš ï¸ æ³¨æ„:\")\n",
    "print(\"  - ã‚»ãƒ«06ï¼ˆAI Agentä½œæˆï¼‰ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„å ´åˆã€\")\n",
    "print(\"    evaluate_domainé–¢æ•°ãŒæœªå®šç¾©ã®ãŸã‚å‹•ä½œã—ã¾ã›ã‚“\")\n",
    "print(\"  - ãã®å ´åˆã¯ã€ã‚»ãƒ«06ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12e4f756-fae6-462a-9e9e-261f714454c5",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ¯ é–¢æ•°çµ±ä¸€è¨­å®šï¼ˆå®‰å…¨ç‰ˆï¼‰\n",
      "======================================================================\n",
      "  âœ… evaluate_domain â†’ æ—¢å­˜ç‰ˆã‚’ä½¿ç”¨\n",
      "  âœ… process_domains_batch â†’ æ—¢å­˜ç‰ˆã‚’ä½¿ç”¨\n",
      "  âš ï¸ AI_AGENT ãŒæœªå®šç¾©ï¼ˆevaluate_domain ã«ä¾å­˜å‹•ä½œã™ã‚‹ãŸã‚ç¶šè¡Œå¯èƒ½ï¼‰\n",
      "\n",
      "ğŸ“‹ å®šç¾©çŠ¶æ³ãƒã‚§ãƒƒã‚¯:\n",
      "  â€¢ evaluate_domain: å®šç¾©ã‚ã‚Š\n",
      "  â€¢ evaluate_domain_fixed: ï¼ˆæœªå®šç¾©ï¼‰\n",
      "  â€¢ process_domains_batch: å®šç¾©ã‚ã‚Š\n",
      "  â€¢ process_domains_batch_fixed: ï¼ˆæœªå®šç¾©ï¼‰\n",
      "  â€¢ AI_AGENT: ï¼ˆæœªå®šç¾©ï¼‰\n",
      "  â€¢ FIXED_AI_AGENT: ï¼ˆæœªå®šç¾©ï¼‰\n",
      "\n",
      "âœ… å®‰å…¨ãªé–¢æ•°çµ±ä¸€ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Cell 06-D-ALIAS: Safe unification (fixed ãŒç„¡ã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯) ===\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ¯ é–¢æ•°çµ±ä¸€è¨­å®šï¼ˆå®‰å…¨ç‰ˆï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# evaluate_domain: *_fixed ãŒã‚ã‚Œã°æ¡ç”¨ã€ç„¡ã‘ã‚Œã°æ—¢å­˜ã‚’ä½¿ã†\n",
    "if 'evaluate_domain_fixed' in globals():\n",
    "    evaluate_domain = evaluate_domain_fixed\n",
    "    print(\"  âœ… evaluate_domain â†’ evaluate_domain_fixed ã‚’ä½¿ç”¨\")\n",
    "elif 'evaluate_domain' in globals():\n",
    "    print(\"  âœ… evaluate_domain â†’ æ—¢å­˜ç‰ˆã‚’ä½¿ç”¨\")\n",
    "else:\n",
    "    raise RuntimeError(\"evaluate_domain / evaluate_domain_fixed ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å®šç¾©ã‚»ãƒ«ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# process_domains_batch: *_fixed ãŒã‚ã‚Œã°æ¡ç”¨ã€ç„¡ã‘ã‚Œã°æ—¢å­˜ã‚’ä½¿ã†\n",
    "if 'process_domains_batch_fixed' in globals():\n",
    "    process_domains_batch = process_domains_batch_fixed\n",
    "    print(\"  âœ… process_domains_batch â†’ process_domains_batch_fixed ã‚’ä½¿ç”¨\")\n",
    "elif 'process_domains_batch' in globals():\n",
    "    print(\"  âœ… process_domains_batch â†’ æ—¢å­˜ç‰ˆã‚’ä½¿ç”¨\")\n",
    "else:\n",
    "    # ã©ã¡ã‚‰ã‚‚ç„¡ã‘ã‚Œã° evaluate_domain ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§1æœ¬ãšã¤å‡¦ç†ã™ã‚‹ç°¡æ˜“ç‰ˆã‚’å®šç¾©\n",
    "    print(\"  âš ï¸ process_domains_batch ãŒæœªå®šç¾©ã®ãŸã‚ã€ç°¡æ˜“ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®šç¾©\")\n",
    "    def process_domains_batch(domains, probs=None, cfg=None, max_concurrent=10):\n",
    "        probs = probs or [0.0] * len(domains)\n",
    "        return [evaluate_domain(d, float(p), cfg=cfg or {}) for d, p in zip(domains, probs)]\n",
    "\n",
    "# AI_AGENT ã®äº’æ›ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã‚’æ¡ç”¨ï¼‰\n",
    "if 'FIXED_AI_AGENT' in globals():\n",
    "    AI_AGENT = FIXED_AI_AGENT\n",
    "    print(\"  âœ… AI_AGENT â†’ FIXED_AI_AGENT ã‚’ä½¿ç”¨\")\n",
    "elif 'AI_AGENT' in globals():\n",
    "    print(\"  âœ… AI_AGENT â†’ æ—¢å­˜ç‰ˆã‚’ä½¿ç”¨\")\n",
    "else:\n",
    "    print(\"  âš ï¸ AI_AGENT ãŒæœªå®šç¾©ï¼ˆevaluate_domain ã«ä¾å­˜å‹•ä½œã™ã‚‹ãŸã‚ç¶šè¡Œå¯èƒ½ï¼‰\")\n",
    "\n",
    "print(\"\\nğŸ“‹ å®šç¾©çŠ¶æ³ãƒã‚§ãƒƒã‚¯:\")\n",
    "for func in ['evaluate_domain', 'evaluate_domain_fixed',\n",
    "             'process_domains_batch', 'process_domains_batch_fixed',\n",
    "             'AI_AGENT', 'FIXED_AI_AGENT']:\n",
    "    if func in globals():\n",
    "        print(f\"  â€¢ {func}: å®šç¾©ã‚ã‚Š\")\n",
    "    else:\n",
    "        print(f\"  â€¢ {func}: ï¼ˆæœªå®šç¾©ï¼‰\")\n",
    "\n",
    "print(\"\\nâœ… å®‰å…¨ãªé–¢æ•°çµ±ä¸€ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3640165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª quick_test_process_domains ã‚’å®Ÿè¡Œã—ã¾ã™â€¦\n",
      "\n",
      "======================================================================\n",
      "ğŸ§ª process_domains_batchç°¡æ˜“ãƒ†ã‚¹ãƒˆ\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ ãƒ†ã‚¹ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³:\n",
      "  - test-phishing.com (MLç¢ºç‡: 0.15)\n",
      "  - normal-site.org (MLç¢ºç‡: 0.85)\n",
      "\n",
      "âš¡ ãƒãƒƒãƒå‡¦ç†é–‹å§‹:\n",
      "  - å‡¦ç†æ•°: 2ä»¶\n",
      "  - ä¸¦åˆ—æ•°: 10\n",
      "\n",
      "ğŸ“Š ãƒãƒƒãƒå‡¦ç†å®Œäº†:\n",
      "  - æˆåŠŸ: 2/2\n",
      "  - ã‚¨ãƒ©ãƒ¼: 0/2\n",
      "\n",
      "âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸï¼\n",
      "  å‡¦ç†çµæœ: 2ä»¶\n",
      "  âœ… test-phishing.com: ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°=No\n",
      "  âœ… normal-site.org: ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°=No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3988833/3034150986.py:106: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = fn(**kwargs)  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "# === Cell 06-D-TEST: Minimal smoke test ===\n",
    "try:\n",
    "    if 'quick_test_process_domains' in globals():\n",
    "        print(\"ğŸ§ª quick_test_process_domains ã‚’å®Ÿè¡Œã—ã¾ã™â€¦\")\n",
    "        quick_test_process_domains()\n",
    "    else:\n",
    "        print(\"â„¹ï¸ quick_test_process_domains ãŒç„¡ã„ã®ã§ç°¡æ˜“ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "        samples = [('example.com', 0.1), ('login-paypa1.com', 0.7)]\n",
    "        if 'process_domains_batch' in globals():\n",
    "            domains = [d for d,_ in samples]\n",
    "            probs   = [float(p) for _d,p in samples]\n",
    "            out = process_domains_batch(domains, probs, cfg=(cfg if 'cfg' in globals() else {}))\n",
    "        elif 'evaluate_domain' in globals():\n",
    "            out = [evaluate_domain(d, float(p), cfg=(cfg if 'cfg' in globals() else {})) for d,p in samples]\n",
    "        else:\n",
    "            raise RuntimeError(\"è©•ä¾¡é–¢æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚evaluate_domain / process_domains_batch ã®å®šç¾©ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            if isinstance(out, dict): out = [out]\n",
    "            display(pd.DataFrame(out))\n",
    "        except Exception as e:\n",
    "            print(\"Raw output:\", out)\n",
    "            print(\"Display error:\", e)\n",
    "except Exception as e:\n",
    "    print(\"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã«ã‚¨ãƒ©ãƒ¼:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe39f752-9c4b-4686-af39-b62d8f395a33",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[20]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90d1a08c-bf93-4834-abd7-48e5a9c33ad9",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ” é–‹ç™ºãƒ¢ãƒ¼ãƒ‰è¨­å®šã®ç¢ºèª\n",
      "======================================================================\n",
      "\n",
      "1ï¸âƒ£ ç’°å¢ƒå¤‰æ•° DEV_MODE ã®å€¤:\n",
      "   åŸå€¤: 'False'\n",
      "   å°æ–‡å­—å¤‰æ›å¾Œ: 'false'\n",
      "\n",
      "2ï¸âƒ£ DEVELOPMENT_MODE ã®åˆ¤å®šçµæœ:\n",
      "   False\n",
      "\n",
      "ğŸ“Œ ç¾åœ¨ã®çŠ¶æ…‹: ğŸ”´ æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ï¼ˆé–‹ç™ºãƒ¢ãƒ¼ãƒ‰ç„¡åŠ¹ï¼‰\n",
      "   - ã‚µãƒ³ãƒ—ãƒ«æ•°: 200ä»¶\n",
      "   - ä¸¦åˆ—æ•°: 10\n",
      "   - å‹•ä½œ: å¤§é‡å‡¦ç†\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¡ ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹æ–¹æ³•:\n",
      "======================================================================\n",
      "\n",
      "é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹:\n",
      "   export DEV_MODE=true\n",
      "   ã¾ãŸã¯\n",
      "   DEV_MODE=true jupyter lab\n",
      "\n",
      "æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹ï¼ˆé–‹ç™ºãƒ¢ãƒ¼ãƒ‰ç„¡åŠ¹ï¼‰:\n",
      "   export DEV_MODE=false\n",
      "   ã¾ãŸã¯\n",
      "   unset DEV_MODE\n",
      "   ã¾ãŸã¯ä½•ã‚‚è¨­å®šã—ãªã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ” é–‹ç™ºãƒ¢ãƒ¼ãƒ‰è¨­å®šã®ç¢ºèª\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª\n",
    "env_value = os.getenv('DEV_MODE', 'False')\n",
    "print(f\"\\n1ï¸âƒ£ ç’°å¢ƒå¤‰æ•° DEV_MODE ã®å€¤:\")\n",
    "print(f\"   åŸå€¤: '{env_value}'\")\n",
    "print(f\"   å°æ–‡å­—å¤‰æ›å¾Œ: '{env_value.lower()}'\")\n",
    "\n",
    "# 2. åˆ¤å®šçµæœ\n",
    "development_mode = env_value.lower() == 'true'\n",
    "print(f\"\\n2ï¸âƒ£ DEVELOPMENT_MODE ã®åˆ¤å®šçµæœ:\")\n",
    "print(f\"   {development_mode}\")\n",
    "\n",
    "# 3. ç¾åœ¨ã®ãƒ¢ãƒ¼ãƒ‰\n",
    "if development_mode:\n",
    "    print(\"\\nğŸ“Œ ç¾åœ¨ã®çŠ¶æ…‹: ğŸŸ¢ é–‹ç™ºãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹\")\n",
    "    print(\"   - ã‚µãƒ³ãƒ—ãƒ«æ•°: 50ä»¶\")\n",
    "    print(\"   - ä¸¦åˆ—æ•°: 12\")\n",
    "    print(\"   - å‹•ä½œ: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å‡¦ç†\")\n",
    "else:\n",
    "    print(\"\\nğŸ“Œ ç¾åœ¨ã®çŠ¶æ…‹: ğŸ”´ æœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ï¼ˆé–‹ç™ºãƒ¢ãƒ¼ãƒ‰ç„¡åŠ¹ï¼‰\")\n",
    "    print(\"   - ã‚µãƒ³ãƒ—ãƒ«æ•°: 200ä»¶\")\n",
    "    print(\"   - ä¸¦åˆ—æ•°: 10\")\n",
    "    print(\"   - å‹•ä½œ: å¤§é‡å‡¦ç†\")\n",
    "\n",
    "# 4. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã®ç¢ºèª\n",
    "if 'DEVELOPMENT_MODE' in globals():\n",
    "    print(f\"\\n3ï¸âƒ£ ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° DEVELOPMENT_MODE:\")\n",
    "    print(f\"   {globals()['DEVELOPMENT_MODE']}\")\n",
    "\n",
    "# 5. ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆæ–¹æ³•\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ’¡ ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹æ–¹æ³•:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\né–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹:\")\n",
    "print(\"   export DEV_MODE=true\")\n",
    "print(\"   ã¾ãŸã¯\")\n",
    "print(\"   DEV_MODE=true jupyter lab\")\n",
    "print(\"\\næœ¬ç•ªãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹ï¼ˆé–‹ç™ºãƒ¢ãƒ¼ãƒ‰ç„¡åŠ¹ï¼‰:\")\n",
    "print(\"   export DEV_MODE=false\")\n",
    "print(\"   ã¾ãŸã¯\")\n",
    "print(\"   unset DEV_MODE\")\n",
    "print(\"   ã¾ãŸã¯ä½•ã‚‚è¨­å®šã—ãªã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a15b0c0f-797a-4c1c-bd95-d6f496597baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”§ TRACE åˆæœŸåŒ–ï¼ˆPlan A: llm ã‚’æ­£ã¨ã™ã‚‹ï¼‰\n",
      "======================================================================\n",
      "  âœ… llm æ—¢å­˜ã®ãŸã‚ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™\n",
      "\n",
      "ğŸ“Œ å‚ç…§ãƒãƒ³ãƒ‰ãƒ«ã®è¦ç´„:\n",
      "  - llm ã®å‹: <class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "\n",
      "âœ… TRACE åˆæœŸåŒ–å®Œäº†ï¼ˆä»¥é™ã¯ llm ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼‰\n"
     ]
    }
   ],
   "source": [
    "# === Cell 06-TRACE-FIX (Plan A: tolerant & llm-canonical) ===\n",
    "\n",
    "\"\"\"\n",
    "ç›®çš„:\n",
    " - llm ã‚’æ­£ã¨ã™ã‚‹ï¼ˆllm_fixed ã¯ä»»æ„ï¼‰\n",
    " - llm / llm_fixed ã®ã©ã¡ã‚‰ã‚‚ç„¡ã„å ´åˆã¯ã€ã“ã®ã‚»ãƒ«ã§æœ€å°é™ã®åˆæœŸåŒ–ã‚’è¡Œã†\n",
    "\"\"\"\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "def _init_llm_from_config():\n",
    "    \"\"\"config.json / ENV ã‹ã‚‰æœ€å°é™ã® LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¦è¿”ã™\"\"\"\n",
    "    _cfg = {}\n",
    "    cfg_path = Path(\"/mnt/data/config.json\")\n",
    "    if cfg_path.exists():\n",
    "        try:\n",
    "            _cfg = json.loads(cfg_path.read_text(encoding=\"utf-8\"))\n",
    "            print(\"  âœ… config.json ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
    "        except Exception as e:\n",
    "            print(\"  âš ï¸ config.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—:\", e)\n",
    "    llm_conf = _cfg.get(\"llm\", {}) if isinstance(_cfg, dict) else {}\n",
    "    base_url = llm_conf.get(\"base_url\") or os.getenv(\"VLLM_BASE_URL\") or \"http://127.0.0.1:30000/v1\"\n",
    "    if not (str(base_url).startswith(\"http://\") or str(base_url).startswith(\"https://\")):\n",
    "        base_url = \"http://\" + str(base_url)\n",
    "    model = llm_conf.get(\"model\") or os.getenv(\"LLM_MODEL\") or _cfg.get(\"model\") or \"Qwen/Qwen3-14B-FP8\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\") or llm_conf.get(\"api_key\") or \"EMPTY\"  # vLLMã¯ä»»æ„æ–‡å­—åˆ—ã§å¯\n",
    "\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        print(\"  âœ… OpenAIäº’æ›ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"OpenAIäº’æ›ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆã«å¤±æ•—: {e}\")\n",
    "\n",
    "    class _LLMWrapper:\n",
    "        def __init__(self, c, m):\n",
    "            self.client, self.model = c, m\n",
    "        def chat(self, messages, **kwargs):\n",
    "            return self.client.chat.completions.create(model=self.model, messages=messages, **kwargs)\n",
    "        def completions(self, prompt, **kwargs):\n",
    "            return self.client.completions.create(model=self.model, prompt=prompt, **kwargs)\n",
    "    return _LLMWrapper(client, model)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ”§ TRACE åˆæœŸåŒ–ï¼ˆPlan A: llm ã‚’æ­£ã¨ã™ã‚‹ï¼‰\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1) llm / llm_fixed ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯\n",
    "if 'llm' in globals():\n",
    "    print(\"  âœ… llm æ—¢å­˜ã®ãŸã‚ãã®ã¾ã¾ä½¿ç”¨ã—ã¾ã™\")\n",
    "elif 'llm_fixed' in globals():\n",
    "    llm = llm_fixed\n",
    "    print(\"  âœ… llm_fixed ã‚’æ¤œå‡º â†’ llm ã¸ã‚¨ã‚¤ãƒªã‚¢ã‚¹è¨­å®šã—ã¾ã—ãŸ\")\n",
    "else:\n",
    "    print(\"  â„¹ï¸ llm / llm_fixed ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã“ã®ã‚»ãƒ«ã§æœ€å°é™ã®åˆæœŸåŒ–ã‚’è¡Œã„ã¾ã™\")\n",
    "    llm = _init_llm_from_config()\n",
    "    # äº’æ›æ€§ã®ãŸã‚ llm_fixed ã‚‚ä¸ãˆã¦ãŠãï¼ˆç„¡å®³ï¼‰\n",
    "    llm_fixed = llm\n",
    "    print(\"  âœ… llm / llm_fixed ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")\n",
    "\n",
    "# 2) å‚è€ƒæƒ…å ±ã®è¡¨ç¤ºï¼ˆä»»æ„ï¼‰\n",
    "try:\n",
    "    print(\"\\nğŸ“Œ å‚ç…§ãƒãƒ³ãƒ‰ãƒ«ã®è¦ç´„:\")\n",
    "    print(f\"  - llm ã®å‹: {type(llm)}\")\n",
    "    if 'llm_fixed' in globals():\n",
    "        print(f\"  - llm is llm_fixed: {llm is llm_fixed}\")\n",
    "except Exception as _e:\n",
    "    pass\n",
    "\n",
    "print(\"\\nâœ… TRACE åˆæœŸåŒ–å®Œäº†ï¼ˆä»¥é™ã¯ llm ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6eab3275-62b4-4baf-b5bc-b99e889582a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 06-LEGACY-ALIAS (è¶…ãƒŸãƒ‹äº’æ›ã‚»ãƒ«) ===\n",
    "if 'llm' in globals() and 'llm_fixed' not in globals():\n",
    "    llm_fixed = llm\n",
    "if 'evaluate_domain' in globals() and 'evaluate_domain_fixed' not in globals():\n",
    "    evaluate_domain_fixed = evaluate_domain\n",
    "if 'process_domains_batch' in globals() and 'process_domains_batch_fixed' not in globals():\n",
    "    process_domains_batch_fixed = process_domains_batch\n",
    "if 'AI_AGENT' in globals() and 'FIXED_AI_AGENT' not in globals():\n",
    "    FIXED_AI_AGENT = AI_AGENT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c085dc7-ca19-43d7-ba85-7819593de375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "print('llm' in globals(), 'llm_fixed' in globals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c68b34f-a3b6-46c9-88a6-7f38f641c1cd",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”§ ã‚»ãƒ«06-TRACEç”¨ã®å¤‰æ•°åä¿®æ­£\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ å¤‰æ•°åã®çµ±ä¸€:\n",
      "âœ… llm_fixed â†’ llm: å¤‰æ•°åã‚’çµ±ä¸€ã—ã¾ã—ãŸ\n",
      "   å‹: ChatOpenAI\n",
      "\n",
      "ğŸ“‹ å¿…è¦ãªå¤‰æ•°ã®ç¢ºèª:\n",
      "âœ… tools: 4å€‹ã®ãƒ„ãƒ¼ãƒ«ãŒå®šç¾©æ¸ˆã¿\n",
      "   ãƒ„ãƒ¼ãƒ«: brand_impersonation_check, certificate_analysis, short_domain_analysis, contextual_risk_assessment\n",
      "âœ… brand_keywords: 100å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰ãŒå®šç¾©æ¸ˆã¿\n",
      "   ä¾‹: allegro, facebook, microsoft, at&t, adobe...\n",
      "âœ… DANGEROUS_TLDS: 22å€‹å®šç¾©æ¸ˆã¿\n",
      "âœ… HIGH_RISK_WORDS: 100å€‹å®šç¾©æ¸ˆã¿\n",
      "âœ… cert_full_info_map: 4132ä»¶ã®ãƒ‡ãƒ¼ã‚¿\n",
      "\n",
      "ğŸ“‹ ãƒˆãƒ¬ãƒ¼ã‚¹é–¢é€£å¤‰æ•°ã®ç¢ºèª:\n",
      "âœ… ENABLE_TRACING: False\n",
      "âœ… DEVELOPMENT_MODE: True\n",
      "âœ… ai_session_id: 20251019_171454\n",
      "âœ… output_dirs: å®šç¾©æ¸ˆã¿\n",
      "\n",
      "======================================================================\n",
      "âœ… ã‚»ãƒ«06-TRACEç”¨ã®æº–å‚™å®Œäº†\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š å¤‰æ•°ã‚µãƒãƒªãƒ¼:\n",
      "  - llm: ChatOpenAI\n",
      "  - llm is llm_fixed: True\n",
      "  - tools: 4å€‹\n",
      "  - brand_keywords: 100å€‹ï¼ˆçµ±è¨ˆçš„ã«æŠ½å‡ºæ¸ˆã¿ï¼‰\n",
      "  - DANGEROUS_TLDS: 22å€‹ï¼ˆçµ±è¨ˆçš„ã«æŠ½å‡ºæ¸ˆã¿ï¼‰\n",
      "  - HIGH_RISK_WORDS: 100å€‹ï¼ˆçµ±è¨ˆçš„ã«æŠ½å‡ºæ¸ˆã¿ï¼‰\n",
      "\n",
      "ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
      "  1. ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦å¤‰æ•°åã‚’ä¿®æ­£\n",
      "  2. ã‚»ãƒ«06-TRACEã‚’å†å®Ÿè¡Œ\n",
      "  3. TracedPhishingDetectionAgentãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 06-TRACE-FIX\n",
    "æ¦‚è¦: ã‚»ãƒ«06-TRACEã§å¿…è¦ãªå¤‰æ•°åã®ä¿®æ­£ï¼ˆæ—¢å­˜å¤‰æ•°ã®å‚ç…§ã®ã¿ï¼‰\n",
    "å…¥åŠ›: llm_fixedï¼ˆã‚»ãƒ«06ã‹ã‚‰ï¼‰\n",
    "å‡ºåŠ›: llmï¼ˆllm_fixedã®åˆ¥åã¨ã—ã¦è¨­å®šï¼‰\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”§ ã‚»ãƒ«06-TRACEç”¨ã®å¤‰æ•°åä¿®æ­£\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1. ä¸»è¦ãªå¤‰æ•°ã®ç¢ºèªã¨ä¿®æ­£ ==========\n",
    "\n",
    "print(\"\\nğŸ“‹ å¤‰æ•°åã®çµ±ä¸€:\")\n",
    "\n",
    "# llm_fixedã‚’llmã¨ã—ã¦å‚ç…§å¯èƒ½ã«ã™ã‚‹\n",
    "if 'llm_fixed' in globals():\n",
    "    llm = llm_fixed\n",
    "    globals()['llm'] = llm\n",
    "    print(\"âœ… llm_fixed â†’ llm: å¤‰æ•°åã‚’çµ±ä¸€ã—ã¾ã—ãŸ\")\n",
    "    print(f\"   å‹: {type(llm).__name__}\")\n",
    "else:\n",
    "    print(\"âŒ ã‚¨ãƒ©ãƒ¼: llm_fixedãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"   â†’ ã‚»ãƒ«06ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "    raise NameError(\"llm_fixed ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# ========== 2. å¿…è¦ãªå¤‰æ•°ã®å­˜åœ¨ç¢ºèª ==========\n",
    "\n",
    "print(\"\\nğŸ“‹ å¿…è¦ãªå¤‰æ•°ã®ç¢ºèª:\")\n",
    "\n",
    "# tools\n",
    "if 'tools' in globals():\n",
    "    print(f\"âœ… tools: {len(tools)}å€‹ã®ãƒ„ãƒ¼ãƒ«ãŒå®šç¾©æ¸ˆã¿\")\n",
    "    tool_names = [tool.name for tool in tools]\n",
    "    print(f\"   ãƒ„ãƒ¼ãƒ«: {', '.join(tool_names)}\")\n",
    "else:\n",
    "    print(\"âŒ tools: æœªå®šç¾©\")\n",
    "    print(\"   â†’ ã‚»ãƒ«04ã¾ãŸã¯05ã§ãƒ„ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¦ãã ã•ã„\")\n",
    "    raise NameError(\"tools ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
    "\n",
    "# brand_keywords\n",
    "if 'brand_keywords' in globals():\n",
    "    print(f\"âœ… brand_keywords: {len(brand_keywords)}å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰ãŒå®šç¾©æ¸ˆã¿\")\n",
    "    sample_brands = list(brand_keywords)[:5]\n",
    "    print(f\"   ä¾‹: {', '.join(sample_brands)}...\")\n",
    "else:\n",
    "    print(\"âŒ brand_keywords: æœªå®šç¾©\")\n",
    "    print(\"   â†’ ã‚»ãƒ«01ã¾ãŸã¯02ã§èª­ã¿è¾¼ã‚“ã§ãã ã•ã„\")\n",
    "    raise NameError(\"brand_keywords ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
    "\n",
    "# DANGEROUS_TLDS\n",
    "if 'DANGEROUS_TLDS' in globals():\n",
    "    print(f\"âœ… DANGEROUS_TLDS: {len(DANGEROUS_TLDS)}å€‹å®šç¾©æ¸ˆã¿\")\n",
    "else:\n",
    "    print(\"âš ï¸ DANGEROUS_TLDS: æœªå®šç¾©ï¼ˆã‚»ãƒ«04ã§çµ±è¨ˆçš„ã«ç”Ÿæˆã•ã‚Œã‚‹ã¯ãšï¼‰\")\n",
    "\n",
    "# HIGH_RISK_WORDS\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    print(f\"âœ… HIGH_RISK_WORDS: {len(HIGH_RISK_WORDS)}å€‹å®šç¾©æ¸ˆã¿\")\n",
    "else:\n",
    "    print(\"âš ï¸ HIGH_RISK_WORDS: æœªå®šç¾©ï¼ˆã‚»ãƒ«04ã§çµ±è¨ˆçš„ã«ç”Ÿæˆã•ã‚Œã‚‹ã¯ãšï¼‰\")\n",
    "\n",
    "# cert_full_info_map\n",
    "if 'cert_full_info_map' in globals():\n",
    "    print(f\"âœ… cert_full_info_map: {len(cert_full_info_map)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿\")\n",
    "else:\n",
    "    print(\"âš ï¸ cert_full_info_map: æœªå®šç¾©ï¼ˆã‚»ãƒ«02ã§ä½œæˆã•ã‚Œã‚‹ã¯ãšï¼‰\")\n",
    "\n",
    "# ========== 3. ãƒˆãƒ¬ãƒ¼ã‚¹é–¢é€£å¤‰æ•°ã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\nğŸ“‹ ãƒˆãƒ¬ãƒ¼ã‚¹é–¢é€£å¤‰æ•°ã®ç¢ºèª:\")\n",
    "\n",
    "# ENABLE_TRACING\n",
    "if 'ENABLE_TRACING' not in globals():\n",
    "    ENABLE_TRACING = True  # ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’æœ‰åŠ¹åŒ–\n",
    "    globals()['ENABLE_TRACING'] = ENABLE_TRACING\n",
    "print(f\"âœ… ENABLE_TRACING: {ENABLE_TRACING}\")\n",
    "\n",
    "# DEVELOPMENT_MODE  \n",
    "if 'DEVELOPMENT_MODE' not in globals():\n",
    "    import os\n",
    "    DEVELOPMENT_MODE = os.getenv('DEV_MODE', 'True').lower() == 'true'\n",
    "    globals()['DEVELOPMENT_MODE'] = DEVELOPMENT_MODE\n",
    "print(f\"âœ… DEVELOPMENT_MODE: {DEVELOPMENT_MODE}\")\n",
    "\n",
    "# ai_session_id\n",
    "if 'ai_session_id' not in globals():\n",
    "    from datetime import datetime\n",
    "    ai_session_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    globals()['ai_session_id'] = ai_session_id\n",
    "print(f\"âœ… ai_session_id: {ai_session_id}\")\n",
    "\n",
    "# output_dirs\n",
    "if 'output_dirs' not in globals():\n",
    "    output_dirs = {\n",
    "        'results': os.path.join(RESULTS_DIR, \"ai_agent\", str(ai_session_id)),\n",
    "        'logs': f'logs/ai_agent/{ai_session_id}',\n",
    "        'traces': f'traces/ai_agent/{ai_session_id}'\n",
    "    }\n",
    "    globals()['output_dirs'] = output_dirs\n",
    "print(\"âœ… output_dirs: å®šç¾©æ¸ˆã¿\")\n",
    "\n",
    "# ========== 4. æœ€çµ‚ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… ã‚»ãƒ«06-TRACEç”¨ã®æº–å‚™å®Œäº†\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“Š å¤‰æ•°ã‚µãƒãƒªãƒ¼:\")\n",
    "print(f\"  - llm: {type(llm).__name__}\")\n",
    "print(f\"  - llm is llm_fixed: {llm is llm_fixed}\")\n",
    "print(f\"  - tools: {len(tools)}å€‹\")\n",
    "print(f\"  - brand_keywords: {len(brand_keywords)}å€‹ï¼ˆçµ±è¨ˆçš„ã«æŠ½å‡ºæ¸ˆã¿ï¼‰\")\n",
    "\n",
    "if 'DANGEROUS_TLDS' in globals():\n",
    "    print(f\"  - DANGEROUS_TLDS: {len(DANGEROUS_TLDS)}å€‹ï¼ˆçµ±è¨ˆçš„ã«æŠ½å‡ºæ¸ˆã¿ï¼‰\")\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    print(f\"  - HIGH_RISK_WORDS: {len(HIGH_RISK_WORDS)}å€‹ï¼ˆçµ±è¨ˆçš„ã«æŠ½å‡ºæ¸ˆã¿ï¼‰\")\n",
    "\n",
    "print(\"\\nğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"  1. ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦å¤‰æ•°åã‚’ä¿®æ­£\")\n",
    "print(\"  2. ã‚»ãƒ«06-TRACEã‚’å†å®Ÿè¡Œ\")\n",
    "print(\"  3. TracedPhishingDetectionAgentãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª\")\n",
    "\n",
    "# é‡è¦ãªè­¦å‘Š\n",
    "if 'DANGEROUS_TLDS' not in globals() or 'HIGH_RISK_WORDS' not in globals():\n",
    "    print(\"\\nâš ï¸ è­¦å‘Š:\")\n",
    "    print(\"  DANGEROUS_TLDSã¾ãŸã¯HIGH_RISK_WORDSãŒæœªå®šç¾©ã§ã™ã€‚\")\n",
    "    print(\"  ã‚»ãƒ«04ï¼ˆå±é™ºå˜èªãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ±è¨ˆçš„æŠ½å‡ºï¼‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "    print(\"  ã“ã‚Œã‚‰ã¯å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çµ±è¨ˆçš„ã«ç”Ÿæˆã•ã‚Œã‚‹é‡è¦ãªãƒªã‚¹ãƒˆã§ã™ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "736ded61-8d3e-47cf-bfef-5d4cf40d7cb3",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[22]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8863a58a-8eae-45ae-ba21-b8604e84c732",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ”§ ã‚»ãƒ«07-RERUNç”¨ã®é–¢æ•°ä¿®æ­£\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ é–¢æ•°ã‚·ã‚°ãƒãƒãƒ£ã®ç¢ºèª:\n",
      "âœ… process_domains_batch_fixed ã®ã‚·ã‚°ãƒãƒãƒ£:\n",
      "   (domains_data: 'List[Tuple[str, float]]', max_concurrent: 'int' = 10) -> 'List[Dict]'\n",
      "\n",
      "   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\n",
      "   - domains_data: å¿…é ˆ\n",
      "   - max_concurrent: 10\n",
      "\n",
      "======================================================================\n",
      "âœ… é–¢æ•°ä¿®æ­£å®Œäº†\n",
      "======================================================================\n",
      "\n",
      "ğŸ“‹ ä¿®æ­£å†…å®¹:\n",
      "  1. process_domains_batch_fixed ã®æ­£ã—ã„å‘¼ã³å‡ºã—æ–¹æ³•\n",
      "     - awaitã‚’ä½¿ã‚ãªã„ï¼ˆåŒæœŸé–¢æ•°ã®ãŸã‚ï¼‰\n",
      "     - max_concurrent â†’ batch_size ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
      "     - ThreadPoolExecutorã§éåŒæœŸåŒ–\n",
      "\n",
      "ğŸ“‹ æä¾›ã™ã‚‹é–¢æ•°:\n",
      "  - evaluate_with_fixed_agent_corrected: éåŒæœŸç‰ˆï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰\n",
      "  - evaluate_samples_sync: åŒæœŸç‰ˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰\n",
      "\n",
      "ğŸ¯ æ¨å¥¨ã•ã‚Œã‚‹ä½¿ç”¨æ–¹æ³•:\n",
      "  1. éåŒæœŸç’°å¢ƒï¼ˆJupyterï¼‰:\n",
      "     results_df = await evaluate_with_fixed_agent_corrected(samples_df)\n",
      "  2. åŒæœŸç’°å¢ƒ:\n",
      "     results_df = evaluate_samples_sync(samples_df)\n",
      "\n",
      "ğŸ’¡ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯:\n",
      "   test_evaluation_functions()\n",
      "\n",
      "ã¾ãŸã¯ã€ã‚»ãƒ«07-RERUNã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 07-RERUN-FIX\n",
    "æ¦‚è¦: process_domains_batch_fixedé–¢æ•°ã®å‘¼ã³å‡ºã—æ–¹æ³•ã‚’ä¿®æ­£\n",
    "å…¥åŠ›: evaluate_domain, process_domains_batch_fixed\n",
    "å‡ºåŠ›: ä¿®æ­£ç‰ˆã®evaluate_with_fixed_agenté–¢æ•°\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ”§ ã‚»ãƒ«07-RERUNç”¨ã®é–¢æ•°ä¿®æ­£\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ========== 1. é–¢æ•°ã®ã‚·ã‚°ãƒãƒãƒ£ã‚’ç¢ºèª ==========\n",
    "\n",
    "print(\"\\nğŸ“‹ é–¢æ•°ã‚·ã‚°ãƒãƒãƒ£ã®ç¢ºèª:\")\n",
    "\n",
    "if 'process_domains_batch_fixed' in globals():\n",
    "    import inspect\n",
    "    sig = inspect.signature(process_domains_batch_fixed)\n",
    "    print(f\"âœ… process_domains_batch_fixed ã®ã‚·ã‚°ãƒãƒãƒ£:\")\n",
    "    print(f\"   {sig}\")\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°\n",
    "    print(\"\\n   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:\")\n",
    "    for param_name, param in sig.parameters.items():\n",
    "        default = param.default\n",
    "        if default == inspect.Parameter.empty:\n",
    "            default = \"å¿…é ˆ\"\n",
    "        print(f\"   - {param_name}: {default}\")\n",
    "else:\n",
    "    print(\"âŒ process_domains_batch_fixed ãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 2. ä¿®æ­£ç‰ˆã®evaluate_with_fixed_agenté–¢æ•° ==========\n",
    "\n",
    "async def evaluate_with_fixed_agent_corrected(samples_df: pd.DataFrame, \n",
    "                                              max_concurrent: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ä¿®æ­£ç‰ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§è©•ä¾¡ï¼ˆé–¢æ•°å‘¼ã³å‡ºã—ä¿®æ­£ç‰ˆï¼‰\n",
    "    \n",
    "    Args:\n",
    "        samples_df: è©•ä¾¡å¯¾è±¡ã®DataFrame\n",
    "        max_concurrent: æœ€å¤§ä¸¦åˆ—æ•°\n",
    "    \n",
    "    Returns:\n",
    "        è©•ä¾¡çµæœã®DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    if samples_df.empty:\n",
    "        print(\"âŒ No samples to evaluate\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # MLç¢ºç‡ã‚«ãƒ©ãƒ ã®ç¢ºèª\n",
    "    prob_col = 'prediction_proba' if 'prediction_proba' in samples_df.columns else 'ml_probability'\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "    domains = samples_df['domain'].tolist()\n",
    "    ml_probabilities = samples_df[prob_col].tolist()\n",
    "    \n",
    "    print(f\"\\nâš¡ è©•ä¾¡é–‹å§‹:\")\n",
    "    print(f\"  - å¯¾è±¡: {len(domains)}ä»¶\")\n",
    "    print(f\"  - ä¸¦åˆ—æ•°: {max_concurrent}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # process_domains_batch_fixedã¯åŒæœŸé–¢æ•°ãªã®ã§ã€ThreadPoolExecutorã§éåŒæœŸåŒ–\n",
    "    loop = asyncio.get_event_loop()\n",
    "    \n",
    "    # ä¿®æ­£ç‰ˆé–¢æ•°ã‚’ä½¿ç”¨ï¼ˆæ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ï¼‰\n",
    "    if 'process_domains_batch_fixed' in globals():\n",
    "        print(\"  - ä½¿ç”¨: process_domains_batch_fixedï¼ˆä¿®æ­£ç‰ˆãƒ»åŒæœŸï¼‰\")\n",
    "        \n",
    "        # ThreadPoolExecutorã§éåŒæœŸå®Ÿè¡Œ\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = loop.run_in_executor(\n",
    "                executor,\n",
    "                process_domains_batch_fixed,\n",
    "                domains,           # ç¬¬1å¼•æ•°: domains\n",
    "                ml_probabilities,  # ç¬¬2å¼•æ•°: ml_probabilities\n",
    "                max_concurrent,    # ç¬¬3å¼•æ•°: batch_sizeï¼ˆmax_concurrentã‚’ä½¿ç”¨ï¼‰\n",
    "                False             # ç¬¬4å¼•æ•°: enable_trace\n",
    "            )\n",
    "            results = await future\n",
    "            \n",
    "    elif 'process_domains_batch' in globals():\n",
    "        print(\"  - ä½¿ç”¨: process_domains_batchï¼ˆå…ƒç‰ˆãƒ»éåŒæœŸï¼‰\")\n",
    "        \n",
    "        # (domain, ml_probability)ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "        domains_data = list(zip(domains, ml_probabilities))\n",
    "        \n",
    "        # éåŒæœŸç‰ˆã‚’ç›´æ¥å‘¼ã³å‡ºã—\n",
    "        results = await process_domains_batch(domains_data, max_concurrent=max_concurrent)\n",
    "    else:\n",
    "        print(\"âŒ è©•ä¾¡é–¢æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # çµæœã‚’DataFrameã«å¤‰æ›\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # å…ƒã®DataFrameã®æƒ…å ±ã‚’è¿½åŠ \n",
    "    if not results_df.empty and not samples_df.empty:\n",
    "        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã‚‹\n",
    "        results_df.index = samples_df.index[:len(results_df)]\n",
    "        \n",
    "        # å…ƒã®ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰\n",
    "        for col in samples_df.columns:\n",
    "            if col not in results_df.columns:\n",
    "                results_df[col] = samples_df[col]\n",
    "    \n",
    "    print(f\"\\nâœ… è©•ä¾¡å®Œäº†:\")\n",
    "    print(f\"  - å‡¦ç†æ™‚é–“: {elapsed:.2f}ç§’\")\n",
    "    print(f\"  - å‡¦ç†é€Ÿåº¦: {len(domains)/elapsed:.2f} ãƒ‰ãƒ¡ã‚¤ãƒ³/ç§’\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ========== 3. åˆ¥ã®å®Ÿè£…æ–¹æ³•ï¼ˆã‚ˆã‚Šç°¡å˜ï¼‰ ==========\n",
    "\n",
    "def evaluate_samples_sync(samples_df: pd.DataFrame, \n",
    "                         batch_size: int = 24) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    åŒæœŸçš„ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’è©•ä¾¡ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰\n",
    "    \n",
    "    Args:\n",
    "        samples_df: è©•ä¾¡å¯¾è±¡ã®DataFrame\n",
    "        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    \n",
    "    Returns:\n",
    "        è©•ä¾¡çµæœã®DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    if samples_df.empty:\n",
    "        print(\"âŒ No samples to evaluate\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # MLç¢ºç‡ã‚«ãƒ©ãƒ ã®ç¢ºèª\n",
    "    prob_col = 'prediction_proba' if 'prediction_proba' in samples_df.columns else 'ml_probability'\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "    domains = samples_df['domain'].tolist()\n",
    "    ml_probabilities = samples_df[prob_col].tolist()\n",
    "    \n",
    "    print(f\"\\nâš¡ åŒæœŸè©•ä¾¡é–‹å§‹:\")\n",
    "    print(f\"  - å¯¾è±¡: {len(domains)}ä»¶\")\n",
    "    print(f\"  - ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # è©•ä¾¡å®Ÿè¡Œ\n",
    "    if 'process_domains_batch_fixed' in globals():\n",
    "        results = process_domains_batch_fixed(\n",
    "            domains, \n",
    "            ml_probabilities, \n",
    "            batch_size=batch_size,\n",
    "            enable_trace=False\n",
    "        )\n",
    "    elif 'evaluate_domain' in globals():\n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å€‹åˆ¥ã«è©•ä¾¡\n",
    "        print(\"  âš ï¸ process_domains_batch_fixedãŒæœªå®šç¾©ã€‚å€‹åˆ¥è©•ä¾¡ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\")\n",
    "        results = []\n",
    "        for domain, ml_prob in zip(domains, ml_probabilities):\n",
    "            result = evaluate_domain(domain, ml_prob, enable_trace=False)\n",
    "            results.append(result)\n",
    "    else:\n",
    "        print(\"âŒ è©•ä¾¡é–¢æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # çµæœã‚’DataFrameã«å¤‰æ›\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\nâœ… è©•ä¾¡å®Œäº†:\")\n",
    "    print(f\"  - å‡¦ç†æ™‚é–“: {elapsed:.2f}ç§’\")\n",
    "    print(f\"  - å‡¦ç†é€Ÿåº¦: {len(domains)/elapsed:.2f} ãƒ‰ãƒ¡ã‚¤ãƒ³/ç§’\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ========== 4. ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ ==========\n",
    "\n",
    "globals()['evaluate_with_fixed_agent_corrected'] = evaluate_with_fixed_agent_corrected\n",
    "globals()['evaluate_samples_sync'] = evaluate_samples_sync\n",
    "\n",
    "# å…ƒã®é–¢æ•°ã‚’ä¸Šæ›¸ãï¼ˆã‚»ãƒ«07-RERUNã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰\n",
    "globals()['evaluate_with_fixed_agent'] = evaluate_with_fixed_agent_corrected\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… é–¢æ•°ä¿®æ­£å®Œäº†\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ“‹ ä¿®æ­£å†…å®¹:\")\n",
    "print(\"  1. process_domains_batch_fixed ã®æ­£ã—ã„å‘¼ã³å‡ºã—æ–¹æ³•\")\n",
    "print(\"     - awaitã‚’ä½¿ã‚ãªã„ï¼ˆåŒæœŸé–¢æ•°ã®ãŸã‚ï¼‰\")\n",
    "print(\"     - max_concurrent â†’ batch_size ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\")\n",
    "print(\"     - ThreadPoolExecutorã§éåŒæœŸåŒ–\")\n",
    "\n",
    "print(\"\\nğŸ“‹ æä¾›ã™ã‚‹é–¢æ•°:\")\n",
    "print(\"  - evaluate_with_fixed_agent_corrected: éåŒæœŸç‰ˆï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰\")\n",
    "print(\"  - evaluate_samples_sync: åŒæœŸç‰ˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰\")\n",
    "\n",
    "print(\"\\nğŸ¯ æ¨å¥¨ã•ã‚Œã‚‹ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(\"  1. éåŒæœŸç’°å¢ƒï¼ˆJupyterï¼‰:\")\n",
    "print(\"     results_df = await evaluate_with_fixed_agent_corrected(samples_df)\")\n",
    "print(\"  2. åŒæœŸç’°å¢ƒ:\")\n",
    "print(\"     results_df = evaluate_samples_sync(samples_df)\")\n",
    "\n",
    "# ========== 5. ãƒ†ã‚¹ãƒˆ ==========\n",
    "\n",
    "def test_evaluation_functions():\n",
    "    \"\"\"è©•ä¾¡é–¢æ•°ã®ãƒ†ã‚¹ãƒˆ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ§ª è©•ä¾¡é–¢æ•°ã®ãƒ†ã‚¹ãƒˆ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ\n",
    "    test_data = pd.DataFrame({\n",
    "        'domain': ['test-phishing.com', 'normal-site.org'],\n",
    "        'prediction_proba': [0.15, 0.85]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nğŸ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:\")\n",
    "    print(test_data)\n",
    "    \n",
    "    try:\n",
    "        # åŒæœŸç‰ˆãƒ†ã‚¹ãƒˆ\n",
    "        print(\"\\n1ï¸âƒ£ åŒæœŸç‰ˆãƒ†ã‚¹ãƒˆ:\")\n",
    "        results = evaluate_samples_sync(test_data, batch_size=2)\n",
    "        if not results.empty:\n",
    "            print(\"âœ… æˆåŠŸ\")\n",
    "            print(f\"   çµæœ: {len(results)}ä»¶\")\n",
    "        else:\n",
    "            print(\"âŒ å¤±æ•—: ç©ºã®çµæœ\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    \n",
    "    print(\"\\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®ææ¡ˆ\n",
    "print(\"\\nğŸ’¡ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯:\")\n",
    "print(\"   test_evaluation_functions()\")\n",
    "print(\"\\nã¾ãŸã¯ã€ã‚»ãƒ«07-RERUNã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "992fb282-5ab6-4270-b3c7-2b4317e9babd",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [],
   "source": [
    "# In[23]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e78577a4-7781-4900-9641-e0293ea6440b",
   "metadata": {
    "title": "Cell 2: Apply config (NEW)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§ª ã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ã®ã¿ï¼‰\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª:\n",
      "----------------------------------------\n",
      "âŒ VLLM_CONFIG ãŒæœªå®šç¾©\n",
      "   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
      "\n",
      "2ï¸âƒ£ è©•ä¾¡é–¢æ•°ã®ç¢ºèª:\n",
      "----------------------------------------\n",
      "âœ… evaluate_domain é–¢æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™\n",
      "\n",
      "3ï¸âƒ£ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™:\n",
      "----------------------------------------\n",
      "âœ… ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«: 5ä»¶\n",
      "\n",
      "ğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³:\n",
      "   1. winrneteik.icu                                     (ML: 0.324)\n",
      "   2. mecano-normand.com                                 (ML: 0.070)\n",
      "   3. lindastilesfox.com                                 (ML: 0.121)\n",
      "   4. huwaifeng.cn                                       (ML: 0.196)\n",
      "   5. lesamisdevesseaux.com                              (ML: 0.142)\n",
      "\n",
      "4ï¸âƒ£ è©•ä¾¡å®Ÿè¡Œï¼ˆ1ä»¶ãšã¤å‡¦ç†ï¼‰:\n",
      "----------------------------------------\n",
      "\n",
      "ğŸš€ ãƒ†ã‚¹ãƒˆè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...\n",
      "================================================================================\n",
      "\n",
      "è©•ä¾¡ä¸­: winrneteik.icu\n",
      "  MLç¢ºç‡: 0.324\n",
      "  çµæœ: âœ… æ­£å¸¸\n",
      "  ä¿¡é ¼åº¦: 0.00\n",
      "  å‡¦ç†æ™‚é–“: 0.0ç§’\n",
      "  âŒ ã‚¨ãƒ©ãƒ¼: unhashable type: 'slice'\n",
      "\n",
      "è©•ä¾¡ä¸­: mecano-normand.com\n",
      "  MLç¢ºç‡: 0.070\n",
      "  çµæœ: âœ… æ­£å¸¸\n",
      "  ä¿¡é ¼åº¦: 0.00\n",
      "  å‡¦ç†æ™‚é–“: 0.0ç§’\n",
      "  âŒ ã‚¨ãƒ©ãƒ¼: unhashable type: 'slice'\n",
      "\n",
      "è©•ä¾¡ä¸­: lindastilesfox.com\n",
      "  MLç¢ºç‡: 0.121\n",
      "  çµæœ: âœ… æ­£å¸¸\n",
      "  ä¿¡é ¼åº¦: 0.00\n",
      "  å‡¦ç†æ™‚é–“: 0.0ç§’\n",
      "  âŒ ã‚¨ãƒ©ãƒ¼: unhashable type: 'slice'\n",
      "\n",
      "è©•ä¾¡ä¸­: huwaifeng.cn\n",
      "  MLç¢ºç‡: 0.196\n",
      "  çµæœ: âœ… æ­£å¸¸\n",
      "  ä¿¡é ¼åº¦: 0.00\n",
      "  å‡¦ç†æ™‚é–“: 0.0ç§’\n",
      "  âŒ ã‚¨ãƒ©ãƒ¼: unhashable type: 'slice'\n",
      "\n",
      "è©•ä¾¡ä¸­: lesamisdevesseaux.com\n",
      "  MLç¢ºç‡: 0.142\n",
      "  çµæœ: âœ… æ­£å¸¸\n",
      "  ä¿¡é ¼åº¦: 0.00\n",
      "  å‡¦ç†æ™‚é–“: 0.0ç§’\n",
      "  âŒ ã‚¨ãƒ©ãƒ¼: unhashable type: 'slice'\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼\n",
      "================================================================================\n",
      "\n",
      "çµ±è¨ˆ:\n",
      "  - å‡¦ç†ä»¶æ•°: 5ä»¶\n",
      "  - ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°æ¤œå‡º: 0ä»¶ (0.0%)\n",
      "  - ã‚¨ãƒ©ãƒ¼: 5ä»¶\n",
      "  - ç·å‡¦ç†æ™‚é–“: 2.5ç§’\n",
      "  - å¹³å‡å‡¦ç†æ™‚é–“: 0.5ç§’/ä»¶\n",
      "\n",
      "è©³ç´°çµæœ:\n",
      "  1. winrneteik.icu                           â†’ âœ… æ­£å¸¸ (ä¿¡é ¼åº¦: 0.00)\n",
      "  2. mecano-normand.com                       â†’ âœ… æ­£å¸¸ (ä¿¡é ¼åº¦: 0.00)\n",
      "  3. lindastilesfox.com                       â†’ âœ… æ­£å¸¸ (ä¿¡é ¼åº¦: 0.00)\n",
      "  4. huwaifeng.cn                             â†’ âœ… æ­£å¸¸ (ä¿¡é ¼åº¦: 0.00)\n",
      "  5. lesamisdevesseaux.com                    â†’ âœ… æ­£å¸¸ (ä¿¡é ¼åº¦: 0.00)\n",
      "\n",
      "================================================================================\n",
      "âœ… ãƒ†ã‚¹ãƒˆå®Œäº†ï¼\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
      "----------------------------------------\n",
      "âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸï¼\n",
      "   å¹³å‡å‡¦ç†æ™‚é–“: 0.5ç§’/ä»¶\n",
      "   4,215ä»¶ã®æ¨å®šå‡¦ç†æ™‚é–“: 35.5åˆ†\n",
      "\n",
      "å…¨ä»¶å‡¦ç†ã‚’é–‹å§‹ã™ã‚‹å ´åˆ:\n",
      "1. ä¸¦åˆ—å‡¦ç†ç‰ˆã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰:\n",
      "   - ã‚»ãƒ«07-BATCH: 10ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†\n",
      "   - ã‚»ãƒ«07-PARALLEL: ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ç‰ˆ\n",
      "\n",
      "2. ã¾ãŸã¯ã€ã“ã®ã‚»ãƒ«ã‚’æ”¹é€ ã—ã¦å…¨ä»¶å‡¦ç†:\n",
      "   for idx, row in false_negatives_df.iterrows():\n",
      "       # å‡¦ç†...\n",
      "\n",
      "================================================================================\n",
      "ğŸ–¥ï¸ GPUä½¿ç”¨çŠ¶æ³\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ GPUä½¿ç”¨å ´æ‰€:\n",
      "  - ãƒ­ãƒ¼ã‚«ãƒ«: XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆäºˆæ¸¬æ™‚ï¼‰\n",
      "  - ãƒªãƒ¢ãƒ¼ãƒˆ: vLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆ192.168.100.71ï¼‰\n",
      "    â†’ Qwen3-14Bãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«GPUã‚’ä½¿ç”¨\n",
      "\n",
      "âš ï¸ æ³¨æ„:\n",
      "  - ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯GPUä½¿ç”¨ã¯è¦‹ãˆã¾ã›ã‚“\n",
      "  - vLLMã‚µãƒ¼ãƒãƒ¼å´ã§GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\n",
      "  - ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã§GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„\n",
      "\n",
      "ğŸ’¾ å¤‰æ•°ã‚’ä¿å­˜ã—ã¾ã—ãŸ:\n",
      "  - test_results: çµæœãƒªã‚¹ãƒˆ\n",
      "  - test_results_df: çµæœDataFrame\n",
      "================================================================================\n",
      "ğŸ” å½é™½æ€§å•é¡Œã®èª¿æŸ»: kobe-denki.co.jp\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ DANGEROUS_TLDSã®å†…å®¹ç¢ºèª:\n",
      "----------------------------------------\n",
      "DANGEROUS_TLDS (22å€‹):\n",
      "\n",
      "âš ï¸ æ—¥æœ¬é–¢é€£TLDãŒå±é™ºãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã™:\n",
      "   - .co.vu\n",
      "\n",
      "å±é™ºTLDãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®20å€‹ï¼‰:\n",
      "    1. .ne.pw\n",
      "    2. .COM\n",
      "    3. .gq\n",
      "    4. .wang\n",
      "    5. .co.vu\n",
      "    6. .cOm\n",
      "    7. .cOM\n",
      "    8. .ga\n",
      "    9. .cfd\n",
      "   10. .ci\n",
      "   11. .mw\n",
      "   12. .bar\n",
      "   13. .tk\n",
      "   14. .icu\n",
      "   15. .cn\n",
      "   16. .ml\n",
      "   17. .cyou\n",
      "   18. .buzz\n",
      "   19. .dev\n",
      "   20. .xyz\n",
      "\n",
      "2ï¸âƒ£ HIGH_RISK_WORDSã®å†…å®¹ç¢ºèª:\n",
      "----------------------------------------\n",
      "HIGH_RISK_WORDS (100å€‹):\n",
      "âœ… 'co', 'jp'ãªã©ã¯é«˜ãƒªã‚¹ã‚¯å˜èªã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“\n",
      "\n",
      "é«˜ãƒªã‚¹ã‚¯å˜èªãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®20å€‹ï¼‰:\n",
      "    1. abnamrobanknv\n",
      "    2. absabank\n",
      "    3. accurint\n",
      "    4. adobe\n",
      "    5. aeoncard\n",
      "    6. aetna\n",
      "    7. alibabagroup\n",
      "    8. allegro\n",
      "    9. alliedbanklimited\n",
      "   10. amazon\n",
      "   11. americanexpress\n",
      "   12. aol\n",
      "   13. apple\n",
      "   14. at&t\n",
      "   15. australiantaxationoffice\n",
      "   16. bancobilbaovizcayaargentaria\n",
      "   17. bancodobrasil\n",
      "   18. bancosantandersa\n",
      "   19. bankmillennium\n",
      "   20. bankofamericacorporation\n",
      "\n",
      "3ï¸âƒ£ kobe-denki.co.jpã®å®Ÿéš›ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:\n",
      "----------------------------------------\n",
      "kobe-denki.co.jpã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n",
      "\n",
      "4ï¸âƒ£ å•é¡Œã®åˆ†æ:\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ” ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:\n",
      "\n",
      "âš ï¸ å¯èƒ½æ€§2: ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œ\n",
      "   - 'kobe-denki.co.jp'ãŒ'kobe', 'denki', 'co', 'jp'ã«åˆ†å‰²ã•ã‚Œã‚‹\n",
      "   - 'co'ãŒç‹¬ç«‹ã—ãŸå˜èªã¨ã—ã¦è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹\n",
      "   - è§£æ±ºç­–: TLDéƒ¨åˆ†ï¼ˆ.co.jpï¼‰ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–ã™ã¹ã\n",
      "\n",
      "\n",
      "âš ï¸ å¯èƒ½æ€§3: contextual_risk_assessmentãƒ„ãƒ¼ãƒ«ã®èª¤åˆ¤å®š\n",
      "   - ä½MLç¢ºç‡ï¼ˆ0.144ï¼‰ã‚’ã€Œé€†èª¬çš„ã«å±é™ºã€ã¨è§£é‡ˆ\n",
      "   - .co.jpã‚’å±é™ºã¨èª¤èªè­˜\n",
      "   - è§£æ±ºç­–: æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jp, .ne.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
      "\n",
      "\n",
      "5ï¸âƒ£ ä¿®æ­£ææ¡ˆ:\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“ æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£:\n",
      "\n",
      "1. **HIGH_RISK_WORDSã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**:\n",
      "   ```python\n",
      "   # å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–\n",
      "   problematic = ['co', 'com', 'jp', 'net', 'org']\n",
      "   HIGH_RISK_WORDS = [w for w in HIGH_RISK_WORDS if w not in problematic]\n",
      "   ```\n",
      "\n",
      "2. **æ­£è¦TLDã®å®‰å…¨ãƒªã‚¹ãƒˆä½œæˆ**:\n",
      "   ```python\n",
      "   SAFE_TLDS = [\n",
      "       '.co.jp',  # æ—¥æœ¬ä¼æ¥­\n",
      "       '.ne.jp',  # æ—¥æœ¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
      "       '.or.jp',  # æ—¥æœ¬çµ„ç¹”\n",
      "       '.ac.jp',  # æ—¥æœ¬æ•™è‚²æ©Ÿé–¢\n",
      "       '.go.jp',  # æ—¥æœ¬æ”¿åºœ\n",
      "       '.edu',    # æ•™è‚²æ©Ÿé–¢\n",
      "       '.gov',    # æ”¿åºœæ©Ÿé–¢\n",
      "   ]\n",
      "   ```\n",
      "\n",
      "3. **ãƒ‰ãƒ¡ã‚¤ãƒ³è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„**:\n",
      "   - TLDéƒ¨åˆ†ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–\n",
      "   - æ­£è¦TLDã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’æ¸›ã‚‰ã™\n",
      "   - è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
      "\n",
      "\n",
      "6ï¸âƒ£ ä¿®æ­£ç‰ˆã®å˜èªãƒªã‚¹ãƒˆã‚’ä½œæˆ:\n",
      "----------------------------------------\n",
      "é™¤å¤–ã•ã‚ŒãŸå˜èª: set()\n",
      "ä¿®æ­£å‰: 100å€‹\n",
      "ä¿®æ­£å¾Œ: 100å€‹\n",
      "\n",
      "âœ… HIGH_RISK_WORDS_FIXEDã‚’ä½œæˆã—ã¾ã—ãŸ\n",
      "   ã“ã‚Œã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§å½é™½æ€§ã‚’æ¸›ã‚‰ã›ã¾ã™\n",
      "\n",
      "7ï¸âƒ£ kobe-denki.co.jpã®å†è©•ä¾¡ï¼ˆå¯èƒ½ãªå ´åˆï¼‰:\n",
      "----------------------------------------\n",
      "ãƒ‰ãƒ¡ã‚¤ãƒ³: kobe-denki.co.jp\n",
      "  - å…¨ä½“ã®é•·ã•: 16æ–‡å­—\n",
      "  - ãƒ‘ãƒ¼ãƒ„: ['kobe-denki', 'co', 'jp']\n",
      "  - TLD: .co.jp\n",
      "  - ãƒ‰ãƒ¡ã‚¤ãƒ³åéƒ¨åˆ†: kobe-denki\n",
      "  âœ… .co.jpã¯æ—¥æœ¬ã®æ­£è¦ä¼æ¥­/çµ„ç¹”å‘ã‘TLDã§ã™\n",
      "     â†’ ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ­£å½“ãªå¯èƒ½æ€§ãŒé«˜ã„\n",
      "  - æŠ½å‡ºã•ã‚ŒãŸå˜èª: ['kobe', 'denki']\n",
      "  - 'kobe' = ç¥æˆ¸ï¼ˆæ—¥æœ¬ã®éƒ½å¸‚åï¼‰\n",
      "  - 'denki' = é›»æ©Ÿ/é›»æ°—ï¼ˆæ—¥æœ¬èªï¼‰\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š çµè«–:\n",
      "================================================================================\n",
      "\n",
      "kobe-denki.co.jpãŒèª¤åˆ¤å®šã•ã‚Œã‚‹åŸå› :\n",
      "1. 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ï¼ˆæœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ï¼‰\n",
      "2. .co.jpãŒå±é™ºTLDã¨ã—ã¦èª¤èªè­˜ã•ã‚Œã¦ã„ã‚‹\n",
      "3. æ—¥æœ¬ã®æ­£è¦ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹å¾´ãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„\n",
      "\n",
      "æ¨å¥¨å¯¾å¿œ:\n",
      "- HIGH_RISK_WORDSã‹ã‚‰'co'ã‚’é™¤å¤–\n",
      "- æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†\n",
      "- è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 07-SIMPLE\n",
    "æ¦‚è¦: ã‚·ãƒ³ãƒ—ãƒ«ãªå‹•ä½œç¢ºèªç‰ˆï¼ˆã¾ãš5ä»¶ã§ãƒ†ã‚¹ãƒˆï¼‰\n",
    "å…¥åŠ›: false_negatives_df, evaluate_domain\n",
    "å‡ºåŠ›: ãƒ†ã‚¹ãƒˆçµæœã®è¡¨ç¤ºã¨å‹•ä½œç¢ºèª\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ§ª ã‚·ãƒ³ãƒ—ãƒ«å‹•ä½œãƒ†ã‚¹ãƒˆï¼ˆ5ä»¶ã®ã¿ï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== 1. vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª ==========\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    # vLLMè¨­å®šã®ç¢ºèª\n",
    "    if 'VLLM_CONFIG' in globals():\n",
    "        print(f\"âœ… VLLMè¨­å®š:\")\n",
    "        print(f\"   - URL: {VLLM_CONFIG['base_url']}\")\n",
    "        print(f\"   - Model: {VLLM_CONFIG['model']}\")\n",
    "        \n",
    "        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ\n",
    "        test_llm = ChatOpenAI(**VLLM_CONFIG)\n",
    "        response = test_llm.invoke(\"Say 'OK' if you are working\")\n",
    "        print(f\"âœ… vLLMã‚µãƒ¼ãƒãƒ¼å¿œç­”ç¢ºèª\")\n",
    "        print(f\"   ã‚µãƒ¼ãƒãƒ¼å´ã§GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "    else:\n",
    "        print(\"âŒ VLLM_CONFIG ãŒæœªå®šç¾©\")\n",
    "        print(\"   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ vLLMã‚µãƒ¼ãƒãƒ¼æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "\n",
    "# ========== 2. è©•ä¾¡é–¢æ•°ã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ è©•ä¾¡é–¢æ•°ã®ç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'evaluate_domain' in globals():\n",
    "    print(\"âœ… evaluate_domain é–¢æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "else:\n",
    "    print(\"âŒ evaluate_domain ãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«06ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ==========\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'false_negatives_df' not in globals():\n",
    "    print(\"âŒ false_negatives_df ãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«01ã¾ãŸã¯02ã§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„\")\n",
    "else:\n",
    "    # 5ä»¶ã ã‘å–å¾—\n",
    "    test_samples = false_negatives_df.head(5).copy()\n",
    "    print(f\"âœ… ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«: {len(test_samples)}ä»¶\")\n",
    "    \n",
    "    # MLç¢ºç‡ã‚«ãƒ©ãƒ ã®ç¢ºèª\n",
    "    prob_col = 'prediction_proba' if 'prediction_proba' in test_samples.columns else 'ml_probability'\n",
    "    \n",
    "    print(\"\\nğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‰ãƒ¡ã‚¤ãƒ³:\")\n",
    "    for idx, row in test_samples.iterrows():\n",
    "        print(f\"   {idx+1}. {row['domain'][:50]:50s} (ML: {row[prob_col]:.3f})\")\n",
    "\n",
    "# ========== 4. 1ä»¶ãšã¤è©•ä¾¡ï¼ˆæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ï¼‰ ==========\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ è©•ä¾¡å®Ÿè¡Œï¼ˆ1ä»¶ãšã¤å‡¦ç†ï¼‰:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def simple_evaluate_one(domain, ml_prob):\n",
    "    \"\"\"1ä»¶ã ã‘è©•ä¾¡ã™ã‚‹æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªé–¢æ•°\"\"\"\n",
    "    \n",
    "    print(f\"\\nè©•ä¾¡ä¸­: {domain}\")\n",
    "    print(f\"  MLç¢ºç‡: {ml_prob:.3f}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # evaluate_domain ã‚’å‘¼ã³å‡ºã—\n",
    "        result = evaluate_domain(domain, ml_prob, enable_trace=False)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # çµæœã®è¡¨ç¤º\n",
    "        is_phishing = result.get('ai_is_phishing', False)\n",
    "        confidence = result.get('ai_confidence', 0.0)\n",
    "        \n",
    "        print(f\"  çµæœ: {'ğŸš¨ ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°' if is_phishing else 'âœ… æ­£å¸¸'}\")\n",
    "        print(f\"  ä¿¡é ¼åº¦: {confidence:.2f}\")\n",
    "        print(f\"  å‡¦ç†æ™‚é–“: {elapsed:.1f}ç§’\")\n",
    "        \n",
    "        # è©³ç´°æƒ…å ±\n",
    "        if result.get('detected_brands'):\n",
    "            print(f\"  æ¤œå‡ºãƒ–ãƒ©ãƒ³ãƒ‰: {', '.join(result['detected_brands'])}\")\n",
    "        if result.get('risk_factors'):\n",
    "            print(f\"  ãƒªã‚¹ã‚¯è¦å› : {', '.join(result['risk_factors'][:3])}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return {\n",
    "            'domain': domain,\n",
    "            'ml_probability': ml_prob,\n",
    "            'ai_is_phishing': False,\n",
    "            'ai_confidence': 0.0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# ========== 5. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ==========\n",
    "\n",
    "if 'test_samples' in locals() and 'evaluate_domain' in globals():\n",
    "    print(\"\\nğŸš€ ãƒ†ã‚¹ãƒˆè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    total_start = time.time()\n",
    "    \n",
    "    # 1ä»¶ãšã¤å‡¦ç†\n",
    "    for idx, row in test_samples.iterrows():\n",
    "        domain = row['domain']\n",
    "        ml_prob = row[prob_col]\n",
    "        \n",
    "        result = simple_evaluate_one(domain, ml_prob)\n",
    "        results.append(result)\n",
    "        \n",
    "        # å°‘ã—å¾…æ©Ÿï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    total_elapsed = time.time() - total_start\n",
    "    \n",
    "    # ========== 6. çµæœã‚µãƒãƒªãƒ¼ ==========\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # çµ±è¨ˆ\n",
    "    detected = sum(1 for r in results if r.get('ai_is_phishing', False))\n",
    "    errors = sum(1 for r in results if r.get('error'))\n",
    "    \n",
    "    print(f\"\\nçµ±è¨ˆ:\")\n",
    "    print(f\"  - å‡¦ç†ä»¶æ•°: {len(results)}ä»¶\")\n",
    "    print(f\"  - ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°æ¤œå‡º: {detected}ä»¶ ({detected/len(results)*100:.1f}%)\")\n",
    "    print(f\"  - ã‚¨ãƒ©ãƒ¼: {errors}ä»¶\")\n",
    "    print(f\"  - ç·å‡¦ç†æ™‚é–“: {total_elapsed:.1f}ç§’\")\n",
    "    print(f\"  - å¹³å‡å‡¦ç†æ™‚é–“: {total_elapsed/len(results):.1f}ç§’/ä»¶\")\n",
    "    \n",
    "    # çµæœè©³ç´°\n",
    "    print(f\"\\nè©³ç´°çµæœ:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        domain = test_samples.iloc[i-1]['domain']\n",
    "        is_phishing = \"ğŸš¨ ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°\" if result.get('ai_is_phishing') else \"âœ… æ­£å¸¸\"\n",
    "        confidence = result.get('ai_confidence', 0.0)\n",
    "        print(f\"  {i}. {domain[:40]:40s} â†’ {is_phishing} (ä¿¡é ¼åº¦: {confidence:.2f})\")\n",
    "    \n",
    "    # DataFrameã«å¤‰æ›\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… ãƒ†ã‚¹ãƒˆå®Œäº†ï¼\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâŒ ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã§ãã¾ã›ã‚“\")\n",
    "    print(\"   å¿…è¦ãªå¤‰æ•°ã¾ãŸã¯é–¢æ•°ãŒæœªå®šç¾©ã§ã™\")\n",
    "\n",
    "# ========== 7. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ==========\n",
    "\n",
    "print(\"\\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'results' in locals() and len(results) > 0:\n",
    "    avg_time = total_elapsed / len(results)\n",
    "    estimated_total = 4215 * avg_time / 60\n",
    "    \n",
    "    print(f\"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸï¼\")\n",
    "    print(f\"   å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.1f}ç§’/ä»¶\")\n",
    "    print(f\"   4,215ä»¶ã®æ¨å®šå‡¦ç†æ™‚é–“: {estimated_total:.1f}åˆ†\")\n",
    "    \n",
    "    print(\"\\nå…¨ä»¶å‡¦ç†ã‚’é–‹å§‹ã™ã‚‹å ´åˆ:\")\n",
    "    print(\"1. ä¸¦åˆ—å‡¦ç†ç‰ˆã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰:\")\n",
    "    print(\"   - ã‚»ãƒ«07-BATCH: 10ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†\")\n",
    "    print(\"   - ã‚»ãƒ«07-PARALLEL: ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ç‰ˆ\")\n",
    "    print(\"\")\n",
    "    print(\"2. ã¾ãŸã¯ã€ã“ã®ã‚»ãƒ«ã‚’æ”¹é€ ã—ã¦å…¨ä»¶å‡¦ç†:\")\n",
    "    print(\"   for idx, row in false_negatives_df.iterrows():\")\n",
    "    print(\"       # å‡¦ç†...\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"   ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±ºã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ========== 8. GPUä½¿ç”¨çŠ¶æ³ã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ–¥ï¸ GPUä½¿ç”¨çŠ¶æ³\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ“ GPUä½¿ç”¨å ´æ‰€:\")\n",
    "print(\"  - ãƒ­ãƒ¼ã‚«ãƒ«: XGBoostãƒ¢ãƒ‡ãƒ«ï¼ˆäºˆæ¸¬æ™‚ï¼‰\")\n",
    "print(\"  - ãƒªãƒ¢ãƒ¼ãƒˆ: vLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆ192.168.100.71ï¼‰\")\n",
    "print(\"    â†’ Qwen3-14Bãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«GPUã‚’ä½¿ç”¨\")\n",
    "print(\"\")\n",
    "print(\"âš ï¸ æ³¨æ„:\")\n",
    "print(\"  - ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯GPUä½¿ç”¨ã¯è¦‹ãˆã¾ã›ã‚“\")\n",
    "print(\"  - vLLMã‚µãƒ¼ãƒãƒ¼å´ã§GPUãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "print(\"  - ã‚µãƒ¼ãƒãƒ¼ãƒ­ã‚°ã§GPUä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰\n",
    "if 'results' in locals():\n",
    "    globals()['test_results'] = results\n",
    "    globals()['test_results_df'] = results_df\n",
    "    print(\"\\nğŸ’¾ å¤‰æ•°ã‚’ä¿å­˜ã—ã¾ã—ãŸ:\")\n",
    "    print(\"  - test_results: çµæœãƒªã‚¹ãƒˆ\")\n",
    "    print(\"  - test_results_df: çµæœDataFrame\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-17\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 08-INVESTIGATE\n",
    "æ¦‚è¦: kobe-denki.co.jpã®å½é™½æ€§å•é¡Œã‚’èª¿æŸ»\n",
    "å…¥åŠ›: DANGEROUS_TLDS, HIGH_RISK_WORDS, false_negatives_df\n",
    "å‡ºåŠ›: å•é¡Œã®åŸå› åˆ†æ\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” å½é™½æ€§å•é¡Œã®èª¿æŸ»: kobe-denki.co.jp\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ========== 1. DANGEROUS_TLDSã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ DANGEROUS_TLDSã®å†…å®¹ç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'DANGEROUS_TLDS' in globals():\n",
    "    print(f\"DANGEROUS_TLDS ({len(DANGEROUS_TLDS)}å€‹):\")\n",
    "    # .jpã‚„.co.jpãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    jp_related = [tld for tld in DANGEROUS_TLDS if 'jp' in tld or 'co' in tld]\n",
    "    if jp_related:\n",
    "        print(f\"\\nâš ï¸ æ—¥æœ¬é–¢é€£TLDãŒå±é™ºãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã™:\")\n",
    "        for tld in jp_related:\n",
    "            print(f\"   - {tld}\")\n",
    "    else:\n",
    "        print(\"âœ… .jpã‚„.co.jpã¯å±é™ºTLDãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    \n",
    "    # å®Ÿéš›ã®ãƒªã‚¹ãƒˆå†…å®¹ï¼ˆæœ€åˆã®20å€‹ï¼‰\n",
    "    print(f\"\\nå±é™ºTLDãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®20å€‹ï¼‰:\")\n",
    "    for i, tld in enumerate(DANGEROUS_TLDS[:20], 1):\n",
    "        print(f\"   {i:2d}. {tld}\")\n",
    "else:\n",
    "    print(\"âŒ DANGEROUS_TLDSãŒæœªå®šç¾©ã§ã™\")\n",
    "    print(\"   ã‚»ãƒ«04ã§çµ±è¨ˆçš„ã«ç”Ÿæˆã•ã‚Œã‚‹ã¯ãšã§ã™\")\n",
    "\n",
    "# ========== 2. HIGH_RISK_WORDSã®ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ HIGH_RISK_WORDSã®å†…å®¹ç¢ºèª:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    print(f\"HIGH_RISK_WORDS ({len(HIGH_RISK_WORDS)}å€‹):\")\n",
    "    \n",
    "    # 'co'ã‚„'jp'ãŒå˜èªã¨ã—ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "    problematic_words = [word for word in HIGH_RISK_WORDS if word in ['co', 'jp', 'com']]\n",
    "    if problematic_words:\n",
    "        print(f\"\\nâŒ å•é¡Œã®ã‚ã‚‹å˜èªãŒé«˜ãƒªã‚¹ã‚¯ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã¾ã™:\")\n",
    "        for word in problematic_words:\n",
    "            print(f\"   - '{word}' â† ã“ã‚ŒãŒåŸå› ã®å¯èƒ½æ€§ï¼\")\n",
    "    else:\n",
    "        print(\"âœ… 'co', 'jp'ãªã©ã¯é«˜ãƒªã‚¹ã‚¯å˜èªã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    \n",
    "    # å®Ÿéš›ã®ãƒªã‚¹ãƒˆå†…å®¹ï¼ˆæœ€åˆã®20å€‹ï¼‰\n",
    "    print(f\"\\né«˜ãƒªã‚¹ã‚¯å˜èªãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®20å€‹ï¼‰:\")\n",
    "    for i, word in enumerate(HIGH_RISK_WORDS[:20], 1):\n",
    "        print(f\"   {i:2d}. {word}\")\n",
    "else:\n",
    "    print(\"âŒ HIGH_RISK_WORDSãŒæœªå®šç¾©ã§ã™\")\n",
    "\n",
    "# ========== 3. kobe-denki.co.jpã®å®Ÿéš›ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª ==========\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ kobe-denki.co.jpã®å®Ÿéš›ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'false_negatives_df' in globals():\n",
    "    # ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒæœ¬å½“ã«å½é™°æ€§ï¼ˆãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ï¼‰ãªã®ã‹ç¢ºèª\n",
    "    kobe_rows = false_negatives_df[false_negatives_df['domain'].str.contains('kobe-denki', na=False)]\n",
    "    \n",
    "    if not kobe_rows.empty:\n",
    "        print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®æƒ…å ±:\")\n",
    "        for idx, row in kobe_rows.iterrows():\n",
    "            print(f\"   - ãƒ‰ãƒ¡ã‚¤ãƒ³: {row['domain']}\")\n",
    "            if 'prediction_proba' in row:\n",
    "                print(f\"   - MLäºˆæ¸¬ç¢ºç‡: {row['prediction_proba']:.3f}\")\n",
    "            if 'is_phishing' in row:\n",
    "                print(f\"   - å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«: {'ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°' if row['is_phishing'] else 'æ­£å¸¸'}\")\n",
    "            print(f\"   - ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: å½é™°æ€§ãƒªã‚¹ãƒˆï¼ˆMLãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ï¼‰\")\n",
    "        \n",
    "        print(\"\\nâš ï¸ é‡è¦ãªè¦³å¯Ÿ:\")\n",
    "        print(\"   ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯å½é™°æ€§ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€\")\n",
    "        print(\"   å®Ÿéš›ã«ã¯ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "        print(\"   ãŸã ã—ã€.co.jpãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ—¥æœ¬ä¼æ¥­ã®æ­£è¦ãƒ‰ãƒ¡ã‚¤ãƒ³ãªã®ã§ã€\")\n",
    "        print(\"   ãƒ‡ãƒ¼ã‚¿ãƒ©ãƒ™ãƒ«ãŒé–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚è€ƒæ…®ã™ã¹ãã§ã™ã€‚\")\n",
    "    else:\n",
    "        print(\"kobe-denki.co.jpã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "else:\n",
    "    print(\"false_negatives_dfãŒæœªå®šç¾©ã§ã™\")\n",
    "\n",
    "# ========== 4. å•é¡Œã®åˆ†æ ==========\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ å•é¡Œã®åˆ†æ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\\nğŸ” ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:\")\n",
    "\n",
    "problems = []\n",
    "\n",
    "# å•é¡Œ1: 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦æ‰±ã‚ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§\n",
    "if 'HIGH_RISK_WORDS' in globals() and 'co' in HIGH_RISK_WORDS:\n",
    "    problems.append(\"\"\"\n",
    "âŒ å•é¡Œ1: 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹\n",
    "   - 'co'ã¯.co.jpã‚„.co.ukãªã©ã®æ­£è¦TLDã®ä¸€éƒ¨\n",
    "   - ã“ã‚ŒãŒåŸå› ã§æ­£è¦ã®ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒèª¤åˆ¤å®šã•ã‚Œã‚‹\n",
    "   - è§£æ±ºç­–: HIGH_RISK_WORDSã‹ã‚‰'co'ã‚’é™¤å¤–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹\n",
    "\"\"\")\n",
    "\n",
    "# å•é¡Œ2: å˜èªæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œ\n",
    "problems.append(\"\"\"\n",
    "âš ï¸ å¯èƒ½æ€§2: ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æãƒ­ã‚¸ãƒƒã‚¯ã®å•é¡Œ\n",
    "   - 'kobe-denki.co.jp'ãŒ'kobe', 'denki', 'co', 'jp'ã«åˆ†å‰²ã•ã‚Œã‚‹\n",
    "   - 'co'ãŒç‹¬ç«‹ã—ãŸå˜èªã¨ã—ã¦è©•ä¾¡ã•ã‚Œã¦ã„ã‚‹\n",
    "   - è§£æ±ºç­–: TLDéƒ¨åˆ†ï¼ˆ.co.jpï¼‰ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–ã™ã¹ã\n",
    "\"\"\")\n",
    "\n",
    "# å•é¡Œ3: contextual_risk_assessmentã®èª¤åˆ¤å®š\n",
    "problems.append(\"\"\"\n",
    "âš ï¸ å¯èƒ½æ€§3: contextual_risk_assessmentãƒ„ãƒ¼ãƒ«ã®èª¤åˆ¤å®š\n",
    "   - ä½MLç¢ºç‡ï¼ˆ0.144ï¼‰ã‚’ã€Œé€†èª¬çš„ã«å±é™ºã€ã¨è§£é‡ˆ\n",
    "   - .co.jpã‚’å±é™ºã¨èª¤èªè­˜\n",
    "   - è§£æ±ºç­–: æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jp, .ne.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "\"\"\")\n",
    "\n",
    "for problem in problems:\n",
    "    print(problem)\n",
    "\n",
    "# ========== 5. ä¿®æ­£ææ¡ˆ ==========\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ ä¿®æ­£ææ¡ˆ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“ æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£:\n",
    "\n",
    "1. **HIGH_RISK_WORDSã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**:\n",
    "   ```python\n",
    "   # å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–\n",
    "   problematic = ['co', 'com', 'jp', 'net', 'org']\n",
    "   HIGH_RISK_WORDS = [w for w in HIGH_RISK_WORDS if w not in problematic]\n",
    "   ```\n",
    "\n",
    "2. **æ­£è¦TLDã®å®‰å…¨ãƒªã‚¹ãƒˆä½œæˆ**:\n",
    "   ```python\n",
    "   SAFE_TLDS = [\n",
    "       '.co.jp',  # æ—¥æœ¬ä¼æ¥­\n",
    "       '.ne.jp',  # æ—¥æœ¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯\n",
    "       '.or.jp',  # æ—¥æœ¬çµ„ç¹”\n",
    "       '.ac.jp',  # æ—¥æœ¬æ•™è‚²æ©Ÿé–¢\n",
    "       '.go.jp',  # æ—¥æœ¬æ”¿åºœ\n",
    "       '.edu',    # æ•™è‚²æ©Ÿé–¢\n",
    "       '.gov',    # æ”¿åºœæ©Ÿé–¢\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **ãƒ‰ãƒ¡ã‚¤ãƒ³è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„**:\n",
    "   - TLDéƒ¨åˆ†ã‚’å˜èªæŠ½å‡ºã‹ã‚‰é™¤å¤–\n",
    "   - æ­£è¦TLDã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’æ¸›ã‚‰ã™\n",
    "   - è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
    "\"\"\")\n",
    "\n",
    "# ========== 6. ãƒ†ã‚¹ãƒˆ: ä¿®æ­£ç‰ˆã®å˜èªãƒªã‚¹ãƒˆ ==========\n",
    "\n",
    "print(\"\\n6ï¸âƒ£ ä¿®æ­£ç‰ˆã®å˜èªãƒªã‚¹ãƒˆã‚’ä½œæˆ:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'HIGH_RISK_WORDS' in globals():\n",
    "    # å•é¡Œã®ã‚ã‚‹å˜èªã‚’é™¤å¤–\n",
    "    problematic = ['co', 'com', 'jp', 'net', 'org', 'www']\n",
    "    HIGH_RISK_WORDS_FIXED = [w for w in HIGH_RISK_WORDS if w not in problematic]\n",
    "    \n",
    "    removed = set(HIGH_RISK_WORDS) - set(HIGH_RISK_WORDS_FIXED)\n",
    "    print(f\"é™¤å¤–ã•ã‚ŒãŸå˜èª: {removed}\")\n",
    "    print(f\"ä¿®æ­£å‰: {len(HIGH_RISK_WORDS)}å€‹\")\n",
    "    print(f\"ä¿®æ­£å¾Œ: {len(HIGH_RISK_WORDS_FIXED)}å€‹\")\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "    globals()['HIGH_RISK_WORDS_FIXED'] = HIGH_RISK_WORDS_FIXED\n",
    "    \n",
    "    print(\"\\nâœ… HIGH_RISK_WORDS_FIXEDã‚’ä½œæˆã—ã¾ã—ãŸ\")\n",
    "    print(\"   ã“ã‚Œã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§å½é™½æ€§ã‚’æ¸›ã‚‰ã›ã¾ã™\")\n",
    "\n",
    "# ========== 7. å†è©•ä¾¡ãƒ†ã‚¹ãƒˆ ==========\n",
    "\n",
    "print(\"\\n7ï¸âƒ£ kobe-denki.co.jpã®å†è©•ä¾¡ï¼ˆå¯èƒ½ãªå ´åˆï¼‰:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def analyze_domain_parts(domain):\n",
    "    \"\"\"ãƒ‰ãƒ¡ã‚¤ãƒ³ã®æ§‹æˆè¦ç´ ã‚’åˆ†æ\"\"\"\n",
    "    parts = domain.split('.')\n",
    "    \n",
    "    print(f\"ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}\")\n",
    "    print(f\"  - å…¨ä½“ã®é•·ã•: {len(domain)}æ–‡å­—\")\n",
    "    print(f\"  - ãƒ‘ãƒ¼ãƒ„: {parts}\")\n",
    "    \n",
    "    # TLDåˆ¤å®š\n",
    "    if len(parts) >= 2:\n",
    "        if len(parts) >= 3 and parts[-2] in ['co', 'ac', 'or', 'ne', 'go']:\n",
    "            tld = f\".{parts[-2]}.{parts[-1]}\"\n",
    "            domain_without_tld = '.'.join(parts[:-2])\n",
    "        else:\n",
    "            tld = f\".{parts[-1]}\"\n",
    "            domain_without_tld = '.'.join(parts[:-1])\n",
    "        \n",
    "        print(f\"  - TLD: {tld}\")\n",
    "        print(f\"  - ãƒ‰ãƒ¡ã‚¤ãƒ³åéƒ¨åˆ†: {domain_without_tld}\")\n",
    "        \n",
    "        # æ—¥æœ¬ã®æ­£è¦TLDã‹ãƒã‚§ãƒƒã‚¯\n",
    "        jp_official_tlds = ['.co.jp', '.ne.jp', '.or.jp', '.ac.jp', '.go.jp']\n",
    "        if tld in jp_official_tlds:\n",
    "            print(f\"  âœ… {tld}ã¯æ—¥æœ¬ã®æ­£è¦ä¼æ¥­/çµ„ç¹”å‘ã‘TLDã§ã™\")\n",
    "            print(f\"     â†’ ã“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æ­£å½“ãªå¯èƒ½æ€§ãŒé«˜ã„\")\n",
    "        \n",
    "        # å˜èªæŠ½å‡ºï¼ˆãƒã‚¤ãƒ•ãƒ³ã§åˆ†å‰²ï¼‰\n",
    "        words = domain_without_tld.replace('-', ' ').replace('_', ' ').split()\n",
    "        print(f\"  - æŠ½å‡ºã•ã‚ŒãŸå˜èª: {words}\")\n",
    "        \n",
    "        # 'kobe'ã¯ç¥æˆ¸ã€'denki'ã¯é›»æ©Ÿ/é›»æ°—\n",
    "        if 'kobe' in words:\n",
    "            print(\"  - 'kobe' = ç¥æˆ¸ï¼ˆæ—¥æœ¬ã®éƒ½å¸‚åï¼‰\")\n",
    "        if 'denki' in words:\n",
    "            print(\"  - 'denki' = é›»æ©Ÿ/é›»æ°—ï¼ˆæ—¥æœ¬èªï¼‰\")\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "analyze_domain_parts('kobe-denki.co.jp')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š çµè«–:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "kobe-denki.co.jpãŒèª¤åˆ¤å®šã•ã‚Œã‚‹åŸå› :\n",
    "1. 'co'ãŒé«˜ãƒªã‚¹ã‚¯å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ï¼ˆæœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ï¼‰\n",
    "2. .co.jpãŒå±é™ºTLDã¨ã—ã¦èª¤èªè­˜ã•ã‚Œã¦ã„ã‚‹\n",
    "3. æ—¥æœ¬ã®æ­£è¦ä¼æ¥­ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ç‰¹å¾´ãŒè€ƒæ…®ã•ã‚Œã¦ã„ãªã„\n",
    "\n",
    "æ¨å¥¨å¯¾å¿œ:\n",
    "- HIGH_RISK_WORDSã‹ã‚‰'co'ã‚’é™¤å¤–\n",
    "- æ—¥æœ¬ã®æ­£è¦TLDï¼ˆ.co.jpç­‰ï¼‰ã‚’å®‰å…¨ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†\n",
    "- è¨¼æ˜æ›¸ã«çµ„ç¹”æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca5ac68f",
   "metadata": {
    "title": "Cell 10: LLMè¨­å®šï¼ˆConfigå®Œå…¨å¯¾å¿œãƒ»ä¸¦åˆ—å¯¾å¿œï¼‰ REPLACE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ¤– LLM setup from cfg: provider=vllm, model=Qwen/Qwen3-14B-FP8, session=phishing_agent_1760861696\n",
      "======================================================================\n",
      "âœ… LLM sync connection OK\n",
      "âœ… LLM async connection OK\n",
      "\n",
      "ğŸ“Š LLM CONFIG: {'temperature': 0.1, 'max_tokens': 2048, 'timeout': 60, 'model': 'Qwen/Qwen3-14B-FP8', 'max_concurrent': 10}\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: LLMè¨­å®šï¼ˆConfigå®Œå…¨å¯¾å¿œãƒ»ä¸¦åˆ—å¯¾å¿œï¼‰ REPLACE ===\n",
    "import os, time, asyncio\n",
    "from typing import Optional\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "\n",
    "try:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "except Exception:\n",
    "    print(\"âš ï¸ nest_asyncio not available; async tests may be limited\")\n",
    "\n",
    "prov = str(cfg[\"llm\"][\"provider\"]).lower()\n",
    "base_vllm   = cfg[\"llm\"][\"vllm_base_url\"]\n",
    "base_ollama = cfg[\"llm\"][\"ollama_base_url\"]\n",
    "model       = cfg[\"llm\"][\"model\"]\n",
    "temperature = float(cfg[\"llm\"][\"temperature\"])\n",
    "max_tokens  = int(cfg[\"llm\"][\"max_tokens\"])\n",
    "timeout_s   = int(cfg[\"llm\"][\"timeout\"])\n",
    "max_conc    = int(cfg[\"llm\"][\"max_concurrent\"])\n",
    "\n",
    "SESSION_ID = f\"phishing_agent_{int(time.time())}\"\n",
    "print(\"=\"*70)\n",
    "print(f\"ğŸ¤– LLM setup from cfg: provider={prov}, model={model}, session={SESSION_ID}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "client: Optional[OpenAI] = None\n",
    "async_client: Optional[AsyncOpenAI] = None\n",
    "DEFAULT_MODEL = model\n",
    "LLM_TYPE = prov\n",
    "\n",
    "if prov == \"vllm\":\n",
    "    client = OpenAI(base_url=base_vllm, api_key=\"EMPTY\")\n",
    "    async_client = AsyncOpenAI(base_url=base_vllm, api_key=\"EMPTY\")\n",
    "elif prov == \"ollama\":\n",
    "    client = OpenAI(base_url=base_ollama, api_key=\"ollama\")\n",
    "    async_client = None\n",
    "else:\n",
    "    raise ValueError(f\"Unknown LLM provider: {prov}\")\n",
    "\n",
    "try:\n",
    "    r = client.chat.completions.create(\n",
    "        model=DEFAULT_MODEL,\n",
    "        messages=[{\"role\":\"user\",\"content\":\"Hello\"}],\n",
    "        max_tokens=8,\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"âœ… LLM sync connection OK\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ LLM sync connection failed: {e}\")\n",
    "    raise\n",
    "\n",
    "if prov == \"vllm\" and async_client:\n",
    "    async def _ping():\n",
    "        rr = await async_client.chat.completions.create(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=[{\"role\":\"user\",\"content\":\"Hello async\"}],\n",
    "            max_tokens=8,\n",
    "            temperature=0\n",
    "        )\n",
    "        return rr.choices[0].message.content[:50]\n",
    "    try:\n",
    "        asyncio.run(_ping())\n",
    "        print(\"âœ… LLM async connection OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ LLM async connection test failed: {e}\")\n",
    "\n",
    "LLM_CONFIG = {\n",
    "    \"temperature\": temperature,\n",
    "    \"max_tokens\": max_tokens,\n",
    "    \"timeout\": timeout_s,\n",
    "    \"model\": DEFAULT_MODEL,\n",
    "    \"max_concurrent\": max_conc if prov == \"vllm\" else 1\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“Š LLM CONFIG:\", LLM_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78ed57f2",
   "metadata": {
    "title": "Cell 12B: Config gates wrappers (NEW)"
   },
   "outputs": [],
   "source": [
    "# === Cell 12B: Config gates wrappers (NEW) ===\n",
    "# æ—¢å­˜ã®é–¢æ•°ã‚’ alias ã—ã¦ãƒ©ãƒƒãƒ—ã—ã€Configã§æœ‰åŠ¹/ç„¡åŠ¹ã‚’åˆ‡æ›¿ãˆã‚‹ã€‚\n",
    "# Decorator(@tool)ã¯ãƒ©ãƒƒãƒ‘ãƒ¼ã«ä»˜ä¸ã—ã€å…ƒé–¢æ•°ã¯ä¿æŒã™ã‚‹ã€‚\n",
    "\n",
    "# extract_features_improved\n",
    "try:\n",
    "    extract_features_improved_core = extract_features_improved\n",
    "except Exception:\n",
    "    extract_features_improved_core = None\n",
    "\n",
    "def extract_features_improved(domain: str) -> dict:\n",
    "    # gateã«ã‚ˆã‚Šç°¡æ˜“å‡¦ç† or æœ¬ä½“å‘¼ã³å‡ºã—\n",
    "    d = (domain or \"\").strip().lower()\n",
    "    res = {'clean_words': [], 'risk_patterns': [], 'risk_scores': {}, 'tld_info': {}, 'structure': {}}\n",
    "\n",
    "    # TLD\n",
    "    if cfg[\"analysis\"][\"enable_tld_analysis\"]:\n",
    "        if extract_features_improved_core:\n",
    "            try:\n",
    "                # å…ˆã«æœ¬ä½“ã‚’å‘¼ã‚“ã§ãŠã\n",
    "                core_res = extract_features_improved_core(domain)\n",
    "                res.update({k: core_res.get(k,res.get(k)) for k in res.keys()})\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ extract_features_improved_core error: {e}\")\n",
    "                parts = d.split('.')\n",
    "                res['tld_info'] = {'tld': parts[-1] if len(parts)>=2 else '', 'domain_body': '.'.join(parts[:-1]) if len(parts)>=2 else d}\n",
    "        else:\n",
    "            parts = d.split('.')\n",
    "            res['tld_info'] = {'tld': parts[-1] if len(parts)>=2 else '', 'domain_body': '.'.join(parts[:-1]) if len(parts)>=2 else d}\n",
    "    else:\n",
    "        print(\"âš ï¸ TLDåˆ†æã¯ç„¡åŠ¹ï¼ˆcfg.analysis.enable_tld_analysis=Falseï¼‰â†’ ç°¡æ˜“æŠ½å‡º\")\n",
    "        parts = d.split('.')\n",
    "        res['tld_info'] = {'tld': parts[-1] if len(parts)>=2 else '', 'domain_body': '.'.join(parts[:-1]) if len(parts)>=2 else d}\n",
    "\n",
    "    # domain structure\n",
    "    if cfg[\"analysis\"][\"enable_domain_analysis\"]:\n",
    "        res['structure']['depth'] = d.count('.') + 1\n",
    "        res['structure']['length'] = len(d)\n",
    "        res['structure']['hyphen_count'] = d.count('-')\n",
    "    else:\n",
    "        print(\"âš ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³æ§‹é€ åˆ†æã¯ç„¡åŠ¹ï¼ˆcfg.analysis.enable_domain_analysis=Falseï¼‰\")\n",
    "\n",
    "    # brand detection is mostly handled in brand_impersonation_check tool; here we just record flag\n",
    "    if not cfg[\"analysis\"][\"enable_brand_detection\"]:\n",
    "        res['risk_scores']['brand_detection'] = 0.0\n",
    "\n",
    "    return res\n",
    "\n",
    "# Tools\n",
    "try:\n",
    "    original_brand_impersonation_check = brand_impersonation_check\n",
    "except Exception:\n",
    "    original_brand_impersonation_check = None\n",
    "\n",
    "try:\n",
    "    original_certificate_analysis = certificate_analysis\n",
    "except Exception:\n",
    "    original_certificate_analysis = None\n",
    "\n",
    "try:\n",
    "    original_short_domain_analysis = short_domain_analysis\n",
    "except Exception:\n",
    "    original_short_domain_analysis = None\n",
    "\n",
    "try:\n",
    "    original_contextual_risk_assessment = contextual_risk_assessment\n",
    "except Exception:\n",
    "    original_contextual_risk_assessment = None\n",
    "\n",
    "# NOTE: @tool ãŒæ—¢ã« import æ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ã‚’æƒ³å®š\n",
    "try:\n",
    "    tool\n",
    "except NameError:\n",
    "    def tool(func):\n",
    "        return func\n",
    "\n",
    "@tool\n",
    "def brand_impersonation_check(domain: str, ml_probability: float) -> dict:\n",
    "    \"\"\"ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒæ—¢çŸ¥ãƒ–ãƒ©ãƒ³ãƒ‰ã®ãªã‚Šã™ã¾ã—ã‹ã‚’åˆ¤å®šã—ã€ç†ç”±ã¨ä¿¡é ¼åº¦ã‚’è¿”ã™ã€‚\"\"\"\n",
    "    if not cfg[\"analysis\"][\"enable_brand_detection\"]:\n",
    "        return {\n",
    "            'is_impersonation': False,\n",
    "            'confidence': 0.0,\n",
    "            'detected_brands': [],\n",
    "            'reasons': [\"brand detection disabled by config\"],\n",
    "            'risk_factors': {}\n",
    "        }\n",
    "    if original_brand_impersonation_check:\n",
    "        return original_brand_impersonation_check(domain, ml_probability)\n",
    "    return {\n",
    "        'is_impersonation': False,\n",
    "        'confidence': 0.0,\n",
    "        'detected_brands': [],\n",
    "        'reasons': [\"brand tool missing\"],\n",
    "        'risk_factors': {}\n",
    "    }\n",
    "\n",
    "@tool\n",
    "def certificate_analysis(domain: str, ml_probability: float) -> dict:\n",
    "    \"\"\"TLS/è¨¼æ˜æ›¸ã®ã‚·ã‚°ãƒŠãƒ«ã‚’åˆ†æã—ã¦ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚‰ã—ã•ã‚’è¿”ã™ã€‚\"\"\"\n",
    "    if original_certificate_analysis:\n",
    "        return original_certificate_analysis(domain, ml_probability)\n",
    "    return {'valid': True, 'reasons': [\"certificate tool missing\"]}\n",
    "\n",
    "@tool\n",
    "def short_domain_analysis(domain: str, ml_probability: float) -> dict:\n",
    "    \"\"\"ãƒ‰ãƒ¡ã‚¤ãƒ³é•·ãƒ»ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³æ·±ã•ãªã©é•·ã•ç³»ã®ç‰¹å¾´ã§ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚‰ã—ã•ã‚’è©•ä¾¡ã™ã‚‹ã€‚\"\"\"\n",
    "    if not cfg[\"analysis\"][\"enable_domain_analysis\"]:\n",
    "        parts = domain.lower().split('.')\n",
    "        dl = len('.'.join(parts[:-1])) if len(parts)>1 else len(domain)\n",
    "        is_short = dl < 10\n",
    "        return {\n",
    "            'is_suspicious': is_short and (ml_probability < 0.3),\n",
    "            'confidence': 0.65 if (is_short and ml_probability < 0.3) else 0.0,\n",
    "            'reasons': [\"short-domain simplified (domain analysis disabled)\"] if is_short else [],\n",
    "            'risk_factors': {'short_domain': is_short},\n",
    "            'entropy': 0.0,\n",
    "            'domain_length': dl\n",
    "        }\n",
    "    if original_short_domain_analysis:\n",
    "        return original_short_domain_analysis(domain, ml_probability)\n",
    "    return {'is_suspicious': False, 'confidence': 0.0, 'reasons': [\"short tool missing\"]}\n",
    "\n",
    "@tool\n",
    "def contextual_risk_assessment(domain: str, ml_probability: float,\n",
    "                               brand_result: dict = None,\n",
    "                               cert_result: dict = None,\n",
    "                               domain_result: dict = None) -> dict:\n",
    "    \"\"\"å€‹åˆ¥ãƒ„ãƒ¼ãƒ«ã®å‡ºåŠ›ï¼ˆãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»è¨¼æ˜æ›¸ãƒ»é•·ã•ãƒ»æ§‹é€ ãƒ»TLDç­‰ï¼‰ã‚’çµ±åˆã—ã€æœ€çµ‚åˆ¤å®šã¨èª¬æ˜ã‚’è¿”ã™ã€‚\"\"\"\n",
    "    if original_contextual_risk_assessment:\n",
    "        return original_contextual_risk_assessment(domain, ml_probability, brand_result, cert_result, domain_result)\n",
    "    return {'risk': 'low', 'reasons': [\"context tool missing\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20213f2a",
   "metadata": {
    "title": "Cell 25: Controller API é–¢æ•° (NEW)"
   },
   "outputs": [],
   "source": [
    "# === Cell 25: Controller API é–¢æ•° (NEW) ===\n",
    "from typing import Tuple, Dict, Any\n",
    "import os, json, time, traceback\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def _safe_get(d: dict, k: str):\n",
    "    return d[k] if isinstance(d, dict) and k in d else None\n",
    "\n",
    "def generalization_study(session_id: str, cfg_in: Dict[str, Any]) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"ReturnCode: \"OK\" | \"NOT_FOUND\" | \"INVALID_INPUT\" | \"ERROR\"\"\"\n",
    "    try:\n",
    "        cfg_local = load_configuration(cfg_override=cfg_in or {})\n",
    "        dev_mode = bool(cfg_local[\"system\"][\"development_mode\"])\n",
    "        sample_size = int(cfg_local[\"analysis\"][\"sample_size\"]) if dev_mode else None\n",
    "\n",
    "        handoff_path = None\n",
    "        candidates = [\n",
    "            os.path.join(globals().get(\"HANDOFF_DIR\",\"handoff\"), \"03_ai_agent_analysis_part3.pkl\"),\n",
    "            \"handoff/03_ai_agent_analysis_part3.pkl\"\n",
    "        ]\n",
    "        for p in candidates:\n",
    "            if os.path.exists(p):\n",
    "                handoff_path = p; break\n",
    "        if not handoff_path:\n",
    "            return \"NOT_FOUND\", {\"error\": \"handoff(part3) not found\"}\n",
    "\n",
    "        handoff = joblib.load(handoff_path)\n",
    "\n",
    "        fn_df  = _safe_get(handoff, \"false_negatives_df\")\n",
    "        brands = _safe_get(handoff, \"brand_keywords\")\n",
    "        tlds_d = _safe_get(handoff, \"DANGEROUS_TLDS\")\n",
    "        certs  = _safe_get(handoff, \"cert_full_info_map\")\n",
    "\n",
    "        missing = []\n",
    "        if fn_df is None or not hasattr(fn_df, \"shape\"): missing.append(\"false_negatives_df\")\n",
    "        if not brands or len(brands) < 64: missing.append(\"brand_keywords(>=64)\")\n",
    "        if not tlds_d: missing.append(\"DANGEROUS_TLDS\")\n",
    "        if certs is None: missing.append(\"cert_full_info_map\")\n",
    "        if missing:\n",
    "            return \"NOT_FOUND\", {\"error\": f\"handoff keys missing: {missing}\"}\n",
    "\n",
    "        target_df = fn_df.copy()\n",
    "        if dev_mode:\n",
    "            n = min(sample_size or 50, len(target_df))\n",
    "            if \"prediction_proba\" in target_df.columns:\n",
    "                low = target_df[target_df[\"prediction_proba\"] < 0.2]\n",
    "                rest = target_df[target_df[\"prediction_proba\"] >= 0.2]\n",
    "                take_low = min(int(n*0.6), len(low))\n",
    "                take_rest = n - take_low\n",
    "                frames = []\n",
    "                if take_low>0 and len(low)>0:\n",
    "                    frames.append(low.sample(n=take_low, random_state=42))\n",
    "                if take_rest>0 and len(rest)>0:\n",
    "                    frames.append(rest.sample(n=take_rest, random_state=42))\n",
    "                target_df = pd.concat(frames, ignore_index=True) if frames else target_df.sample(n=n, random_state=42)\n",
    "            else:\n",
    "                target_df = target_df.sample(n=n, random_state=42)\n",
    "\n",
    "        if \"domain\" not in target_df.columns:\n",
    "            return \"INVALID_INPUT\", {\"error\": \"false_negatives_df has no 'domain' column\"}\n",
    "        domains = target_df[\"domain\"].astype(str).tolist()\n",
    "        probs = (target_df[\"prediction_proba\"].astype(float).tolist()\n",
    "                 if \"prediction_proba\" in target_df.columns else [0.5]*len(domains))\n",
    "\n",
    "        start = time.time()\n",
    "        try:\n",
    "            if \"process_domains_batch\" in globals():\n",
    "                import asyncio\n",
    "                async def _run():\n",
    "                    return await process_domains_batch(list(zip(domains, probs)),\n",
    "                                                       max_concurrent=cfg_local[\"llm\"][\"max_concurrent\"])\n",
    "                loop = asyncio.new_event_loop()\n",
    "                asyncio.set_event_loop(loop)\n",
    "                results = loop.run_until_complete(_run())\n",
    "                loop.close()\n",
    "            else:\n",
    "                results = []\n",
    "                for d, p in zip(domains, probs):\n",
    "                    results.append(evaluate_domain(d, p))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ LLM error: {e} -> deterministic fallback\")\n",
    "            results = []\n",
    "            for d, p in zip(domains, probs):\n",
    "                is_phish = (p < 0.2)\n",
    "                results.append({\n",
    "                    'domain': d, 'ml_probability': p, 'ai_is_phishing': is_phish,\n",
    "                    'ai_confidence': 0.6 if is_phish else 0.3, 'success': True,\n",
    "                    'ai_risk_level': 'high' if is_phish else 'low', 'deterministic': True\n",
    "                })\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        detected = sum(1 for r in results if isinstance(r, dict) and r.get(\"ai_is_phishing\"))\n",
    "        pr_summary = {\n",
    "            \"evaluated\": len(results),\n",
    "            \"ai_detected\": detected,\n",
    "            \"elapsed_sec\": elapsed,\n",
    "        }\n",
    "        threshold_rec = {\"recommended_threshold\": 0.5, \"rationale\": \"placeholder (optimize in report)\"}\n",
    "\n",
    "        run_tag = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_results = Path(cfg_local[\"paths\"][\"results_dir\"]) / str(session_id) / \"generalization\"\n",
    "        base_logs    = Path(cfg_local[\"paths\"][\"logs_dir\"]) / \"generalization\"\n",
    "        base_results.mkdir(parents=True, exist_ok=True)\n",
    "        base_logs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        paths = {\n",
    "            \"generalization_results\": str(base_results / f\"generalization_results_{run_tag}.pkl\"),\n",
    "            \"pr_analysis\":            str(base_results / f\"pr_analysis_{run_tag}.json\"),\n",
    "            \"threshold_recommendation\": str(base_results / f\"threshold_recommendation_{run_tag}.json\"),\n",
    "            \"comparison_report\":      str(base_results / f\"comparison_report_{run_tag}.html\"),\n",
    "            \"evaluation_csv\":         str(base_results / f\"evaluation_results_{run_tag}.csv\"),\n",
    "            \"logs\":                   str(base_logs    / f\"generalization_study_{run_tag}.log\"),\n",
    "        }\n",
    "\n",
    "        import joblib as _joblib\n",
    "        _joblib.dump({\"results\": results, \"config\": cfg_local}, paths[\"generalization_results\"])\n",
    "        Path(paths[\"pr_analysis\"]).write_text(json.dumps(pr_summary, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "        Path(paths[\"threshold_recommendation\"]).write_text(json.dumps(threshold_rec, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "        html = f\"\"\"\n",
    "        <html><body>\n",
    "        <h1>Generalization Study</h1>\n",
    "        <p>session: {session_id}</p>\n",
    "        <ul>\n",
    "          <li>evaluated: {pr_summary['evaluated']}</li>\n",
    "          <li>ai_detected: {pr_summary['ai_detected']}</li>\n",
    "          <li>elapsed_sec: {pr_summary['elapsed_sec']:.2f}</li>\n",
    "          <li>threshold recommendation: {threshold_rec['recommended_threshold']}</li>\n",
    "        </ul>\n",
    "        </body></html>\n",
    "        \"\"\"\n",
    "        Path(paths[\"comparison_report\"]).write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "        try:\n",
    "            pd.DataFrame(results).to_csv(paths[\"evaluation_csv\"], index=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return \"OK\", paths\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        return \"NOT_FOUND\", {\"error\": str(e)}\n",
    "    except ValueError as e:\n",
    "        return \"INVALID_INPUT\", {\"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        import traceback as _tb\n",
    "        return \"ERROR\", {\"error\": f\"{e}\", \"trace\": _tb.format_exc()[:2000]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32cbce-b184-48b6-8a7d-62a365d69386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
