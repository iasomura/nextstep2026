{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be806b01",
   "metadata": {},
   "source": [
    "# 03_ai_agent_analysis_part2 â€” å®Œå…¨ç‰ˆï¼ˆConfig + ãƒãƒ¼ãƒ‰ãƒ•ã‚§ã‚¤ãƒ«å¯¾å¿œï¼‰\n",
    "- æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯**å…ƒãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å…¨ã‚³ãƒ¼ãƒ‰**ã‚’ã‚»ãƒ«æ¯ã«åéŒ²ã—ã€ã‚»ãƒ«5ã«Configå°å…¥ï¼†ãƒãƒ¼ãƒ‰ãƒ•ã‚§ã‚¤ãƒ«åˆ‡æ›¿ã‚’å®Ÿè£…ã€ã‚»ãƒ«7ã«APIé–¢æ•°ã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚\n",
    "#- `cfg['db']['require_connection']=True` ã§ **DBæœªæ¥ç¶šæ™‚ã«ä¾‹å¤–ã§åœæ­¢**ã€Falseï¼ˆæ—¢å®šï¼‰ãªã‚‰**ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³**ã§ç¶™ç¶šã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe411c10-45ba-4c0c-b40d-72a88de39208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NX] RUN_ID = 2026-01-10_140940 | paths.RUN_ID = 2026-01-10_140940\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0 (02ä»¥é™ å…±é€š): ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰è§£æ±ºã—ã¦ paths ã‚’èª­ã‚€ ===\n",
    "import run_id_registry as runreg\n",
    "rid = runreg.bootstrap()  # envâ†’ãƒ•ã‚¡ã‚¤ãƒ«(artifacts/_current/run_id.txt)â†’Part3â†’latestâ†’æ–°è¦ ã®é †ã§è§£æ±º\n",
    "\n",
    "import importlib\n",
    "import _compat.paths as paths\n",
    "importlib.reload(paths)\n",
    "importlib.reload(paths)\n",
    "print(\"[NX] RUN_ID =\", rid, \"| paths.RUN_ID =\", paths.RUN_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33ceba8-bd16-4330-9ef5-f2fff9c12474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… IO guard ready -> artifacts/2026-01-10_140940\n"
     ]
    }
   ],
   "source": [
    "# === ã‚»ãƒ«1: å…¨ä½“ã‚³ãƒ¼ãƒ‰ ===\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# === IO PATHS (auto-added guard) ===\n",
    "# ã“ã®ã‚»ãƒ«ã¯è‡ªå‹•è¿½åŠ /è£œå¼·ç”¨ã§ã™ã€‚I/O ä»¥å¤–ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯ä¸€åˆ‡å¤‰æ›´ã—ã¾ã›ã‚“ã€‚\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "if 'RUN_ID' not in globals():\n",
    "    RUN_ID = os.environ.get(\"RUN_ID\") or datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "ARTIFACTS = Path(\"artifacts\") / RUN_ID\n",
    "\n",
    "RAW = ARTIFACTS / \"raw\"\n",
    "PROCESSED = ARTIFACTS / \"processed\"\n",
    "MODELS = ARTIFACTS / \"models\"\n",
    "RESULTS = ARTIFACTS / \"results\"\n",
    "HANDOFF = ARTIFACTS / \"handoff\"\n",
    "LOGS = ARTIFACTS / \"logs\"\n",
    "TRACES = ARTIFACTS / \"traces\"\n",
    "\n",
    "for _p in [RAW, PROCESSED, MODELS, RESULTS, HANDOFF, LOGS, TRACES]:\n",
    "    _p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# String shortcuts for os.path.join äº’æ›\n",
    "RAW_DIR = str(RAW); PROCESSED_DIR = str(PROCESSED); MODELS_DIR = str(MODELS)\n",
    "RESULTS_DIR = str(RESULTS); HANDOFF_DIR = str(HANDOFF); LOGS_DIR = str(LOGS); TRACES_DIR = str(TRACES)\n",
    "\n",
    "base_dirs = {\n",
    "    'raw': RAW_DIR,\n",
    "    'data': PROCESSED_DIR,\n",
    "    'models': MODELS_DIR,\n",
    "    'results': RESULTS_DIR,\n",
    "    'handoff': HANDOFF_DIR,\n",
    "    'logs': LOGS_DIR,\n",
    "    'traces': TRACES_DIR,\n",
    "}\n",
    "def resolve(p):\n",
    "    p = Path(p); p.mkdir(parents=True, exist_ok=True); return str(p)\n",
    "def ensure_roots(): pass\n",
    "def load_config(): return {\"root\": str(ARTIFACTS), \"run_id\": RUN_ID}\n",
    "\n",
    "print(f\"âœ… IO guard ready -> artifacts/{RUN_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad1b2f8-f3e2-4a05-b930-140073f97dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… handoff loaded: artifacts/2026-01-10_140940/handoff/03_ai_agent_analysis_part1.pkl\n"
     ]
    }
   ],
   "source": [
    "# ## Part 2 â€” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€cert_full_info_mapã‚’ä½œæˆ\n",
    "# \n",
    "# â€»æœ¬ãƒ‘ãƒ¼ãƒˆã¯åŸæ–‡ã®ã‚»ãƒ«ã‚’**ä¸€å­—ä¸€å¥å¤‰æ›´ã›ãš**åéŒ²ã—ã¦ã„ã¾ã™ã€‚è¿½åŠ ã¯ã“ã®è¦‹å‡ºã—ã¨å…¥å‡ºåŠ›ã‚¹ãƒ­ãƒƒãƒˆã®ã¿ã§ã™ã€‚\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# === robust handoff loader (auto-added) ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "os.makedirs(HANDOFF_DIR, exist_ok=True)\n",
    "\n",
    "primary = os.path.join(HANDOFF_DIR, \"03_ai_agent_analysis_part1.pkl\")\n",
    "legacy  = os.path.join(\"handoff\", \"03_ai_agent_analysis_part1.pkl\")\n",
    "\n",
    "_candidates = []\n",
    "if os.path.exists(primary): _candidates.append(primary)\n",
    "if os.path.exists(legacy):  _candidates.append(legacy)\n",
    "\n",
    "if not _candidates:\n",
    "    arts = Path(\"artifacts\")\n",
    "    if arts.exists():\n",
    "        found = sorted(arts.glob(\"*/handoff/03_ai_agent_analysis_part1.pkl\"))\n",
    "        _candidates.extend(str(p) for p in found)\n",
    "\n",
    "if not _candidates:\n",
    "    raise FileNotFoundError(\n",
    "        \"handoff file not found.\\n\"\n",
    "        f\" - tried: {primary}\\n\"\n",
    "        f\" - tried: {legacy}\\n\"\n",
    "        \" - tried: artifacts/*/handoff/03_ai_agent_analysis_part1.pkl\\n\"\n",
    "        \"å¯¾å‡¦: ç›´å‰ãƒ‘ãƒ¼ãƒˆã‚’å…ˆã«å®Ÿè¡Œã™ã‚‹ã‹ã€RUN_ID ã‚’å›ºå®šã—ã¦åŒä¸€ RUN_ID ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\"\n",
    "    )\n",
    "\n",
    "_handoff_path = _candidates[0]\n",
    "handoff = joblib.load(_handoff_path)\n",
    "print(f\"âœ… handoff loaded: {_handoff_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be98dfb4-2e03-403d-9121-382f02e4eb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ DB_CONFIG ready: {'dbname': 'rapids_data', 'host': 'localhost', 'port': '5432', 'user': 'postgres'}\n"
     ]
    }
   ],
   "source": [
    "# === DB_CONFIG guard (mirrors 01_data_preparation) ===\n",
    "# 1) handoff ã«å«ã¾ã‚Œã¦ã„ã‚Œã°å„ªå…ˆã€2) ãªã‘ã‚Œã° 01 ã¨åŒã˜æ—¢å®šå€¤ï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯ï¼‰\n",
    "import os\n",
    "if 'DB_CONFIG' not in globals():\n",
    "    if 'handoff' in globals() and isinstance(handoff, dict) and 'DB_CONFIG' in handoff:\n",
    "        DB_CONFIG = handoff['DB_CONFIG']\n",
    "    else:\n",
    "        DB_CONFIG = {\n",
    "            'dbname': os.getenv('PGDATABASE','rapids_data'),\n",
    "            'user': os.getenv('PGUSER','postgres'),\n",
    "            'password': os.getenv('PGPASSWORD','asomura'),\n",
    "            'host': os.getenv('PGHOST','localhost'),\n",
    "            'port': os.getenv('PGPORT','5432'),\n",
    "        }\n",
    "# ãƒ­ã‚°ã«ã¯å®‰å…¨æƒ…å ±ã®ã¿\n",
    "print(\"ğŸ”§ DB_CONFIG ready:\", {k: DB_CONFIG[k] for k in ['dbname','host','port','user']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d3584e-8776-4327-ae68-755f924f6834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… handoff unpacked: false_negatives_df rows=55524, brand_keywords=109\n"
     ]
    }
   ],
   "source": [
    "# === handoff unpack (auto-added) ===\n",
    "# handoff ã‹ã‚‰å¿…è¦ã‚­ãƒ¼ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¸å±•é–‹ã€‚æ¬ ææ™‚ã¯æ˜ç¤ºçš„ã«ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã™ã€‚\n",
    "# NOTE:\n",
    "#  - DB_CONFIG ã¯ Part1 ã® handoff ã«ã¯é€šå¸¸å«ã‚ã¾ã›ã‚“ï¼ˆèªè¨¼æƒ…å ±ã‚’å«ã¿å¾—ã‚‹ãŸã‚ï¼‰ã€‚\n",
    "#  - ã“ã® Part2 ã§ã¯ç›´å‰ã‚»ãƒ«ã® DB_CONFIG guard ãŒ DB_CONFIG ã‚’ç”¨æ„ã™ã‚‹ã®ã§ã€handoff ã«ã¯å¿…é ˆã«ã—ã¾ã›ã‚“ã€‚\n",
    "required_keys = ['false_negatives_df', 'brand_keywords']\n",
    "missing = [k for k in required_keys if not ('handoff' in globals() and isinstance(handoff, dict) and k in handoff)]\n",
    "if missing:\n",
    "    found = list(handoff.keys()) if ('handoff' in globals() and isinstance(handoff, dict)) else None\n",
    "    raise KeyError(\n",
    "        f\"handoff keys missing: {missing}. ç›´å‰ãƒ‘ãƒ¼ãƒˆ(03-1)ã‚’åŒä¸€ RUN_ID ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\"\n",
    "        f\" (found keys={found})\"\n",
    "    )\n",
    "\n",
    "# å±•é–‹\n",
    "false_negatives_df = handoff['false_negatives_df']\n",
    "brand_keywords = handoff['brand_keywords']\n",
    "\n",
    "# DB_CONFIG ã¯ç›´å‰ã‚»ãƒ«ã® guard ã§æ—¢ã«å®šç¾©æ¸ˆã¿ã€‚handoff å´ãŒã‚ã‚Œã°ä¸Šæ›¸ãï¼ˆäº’æ›ç”¨ï¼‰\n",
    "if 'DB_CONFIG' in handoff:\n",
    "    try:\n",
    "        DB_CONFIG = handoff['DB_CONFIG']\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ã–ã£ãã‚Šã‚µãƒãƒª\n",
    "try:\n",
    "    n_rows = getattr(false_negatives_df, 'shape', [None, None])[0]\n",
    "except Exception:\n",
    "    n_rows = None\n",
    "bk_n = len(brand_keywords) if isinstance(brand_keywords, (list, dict)) else 'n/a'\n",
    "print(f\"âœ… handoff unpacked: false_negatives_df rows={n_rows}, brand_keywords={bk_n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e734619-fecd-4512-aa54-dbf06cca023e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ãƒãƒƒãƒ”ãƒ³ã‚°\n",
      "================================================================================\n",
      "âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèªå®Œäº†\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 02\n",
    "æ¦‚è¦: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€cert_full_info_mapã‚’ä½œæˆ\n",
    "å…¥åŠ›: false_negatives_df, DB_CONFIGï¼ˆã‚»ãƒ«01ã‹ã‚‰ï¼‰\n",
    "å‡ºåŠ›: cert_full_info_map, fn_features_df\n",
    "\"\"\"\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import pandas as pd\n",
    "from cryptography import x509\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ”§ è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ãƒãƒƒãƒ”ãƒ³ã‚°\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆ\n",
    "def test_db_connection(db_config):\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ãƒ†ã‚¹ãƒˆ\"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        conn.close()\n",
    "        return True\n",
    "    except psycopg2.OperationalError as e:\n",
    "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return False\n",
    "\n",
    "# DBæ¥ç¶šãƒ†ã‚¹ãƒˆ\n",
    "if not test_db_connection(DB_CONFIG):\n",
    "    print(\"\\nâŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š\")\n",
    "    print(f\"   - PostgreSQLãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹\")\n",
    "    print(f\"   - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å: {DB_CONFIG['dbname']}\")\n",
    "    print(f\"   - ãƒ›ã‚¹ãƒˆ: {DB_CONFIG['host']}:{DB_CONFIG['port']}\")\n",
    "    print(f\"   - ãƒ¦ãƒ¼ã‚¶ãƒ¼: {DB_CONFIG['user']}\")\n",
    "    raise ConnectionError(\"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚\")\n",
    "\n",
    "print(\"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèªå®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771a6a04-6533-4481-8360-c7be0d292912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š å½é™°æ€§ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n",
      "  - å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°: 55524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1195851/2514224003.py:22: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  cert_age_days = (datetime.utcnow() - not_before).days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - å‡¦ç†æ¸ˆã¿: 2,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 4,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 6,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 8,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 10,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 12,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 14,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 16,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 18,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 20,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 22,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 24,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 26,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 28,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 30,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 32,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 34,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 36,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 38,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 40,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 42,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 44,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 46,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 48,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 50,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 52,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 54,000/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "  - å‡¦ç†æ¸ˆã¿: 55,524/55,524 ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
      "\n",
      "âœ… è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†:\n",
      "  - è¨¼æ˜æ›¸ã‚ã‚Š: 55524ä»¶\n",
      "  - è¨¼æ˜æ›¸ãªã—: 0ä»¶\n",
      "\n",
      "ğŸ“Š è¨¼æ˜æ›¸çµ±è¨ˆ:\n",
      "  - ç„¡æ–™CAä½¿ç”¨: 31406ä»¶ (56.6%)\n",
      "\n",
      "ğŸ“Š å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã«è¨¼æ˜æ›¸ç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...\n",
      "âœ… è¨¼æ˜æ›¸ç‰¹å¾´é‡ã‚’è¿½åŠ å®Œäº†\n",
      "\n",
      "ğŸ¯ ä½ç¢ºç‡é ˜åŸŸï¼ˆ< 0.2ï¼‰ã§ã®è¨¼æ˜æ›¸ä½¿ç”¨çŠ¶æ³:\n",
      "  - ã‚µãƒ³ãƒ—ãƒ«æ•°: 51772ä»¶\n",
      "  - è¨¼æ˜æ›¸ã‚ã‚Š: 51772ä»¶ (100.0%)\n",
      "  - ç„¡æ–™CAï¼ˆLet's Encryptç­‰ï¼‰: 27844ä»¶ (53.8%)\n",
      "  - æ–°è¦è¨¼æ˜æ›¸ï¼ˆ7æ—¥ä»¥å†…ï¼‰: 0ä»¶\n",
      "  - æ–°è¦è¨¼æ˜æ›¸ï¼ˆ30æ—¥ä»¥å†…ï¼‰: 0ä»¶\n",
      "\n",
      "âœ… è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†\n",
      "  - cert_full_info_map: 55524ä»¶\n",
      "  - fn_features_df: 55524è¡Œ Ã— 12åˆ—\n",
      "\n",
      "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:\n",
      "  - cert_full_info_map: 1.83 MB\n",
      "  - fn_features_df: 11.96 MB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# è¨¼æ˜æ›¸æƒ…å ±ã‚’è§£æã™ã‚‹é–¢æ•°\n",
    "def parse_certificate_info(cert_data):\n",
    "    \"\"\"è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º\"\"\"\n",
    "    try:\n",
    "        # bytesã¾ãŸã¯memoryviewã®å‡¦ç†\n",
    "        if isinstance(cert_data, memoryview):\n",
    "            cert_data = bytes(cert_data)\n",
    "        \n",
    "        # DERå½¢å¼ã¨ã—ã¦è§£æ\n",
    "        cert = x509.load_der_x509_certificate(cert_data, default_backend())\n",
    "        \n",
    "        # ç™ºè¡Œè€…æƒ…å ±\n",
    "        issuer = cert.issuer\n",
    "        issuer_org = None\n",
    "        for attribute in issuer:\n",
    "            if attribute.oid == x509.oid.NameOID.ORGANIZATION_NAME:\n",
    "                issuer_org = attribute.value\n",
    "                break\n",
    "        \n",
    "        # è¨¼æ˜æ›¸ã®å¹´é½¢ï¼ˆæ—¥æ•°ï¼‰\n",
    "        not_before = cert.not_valid_before_utc.replace(tzinfo=None)\n",
    "        cert_age_days = (datetime.utcnow() - not_before).days\n",
    "        \n",
    "        # ç„¡æ–™CAåˆ¤å®šï¼ˆLet's Encryptã‚’é‡ç‚¹çš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰\n",
    "        free_ca_issuers = [\n",
    "            \"Let's Encrypt\", \"ZeroSSL\", \"Cloudflare\", \n",
    "            \"cPanel\", \"Sectigo\", \"DigiCert\"\n",
    "        ]\n",
    "        is_free_ca = False\n",
    "        if issuer_org:\n",
    "            is_free_ca = any(ca.lower() in str(issuer_org).lower() for ca in free_ca_issuers)\n",
    "        \n",
    "        # Let's Encryptã®ç‰¹åˆ¥ãƒã‚§ãƒƒã‚¯ï¼ˆçµ„ç¹”åã ã‘ã§ãªãã€CNç­‰ã‚‚ç¢ºèªï¼‰\n",
    "        if not is_free_ca:\n",
    "            issuer_str = str(issuer)\n",
    "            if \"let's encrypt\" in issuer_str.lower():\n",
    "                is_free_ca = True\n",
    "        \n",
    "        # SANsï¼ˆSubject Alternative Namesï¼‰ã®æ•°\n",
    "        san_count = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤\n",
    "        try:\n",
    "            san_ext = cert.extensions.get_extension_for_oid(\n",
    "                x509.oid.ExtensionOID.SUBJECT_ALTERNATIVE_NAME\n",
    "            )\n",
    "            san_count = len(san_ext.value)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰è¨¼æ˜æ›¸ã‹ã©ã†ã‹\n",
    "        subject = cert.subject\n",
    "        cn = None\n",
    "        for attribute in subject:\n",
    "            if attribute.oid == x509.oid.NameOID.COMMON_NAME:\n",
    "                cn = attribute.value\n",
    "                break\n",
    "        is_wildcard = cn and cn.startswith('*.')\n",
    "        \n",
    "        # è‡ªå·±ç½²åè¨¼æ˜æ›¸ã‹ã©ã†ã‹\n",
    "        is_self_signed = cert.issuer == cert.subject\n",
    "        \n",
    "        # çµ„ç¹”æƒ…å ±ã®æœ‰ç„¡\n",
    "        has_organization = False\n",
    "        for attribute in subject:\n",
    "            if attribute.oid == x509.oid.NameOID.ORGANIZATION_NAME:\n",
    "                has_organization = True\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'issuer_org': issuer_org,\n",
    "            'cert_age_days': cert_age_days,\n",
    "            'is_free_ca': is_free_ca,\n",
    "            'san_count': san_count,\n",
    "            'is_wildcard': is_wildcard,\n",
    "            'is_self_signed': is_self_signed,\n",
    "            'has_organization': has_organization,\n",
    "            'not_before': not_before,\n",
    "            'not_after': cert.not_valid_after_utc.replace(tzinfo=None),\n",
    "            'has_certificate': True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'has_certificate': False,\n",
    "            'parse_error': True,\n",
    "            'error_message': str(e)\n",
    "        }\n",
    "\n",
    "# cert_full_info_mapã®åˆæœŸåŒ–\n",
    "cert_full_info_map = {}\n",
    "\n",
    "# å½é™°æ€§ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "fn_domains = false_negatives_df['domain'].unique().tolist() if 'domain' in false_negatives_df.columns else []\n",
    "\n",
    "if not fn_domains:\n",
    "    print(\"âš ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "    fn_features_df = false_negatives_df.copy()\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š å½é™°æ€§ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\")\n",
    "    print(f\"  - å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³æ•°: {len(fn_domains)}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        cur = conn.cursor(cursor_factory=RealDictCursor)\n",
    "        \n",
    "        # ãƒãƒƒãƒå‡¦ç†ã§åŠ¹ç‡çš„ã«å–å¾—\n",
    "        batch_size = 500\n",
    "        total_found = 0\n",
    "        \n",
    "        for i in range(0, len(fn_domains), batch_size):\n",
    "            batch_domains = fn_domains[i:i+batch_size]\n",
    "            \n",
    "            # 01_data_preparation.pyã¨åŒã˜æ§‹é€ ã§ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ\n",
    "            # UNION ALLã‚’ä½¿ç”¨ã—ã¦å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—ï¼ˆé‡è¤‡ã‚‚å«ã‚€ï¼‰\n",
    "            query = \"\"\"\n",
    "            SELECT domain, cert_data, source FROM (\n",
    "                -- phishtank_entriesãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆcert_domainã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨ï¼‰\n",
    "                SELECT cert_domain as domain, cert_data, 'phishtank' as source\n",
    "                FROM phishtank_entries\n",
    "                WHERE cert_domain = ANY(%s) \n",
    "                AND cert_status = 'SUCCESS'\n",
    "                AND cert_data IS NOT NULL\n",
    "                \n",
    "                UNION ALL\n",
    "                \n",
    "                -- jpcert_phishing_urlsãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "                SELECT domain, cert_data, 'jpcert' as source\n",
    "                FROM jpcert_phishing_urls\n",
    "                WHERE domain = ANY(%s) \n",
    "                AND status = 'SUCCESS'\n",
    "                AND cert_data IS NOT NULL\n",
    "                \n",
    "                UNION ALL\n",
    "                \n",
    "                -- certificatesãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "                SELECT domain, cert_data, 'certificates' as source\n",
    "                FROM certificates\n",
    "                WHERE domain = ANY(%s) \n",
    "                AND status = 'SUCCESS'\n",
    "                AND cert_data IS NOT NULL\n",
    "                \n",
    "                UNION ALL\n",
    "                \n",
    "                -- trusted_certificatesãƒ†ãƒ¼ãƒ–ãƒ«\n",
    "                SELECT domain, cert_data, 'trusted_certificates' as source\n",
    "                FROM trusted_certificates\n",
    "                WHERE domain = ANY(%s) \n",
    "                AND status = 'SUCCESS'\n",
    "                AND cert_data IS NOT NULL\n",
    "            ) as combined_certs\n",
    "            \"\"\"\n",
    "            \n",
    "            cur.execute(query, (batch_domains, batch_domains, batch_domains, batch_domains))\n",
    "            results = cur.fetchall()\n",
    "            \n",
    "            # çµæœã‚’å‡¦ç†ï¼ˆå„ªå…ˆé †ä½: phishtank > jpcert > certificates > trustedï¼‰\n",
    "            domain_cert_map = {}\n",
    "            source_priority = {\n",
    "                'phishtank': 1,\n",
    "                'jpcert': 2, \n",
    "                'certificates': 3,\n",
    "                'trusted_certificates': 4\n",
    "            }\n",
    "            \n",
    "            for row in results:\n",
    "                domain = row['domain']\n",
    "                cert_data = row['cert_data']\n",
    "                source = row['source']\n",
    "                \n",
    "                # æ—¢ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å„ªå…ˆé †ä½ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "                if domain in domain_cert_map:\n",
    "                    current_priority = source_priority.get(domain_cert_map[domain]['source'], 999)\n",
    "                    new_priority = source_priority.get(source, 999)\n",
    "                    if new_priority < current_priority:\n",
    "                        domain_cert_map[domain] = {'cert_data': cert_data, 'source': source}\n",
    "                else:\n",
    "                    domain_cert_map[domain] = {'cert_data': cert_data, 'source': source}\n",
    "            \n",
    "            # è¨¼æ˜æ›¸æƒ…å ±ã‚’è§£æã—ã¦ãƒãƒƒãƒ—ã«è¿½åŠ \n",
    "            for domain, cert_info in domain_cert_map.items():\n",
    "                parsed_info = parse_certificate_info(cert_info['cert_data'])\n",
    "                parsed_info['source'] = cert_info['source']\n",
    "                cert_full_info_map[domain] = parsed_info\n",
    "                total_found += 1\n",
    "            \n",
    "            # é€²æ—è¡¨ç¤º\n",
    "            if (i + batch_size) % 2000 == 0 or (i + batch_size) >= len(fn_domains):\n",
    "                print(f\"  - å‡¦ç†æ¸ˆã¿: {min(i + batch_size, len(fn_domains)):,}/{len(fn_domains):,} ãƒ‰ãƒ¡ã‚¤ãƒ³\")\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"\\nâœ… è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†:\")\n",
    "        print(f\"  - è¨¼æ˜æ›¸ã‚ã‚Š: {total_found}ä»¶\")\n",
    "        print(f\"  - è¨¼æ˜æ›¸ãªã—: {len(fn_domains) - total_found}ä»¶\")\n",
    "        \n",
    "        # è¨¼æ˜æ›¸ãªã—ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚‚è¨˜éŒ²\n",
    "        for domain in fn_domains:\n",
    "            if domain not in cert_full_info_map:\n",
    "                cert_full_info_map[domain] = {\n",
    "                    'has_certificate': False,\n",
    "                    'no_cert_found': True\n",
    "                }\n",
    "        \n",
    "    except psycopg2.OperationalError as e:\n",
    "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        raise\n",
    "\n",
    "    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º\n",
    "    if cert_full_info_map:\n",
    "        # ç„¡æ–™CAä½¿ç”¨ã®çµ±è¨ˆ\n",
    "        free_ca_count = sum(1 for v in cert_full_info_map.values() if v.get('is_free_ca', False))\n",
    "        if free_ca_count > 0:\n",
    "            print(f\"\\nğŸ“Š è¨¼æ˜æ›¸çµ±è¨ˆ:\")\n",
    "            print(f\"  - ç„¡æ–™CAä½¿ç”¨: {free_ca_count}ä»¶ ({free_ca_count/len(cert_full_info_map)*100:.1f}%)\")\n",
    "        \n",
    "        # æ–°ã—ã„è¨¼æ˜æ›¸ï¼ˆ30æ—¥ä»¥å†…ï¼‰ã®çµ±è¨ˆ\n",
    "        new_cert_count = sum(1 for v in cert_full_info_map.values() \n",
    "                            if v.get('cert_age_days', float('inf')) < 30)\n",
    "        if new_cert_count > 0:\n",
    "            print(f\"  - æ–°è¦è¨¼æ˜æ›¸ï¼ˆ30æ—¥ä»¥å†…ï¼‰: {new_cert_count}ä»¶\")\n",
    "        \n",
    "        # è‡ªå·±ç½²åè¨¼æ˜æ›¸ã®çµ±è¨ˆ\n",
    "        self_signed_count = sum(1 for v in cert_full_info_map.values() if v.get('is_self_signed', False))\n",
    "        if self_signed_count > 0:\n",
    "            print(f\"  - è‡ªå·±ç½²åè¨¼æ˜æ›¸: {self_signed_count}ä»¶\")\n",
    "\n",
    "# fn_features_dfã®ä½œæˆï¼ˆå½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã«ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼‰\n",
    "fn_features_df = false_negatives_df.copy()\n",
    "\n",
    "# è¨¼æ˜æ›¸æƒ…å ±ã‚’è¿½åŠ ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "if cert_full_info_map and 'domain' in fn_features_df.columns:\n",
    "    print(\"\\nğŸ“Š å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã«è¨¼æ˜æ›¸ç‰¹å¾´é‡ã‚’è¿½åŠ ä¸­...\")\n",
    "    \n",
    "    # è¨¼æ˜æ›¸ç‰¹å¾´é‡ã‚’è¿½åŠ \n",
    "    fn_features_df['has_certificate'] = fn_features_df['domain'].map(\n",
    "        lambda d: cert_full_info_map.get(d, {}).get('has_certificate', False)\n",
    "    )\n",
    "    fn_features_df['cert_age_days'] = fn_features_df['domain'].map(\n",
    "        lambda d: cert_full_info_map.get(d, {}).get('cert_age_days', -1)\n",
    "    )\n",
    "    fn_features_df['is_free_ca'] = fn_features_df['domain'].map(\n",
    "        lambda d: cert_full_info_map.get(d, {}).get('is_free_ca', False)\n",
    "    )\n",
    "    fn_features_df['san_count'] = fn_features_df['domain'].map(\n",
    "        lambda d: cert_full_info_map.get(d, {}).get('san_count', 0)\n",
    "    )\n",
    "    fn_features_df['is_wildcard'] = fn_features_df['domain'].map(\n",
    "        lambda d: cert_full_info_map.get(d, {}).get('is_wildcard', False)\n",
    "    )\n",
    "    fn_features_df['is_self_signed'] = fn_features_df['domain'].map(\n",
    "        lambda d: cert_full_info_map.get(d, {}).get('is_self_signed', False)\n",
    "    )\n",
    "    fn_features_df['has_organization'] = fn_features_df['domain'].map(\n",
    "        lambda d: cert_full_info_map.get(d, {}).get('has_organization', False)\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… è¨¼æ˜æ›¸ç‰¹å¾´é‡ã‚’è¿½åŠ å®Œäº†\")\n",
    "    \n",
    "    # ä½ç¢ºç‡é ˜åŸŸã§ã®è¨¼æ˜æ›¸ä½¿ç”¨çŠ¶æ³ï¼ˆå•é¡Œã®æ ¸å¿ƒï¼‰\n",
    "    if 'prediction_proba' in fn_features_df.columns:\n",
    "        low_prob_df = fn_features_df[fn_features_df['prediction_proba'] < 0.2]\n",
    "        if len(low_prob_df) > 0:\n",
    "            print(f\"\\nğŸ¯ ä½ç¢ºç‡é ˜åŸŸï¼ˆ< 0.2ï¼‰ã§ã®è¨¼æ˜æ›¸ä½¿ç”¨çŠ¶æ³:\")\n",
    "            print(f\"  - ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(low_prob_df)}ä»¶\")\n",
    "            print(f\"  - è¨¼æ˜æ›¸ã‚ã‚Š: {low_prob_df['has_certificate'].sum()}ä»¶ ({low_prob_df['has_certificate'].mean()*100:.1f}%)\")\n",
    "            \n",
    "            # ç„¡æ–™CAä½¿ç”¨ã®è©³ç´°\n",
    "            free_ca_low = low_prob_df['is_free_ca'].sum()\n",
    "            if free_ca_low > 0:\n",
    "                print(f\"  - ç„¡æ–™CAï¼ˆLet's Encryptç­‰ï¼‰: {free_ca_low}ä»¶ ({free_ca_low/len(low_prob_df)*100:.1f}%)\")\n",
    "            \n",
    "            # æ–°è¦è¨¼æ˜æ›¸ã®è©³ç´°\n",
    "            new_certs = low_prob_df[low_prob_df['cert_age_days'] >= 0]\n",
    "            if len(new_certs) > 0:\n",
    "                very_new = (new_certs['cert_age_days'] < 7).sum()\n",
    "                new_30 = (new_certs['cert_age_days'] < 30).sum()\n",
    "                print(f\"  - æ–°è¦è¨¼æ˜æ›¸ï¼ˆ7æ—¥ä»¥å†…ï¼‰: {very_new}ä»¶\")\n",
    "                print(f\"  - æ–°è¦è¨¼æ˜æ›¸ï¼ˆ30æ—¥ä»¥å†…ï¼‰: {new_30}ä»¶\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦ä¿å­˜\n",
    "globals()['cert_full_info_map'] = cert_full_info_map\n",
    "globals()['fn_features_df'] = fn_features_df\n",
    "\n",
    "print(\"\\nâœ… è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†\")\n",
    "print(f\"  - cert_full_info_map: {len(cert_full_info_map)}ä»¶\")\n",
    "print(f\"  - fn_features_df: {fn_features_df.shape[0]}è¡Œ Ã— {fn_features_df.shape[1]}åˆ—\")\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª\n",
    "import sys\n",
    "map_size = sys.getsizeof(cert_full_info_map) / 1024 / 1024\n",
    "df_size = fn_features_df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "print(f\"\\nãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:\")\n",
    "print(f\"  - cert_full_info_map: {map_size:.2f} MB\")\n",
    "print(f\"  - fn_features_df: {df_size:.2f} MB\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47bdf477-c23b-42c0-9250-c90655f1dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b134292-f005-43c2-a1f4-2d401e810a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved handoff â†’ handoff/03_ai_agent_analysis_part2.pkl\n"
     ]
    }
   ],
   "source": [
    "# === handoff (auto-added) ===\n",
    "# ã“ã®ã‚»ãƒ«ã¯è‡ªå‹•è¿½åŠ : æ¬¡ãƒ‘ãƒ¼ãƒˆãŒå¿…è¦ã¨ã™ã‚‹æœ€å°é›†åˆã®ã¿ã‚’ä¿å­˜ã—ã¾ã™ï¼ˆåŸæ–‡ã‚»ãƒ«ã¯ç„¡æ”¹å¤‰ï¼‰ã€‚\n",
    "import os, joblib\n",
    "os.makedirs(HANDOFF_DIR, exist_ok=True)\n",
    "handoff = {}\n",
    "# --- keys: 'false_negatives_df', 'DB_CONFIG', 'brand_keywords', 'cert_full_info_map', 'fn_features_df'\n",
    "handoff['false_negatives_df'] = false_negatives_df\n",
    "handoff['DB_CONFIG'] = DB_CONFIG\n",
    "handoff['brand_keywords'] = brand_keywords\n",
    "handoff['cert_full_info_map'] = cert_full_info_map\n",
    "handoff['fn_features_df'] = fn_features_df\n",
    "joblib.dump(handoff, os.path.join(HANDOFF_DIR, \"03_ai_agent_analysis_part2.pkl\"))\n",
    "print(\"âœ… Saved handoff â†’ handoff/03_ai_agent_analysis_part2.pkl\")\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff61e9c-a64a-4d8f-af70-1ac5cc896fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ã‚»ãƒ«5ï¼ˆConfigå°å…¥ï¼†é©ç”¨ç‰ˆï½œãƒãƒ¼ãƒ‰ãƒ•ã‚§ã‚¤ãƒ«å¯¾å¿œï¼‰ ===\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 05\n",
    "æ¦‚è¦: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€cert_full_info_mapã‚’ä½œæˆï¼ˆConfigå¯¾å¿œ + require_connectionï¼‰\n",
    "å…¥åŠ›: false_negatives_df, handoff/DB_CONFIGï¼ˆã‚»ãƒ«01ã‹ã‚‰ï¼‰\n",
    "å‡ºåŠ›: cert_full_info_map, fn_features_df\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Configèª­ã¿è¾¼ã¿é–¢æ•°ã®è¿½åŠ \n",
    "# ----------------------------\n",
    "from typing import Optional, Dict, Any\n",
    "import os, json\n",
    "try:\n",
    "    import yaml  # ä»»æ„\n",
    "except Exception:\n",
    "    yaml = None\n",
    "\n",
    "def _deep_update(base: dict, override: dict) -> dict:\n",
    "    base = dict(base or {})\n",
    "    for k, v in (override or {}).items():\n",
    "        if isinstance(v, dict) and isinstance(base.get(k), dict):\n",
    "            base[k] = _deep_update(base[k], v)\n",
    "        else:\n",
    "            base[k] = v\n",
    "    return base\n",
    "\n",
    "def load_configuration(config_path: Optional[str] = None,\n",
    "                       cfg_override: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
    "    \"\"\"03_ai_agent_analysis_part2 ç”¨ã®è¨­å®šã‚’çµ±åˆã—ã¦è¿”ã™ã€‚çµæœã¯ globals()['cfg'] ã«ã‚‚ä¿å­˜ã€‚\"\"\"\n",
    "    defaults = {\n",
    "        \"system\": {\n",
    "            \"cert_only_mode\": True,\n",
    "            \"seed\": 42,\n",
    "            \"batch_size\": 500\n",
    "        },\n",
    "        \"db\": {\n",
    "            \"dbname\": \"rapids_data\",\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"asomura\",\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": \"5432\",\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"low_prob_threshold\": 0.2,\n",
    "            \"new_cert_days\": 30,\n",
    "            \"very_new_cert_days\": 7\n",
    "        },\n",
    "        \"free_ca_list\": [\"Let's Encrypt\", \"ZeroSSL\", \"Cloudflare\", \"cPanel\", \"Sectigo\", \"DigiCert\"]\n",
    "    }\n",
    "    cfg_all = dict(defaults)\n",
    "\n",
    "    def _read_file(p: Optional[str]) -> dict:\n",
    "        if not p: return {}\n",
    "        if not os.path.exists(p): return {}\n",
    "        try:\n",
    "            txt = open(p, \"r\", encoding=\"utf-8\").read()\n",
    "            if p.endswith((\".yml\", \".yaml\")) and yaml is not None:\n",
    "                return yaml.safe_load(txt) or {}\n",
    "            return json.loads(txt)\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    # a) è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä»»æ„ï¼‰\n",
    "    cfg_all = _deep_update(cfg_all, _read_file(config_path or os.getenv(\"CONFIG_PATH\")))\n",
    "\n",
    "    # b) ç’°å¢ƒå¤‰æ•°ã®åæ˜ \n",
    "    env_map = {\n",
    "        \"CERT_BATCH_SIZE\": (\"system\", \"batch_size\"),\n",
    "        \"PGDATABASE\": (\"db\", \"dbname\"),\n",
    "        \"PGUSER\": (\"db\", \"user\"),\n",
    "        \"PGPASSWORD\": (\"db\", \"password\"),\n",
    "        \"PGHOST\": (\"db\", \"host\"),\n",
    "        \"PGPORT\": (\"db\", \"port\"),\n",
    "    }\n",
    "    for k, path in env_map.items():\n",
    "        if k in os.environ and os.environ[k] != \"\":\n",
    "            node = cfg_all\n",
    "            for key in path[:-1]:\n",
    "                node = node.setdefault(key, {})\n",
    "            node[path[-1]] = os.environ[k]\n",
    "\n",
    "    # c) å¼•æ•°override\n",
    "    if isinstance(cfg_override, dict):\n",
    "        cfg_all = _deep_update(cfg_all, cfg_override)\n",
    "\n",
    "    # æ¤œè¨¼\n",
    "    if not bool(cfg_all[\"system\"].get(\"cert_only_mode\", True)):\n",
    "        raise ValueError(\"system.cert_only_mode ã¯ True ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "    for req in (\"dbname\", \"user\", \"host\", \"port\"):\n",
    "        if not str(cfg_all[\"db\"].get(req, \"\")).strip():\n",
    "            raise ValueError(f\"DBæ¥ç¶šæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™: db.{req}\")\n",
    "    bs = int(cfg_all[\"system\"].get(\"batch_size\", 500))\n",
    "    if not (100 <= bs <= 5000):\n",
    "        raise ValueError(\"system.batch_size ã¯ 100ã€œ5000 ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "    globals()[\"cfg\"] = cfg_all\n",
    "    return cfg_all\n",
    "\n",
    "# æ—¢å­˜ã® handoff/cfg ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒ­ãƒ¼ãƒ‰ï¼ˆcfg ãŒæœªå®šç¾©ã§ã‚‚OKï¼‰\n",
    "_cfg_override = globals().get(\"cfg\", None)\n",
    "cfg = load_configuration(cfg_override=_cfg_override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd3bbe2a-c344-42a2-83d3-db6b76ffd5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆConfigå¯¾å¿œï½œãƒãƒ¼ãƒ‰ãƒ•ã‚§ã‚¤ãƒ«å¯¾å¿œï¼‰\n",
      "================================================================================\n",
      "âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèªå®Œäº†\n",
      "\n",
      "ğŸ“Š å½é™°æ€§ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­... (targets=55,524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1195851/3705004247.py:81: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  cert_age_days = (datetime.utcnow() - not_before).days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - å‡¦ç†æ¸ˆã¿: 1,000/55,524 (batch=500, found=1000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 2,000/55,524 (batch=500, found=2000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 3,000/55,524 (batch=500, found=3000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 4,000/55,524 (batch=500, found=4000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 5,000/55,524 (batch=500, found=5000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 6,000/55,524 (batch=500, found=6000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 7,000/55,524 (batch=500, found=7000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 8,000/55,524 (batch=500, found=8000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 9,000/55,524 (batch=500, found=9000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 10,000/55,524 (batch=500, found=10000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 11,000/55,524 (batch=500, found=11000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 12,000/55,524 (batch=500, found=12000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 13,000/55,524 (batch=500, found=13000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 14,000/55,524 (batch=500, found=14000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 15,000/55,524 (batch=500, found=15000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 16,000/55,524 (batch=500, found=16000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 17,000/55,524 (batch=500, found=17000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 18,000/55,524 (batch=500, found=18000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 19,000/55,524 (batch=500, found=19000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 20,000/55,524 (batch=500, found=20000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 21,000/55,524 (batch=500, found=21000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 22,000/55,524 (batch=500, found=22000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 23,000/55,524 (batch=500, found=23000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 24,000/55,524 (batch=500, found=24000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 25,000/55,524 (batch=500, found=25000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 26,000/55,524 (batch=500, found=26000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 27,000/55,524 (batch=500, found=27000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 28,000/55,524 (batch=500, found=28000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 29,000/55,524 (batch=500, found=29000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 30,000/55,524 (batch=500, found=30000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 31,000/55,524 (batch=500, found=31000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 32,000/55,524 (batch=500, found=32000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 33,000/55,524 (batch=500, found=33000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 34,000/55,524 (batch=500, found=34000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 35,000/55,524 (batch=500, found=35000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 36,000/55,524 (batch=500, found=36000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 37,000/55,524 (batch=500, found=37000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 38,000/55,524 (batch=500, found=38000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 39,000/55,524 (batch=500, found=39000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 40,000/55,524 (batch=500, found=40000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 41,000/55,524 (batch=500, found=41000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 42,000/55,524 (batch=500, found=42000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 43,000/55,524 (batch=500, found=43000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 44,000/55,524 (batch=500, found=44000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 45,000/55,524 (batch=500, found=45000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 46,000/55,524 (batch=500, found=46000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 47,000/55,524 (batch=500, found=47000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 48,000/55,524 (batch=500, found=48000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 49,000/55,524 (batch=500, found=49000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 50,000/55,524 (batch=500, found=50000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 51,000/55,524 (batch=500, found=51000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 52,000/55,524 (batch=500, found=52000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 53,000/55,524 (batch=500, found=53000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 54,000/55,524 (batch=500, found=54000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 55,000/55,524 (batch=500, found=55000)\n",
      "  - å‡¦ç†æ¸ˆã¿: 55,524/55,524 (batch=24, found=55524)\n",
      "\n",
      "âœ… è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: è¨¼æ˜æ›¸ã‚ã‚Š=55524 / å…¨55524\n",
      "\n",
      "ğŸ¯ ä½ç¢ºç‡é ˜åŸŸï¼ˆConfigï¼‰ã§ã®çµ±è¨ˆ:\n",
      "  - ã‚µãƒ³ãƒ—ãƒ«æ•°: 51772ä»¶\n",
      "  - è¨¼æ˜æ›¸ã‚ã‚Š: 51772ä»¶ (100.0%)\n",
      "  - ç„¡æ–™CA: 27844ä»¶ (53.8%)\n",
      "  - æ–°è¦ï¼ˆ7æ—¥ä»¥å†…ï¼‰: 0ä»¶ / ï¼ˆ30æ—¥ä»¥å†…ï¼‰: 0ä»¶\n",
      "\n",
      "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: cert_full_info_map=1.83 MB, fn_features_df=11.96 MB\n",
      "================================================================================\n",
      "âœ… è¨¼æ˜æ›¸ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†ï¼ˆConfigå¯¾å¿œ/ãƒãƒ¼ãƒ‰ãƒ•ã‚§ã‚¤ãƒ«åˆ‡æ›¿/ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å¯¾å¿œï¼‰\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 2) ä»¥é™ï¼šæ—¢å­˜ã‚»ãƒ«æœ¬ä½“ï¼ˆDBè¨­å®š/å‡¦ç†ï¼‰ã‚’æ›¸ãæ›ãˆ\n",
    "# ----------------------------\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import pandas as pd\n",
    "from cryptography import x509\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print(\"ğŸ”§ è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆConfigå¯¾å¿œï½œãƒãƒ¼ãƒ‰ãƒ•ã‚§ã‚¤ãƒ«å¯¾å¿œï¼‰\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2-1) DBè¨­å®šã®å„ªå…ˆé †ä½: cfg['db'] -> handoff['DB_CONFIG'] -> ç’°å¢ƒå¤‰æ•°/ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ\n",
    "def _make_env_db():\n",
    "    return {\n",
    "        'dbname': os.getenv('PGDATABASE','rapids_data'),\n",
    "        'user': os.getenv('PGUSER','postgres'),\n",
    "        'password': os.getenv('PGPASSWORD','asomura'),\n",
    "        'host': os.getenv('PGHOST','localhost'),\n",
    "        'port': os.getenv('PGPORT','5432'),\n",
    "    }\n",
    "\n",
    "if 'cfg' in globals() and isinstance(cfg, dict) and 'db' in cfg:\n",
    "    DB_CONFIG = dict(cfg['db'])\n",
    "elif 'handoff' in globals() and isinstance(handoff, dict) and 'DB_CONFIG' in handoff:\n",
    "    DB_CONFIG = dict(handoff['DB_CONFIG'])\n",
    "else:\n",
    "    DB_CONFIG = _make_env_db()\n",
    "\n",
    "# connect_timeout ã®é©ç”¨\n",
    "_connect_timeout = int(cfg.get('db',{}).get('timeout_s', 30))\n",
    "DB_CONFIG_EFF = dict(DB_CONFIG)\n",
    "DB_CONFIG_EFF['connect_timeout'] = _connect_timeout\n",
    "\n",
    "# 2-2) è§£æãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼šç„¡æ–™CAãƒªã‚¹ãƒˆã¯Configã‹ã‚‰\n",
    "FREE_CA_ISSUERS = [str(x) for x in (cfg.get(\"free_ca_list\") or [])]\n",
    "LOW_TH = float(cfg['analysis']['low_prob_threshold'])\n",
    "NEW30 = int(cfg['analysis']['new_cert_days'])\n",
    "NEW7  = int(cfg['analysis']['very_new_cert_days'])\n",
    "\n",
    "def test_db_connection(db_config):\n",
    "    try:\n",
    "        conn = psycopg2.connect(**db_config)\n",
    "        conn.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šä¸å¯: {e}\")\n",
    "        return False\n",
    "\n",
    "db_ok = test_db_connection(DB_CONFIG_EFF)\n",
    "#require_conn = bool(cfg.get('db',{}).get('require_connection', False))\n",
    "#if not db_ok and require_conn:\n",
    "#    # â˜… ãƒãƒ¼ãƒ‰ãƒ•ã‚§ã‚¤ãƒ«: ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæ™‚ã¯ä¾‹å¤–ã§åœæ­¢\n",
    "#    raise ConnectionError(\"Database not available and cfg['db']['require_connection']=True\")\n",
    "\n",
    "if db_ok:\n",
    "    print(\"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèªå®Œäº†\")\n",
    "else:\n",
    "    print(\"âš ï¸ DBæœªæ¥ç¶šã®ãŸã‚ã€ã“ã®ã‚»ãƒ«ã¯ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶šã—ã¾ã™ï¼ˆã‚¨ãƒ©ãƒ¼ã«ã—ã¾ã›ã‚“ï¼‰ã€‚\")\n",
    "\n",
    "def parse_certificate_info(cert_data):\n",
    "    \"\"\"è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ—¢å­˜ä»•æ§˜ã‚’è¸è¥²ï¼‰\"\"\"\n",
    "    try:\n",
    "        if isinstance(cert_data, memoryview):\n",
    "            cert_data = bytes(cert_data)\n",
    "        cert = x509.load_der_x509_certificate(cert_data, default_backend())\n",
    "\n",
    "        issuer = cert.issuer\n",
    "        issuer_org = None\n",
    "        for attribute in issuer:\n",
    "            if attribute.oid == x509.oid.NameOID.ORGANIZATION_NAME:\n",
    "                issuer_org = attribute.value\n",
    "                break\n",
    "\n",
    "        not_before = cert.not_valid_before_utc.replace(tzinfo=None)\n",
    "        not_after  = cert.not_valid_after_utc.replace(tzinfo=None)\n",
    "        cert_age_days = (datetime.utcnow() - not_before).days\n",
    "\n",
    "        # ç„¡æ–™CAåˆ¤å®š\n",
    "        is_free_ca = False\n",
    "        issuer_str_all = f\"{issuer_org or ''} {issuer.rfc4514_string()}\"\n",
    "        low_all = issuer_str_all.lower()\n",
    "        for ca in FREE_CA_ISSUERS:\n",
    "            if ca.lower() in low_all:\n",
    "                is_free_ca = True\n",
    "                break\n",
    "\n",
    "        # SANæ•°\n",
    "        san_count = 1\n",
    "        try:\n",
    "            san_ext = cert.extensions.get_extension_for_oid(\n",
    "                x509.oid.ExtensionOID.SUBJECT_ALTERNATIVE_NAME\n",
    "            )\n",
    "            san_count = len(san_ext.value)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰\n",
    "        is_wildcard = False\n",
    "        subject = cert.subject\n",
    "        for attribute in subject:\n",
    "            if attribute.oid == x509.oid.NameOID.COMMON_NAME:\n",
    "                if str(attribute.value).startswith(\"*.\"):\n",
    "                    is_wildcard = True\n",
    "                break\n",
    "\n",
    "        # è‡ªå·±ç½²å\n",
    "        is_self_signed = (cert.issuer == cert.subject)\n",
    "\n",
    "        # çµ„ç¹”åã®æœ‰ç„¡\n",
    "        has_organization = any(\n",
    "            attr.oid == x509.oid.NameOID.ORGANIZATION_NAME for attr in subject\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'issuer_org': issuer_org,\n",
    "            'cert_age_days': cert_age_days,\n",
    "            'is_free_ca': is_free_ca,\n",
    "            'san_count': san_count,\n",
    "            'is_wildcard': is_wildcard,\n",
    "            'is_self_signed': is_self_signed,\n",
    "            'has_organization': has_organization,\n",
    "            'not_before': not_before,\n",
    "            'not_after': not_after,\n",
    "            'has_certificate': True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'has_certificate': False, 'parse_error': True, 'error_message': str(e)}\n",
    "\n",
    "# 2-3) å¯¾è±¡ãƒ‰ãƒ¡ã‚¤ãƒ³\n",
    "if 'false_negatives_df' in globals() and hasattr(false_negatives_df, 'columns'):\n",
    "    fn_domains = false_negatives_df['domain'].unique().tolist() if 'domain' in false_negatives_df.columns else []\n",
    "else:\n",
    "    fn_domains = []\n",
    "\n",
    "if not fn_domains or not db_ok:\n",
    "    print(\"â„¹ï¸ ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒæœªæä¾›ã€ã¾ãŸã¯DBæœªæ¥ç¶šã®ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒ«ç©ºãƒ‡ãƒ¼ã‚¿ã§ç¶™ç¶šã—ã¾ã™ã€‚\")\n",
    "    import pandas as pd\n",
    "    fn_features_df = pd.DataFrame(columns=[\n",
    "        'domain','prediction_proba','has_certificate','cert_age_days','is_free_ca',\n",
    "        'san_count','is_wildcard','is_self_signed','has_organization'\n",
    "    ])\n",
    "    cert_full_info_map = {}\n",
    "else:\n",
    "    print(f\"\\nğŸ“Š å½é™°æ€§ãƒ‰ãƒ¡ã‚¤ãƒ³ã®è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­... (targets={len(fn_domains):,})\")\n",
    "\n",
    "    # 2-4) ãƒãƒƒãƒå–å¾—ï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚º=cfg.system.batch_sizeã€å¤±æ•—æ™‚ã¯åŠæ¸›ãƒªãƒˆãƒ©ã‚¤ï¼‰\n",
    "    target_batch = int(cfg['system']['batch_size'])\n",
    "    min_batch = 100\n",
    "    progress_every = max(1000, target_batch * 2)\n",
    "\n",
    "    cert_full_info_map = {}\n",
    "    total_found = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(fn_domains):\n",
    "        bs = min(target_batch, len(fn_domains) - i)\n",
    "        batch_domains = fn_domains[i:i+bs]\n",
    "\n",
    "        query = \"\"\"\n",
    "        SELECT domain, cert_data, source FROM (\n",
    "            SELECT cert_domain as domain, cert_data, 'phishtank' as source\n",
    "            FROM phishtank_entries\n",
    "            WHERE cert_status = 'SUCCESS' AND cert_data IS NOT NULL AND cert_domain = ANY(%s)\n",
    "            UNION ALL\n",
    "            SELECT domain, cert_data, 'jpcert' as source\n",
    "            FROM jpcert_phishing_urls\n",
    "            WHERE status = 'SUCCESS' AND cert_data IS NOT NULL AND domain = ANY(%s)\n",
    "            UNION ALL\n",
    "            SELECT domain, cert_data, 'certificates' as source\n",
    "            FROM certificates\n",
    "            WHERE status = 'SUCCESS' AND cert_data IS NOT NULL AND domain = ANY(%s)\n",
    "            UNION ALL\n",
    "            SELECT domain, cert_data, 'trusted_certificates' as source\n",
    "            FROM trusted_certificates\n",
    "            WHERE status = 'SUCCESS' AND cert_data IS NOT NULL AND domain = ANY(%s)\n",
    "        ) as combined_certs\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            conn = psycopg2.connect(**DB_CONFIG_EFF)\n",
    "            cur = conn.cursor(cursor_factory=RealDictCursor)\n",
    "            cur.execute(query, (batch_domains, batch_domains, batch_domains, batch_domains))\n",
    "            results = cur.fetchall()\n",
    "            conn.close()\n",
    "\n",
    "            domain_cert_map = {}\n",
    "            source_priority = {'phishtank':1,'jpcert':2,'certificates':3,'trusted_certificates':4}\n",
    "\n",
    "            for row in results:\n",
    "                d = row['domain']; src = row['source']; cd = row['cert_data']\n",
    "                if d in domain_cert_map:\n",
    "                    if source_priority.get(src, 999) < source_priority.get(domain_cert_map[d]['source'], 999):\n",
    "                        domain_cert_map[d] = {'cert_data': cd, 'source': src}\n",
    "                else:\n",
    "                    domain_cert_map[d] = {'cert_data': cd, 'source': src}\n",
    "\n",
    "            for d, cinfo in domain_cert_map.items():\n",
    "                parsed = parse_certificate_info(cinfo['cert_data'])\n",
    "                parsed['source'] = cinfo['source']\n",
    "                cert_full_info_map[d] = parsed\n",
    "                total_found += 1\n",
    "\n",
    "            # é€²æ—\n",
    "            if ((i + bs) % progress_every == 0) or (i + bs >= len(fn_domains)):\n",
    "                print(f\"  - å‡¦ç†æ¸ˆã¿: {min(i+bs, len(fn_domains)):,}/{len(fn_domains):,} (batch={bs}, found={total_found})\")\n",
    "\n",
    "            i += bs\n",
    "\n",
    "        except MemoryError:\n",
    "            # ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’åŠåˆ†ã«ã—ã¦ç¶™ç¶š\n",
    "            target_batch = max(min_batch, target_batch // 2)\n",
    "            print(f\"âš ï¸ MemoryError: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç¸®å°ã—ã¦å†è©¦è¡Œã—ã¾ã™ -> {target_batch}\")\n",
    "        except Exception as e:\n",
    "            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ç¶™ç¶šï¼ˆãƒ­ã‚°å‡ºåŠ›ï¼‰\n",
    "            print(f\"âš ï¸ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)[:200]}ï¼ˆç¶™ç¶šï¼‰\")\n",
    "            i += bs  # ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶™ç¶š\n",
    "\n",
    "    # è¨¼æ˜æ›¸ãªã—ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚‚è¨˜éŒ²\n",
    "    for d in fn_domains:\n",
    "        if d not in cert_full_info_map:\n",
    "            cert_full_info_map[d] = {'has_certificate': False, 'no_cert_found': True}\n",
    "\n",
    "    print(f\"\\nâœ… è¨¼æ˜æ›¸ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: è¨¼æ˜æ›¸ã‚ã‚Š={sum(v.get('has_certificate',False) for v in cert_full_info_map.values())} / å…¨{len(fn_domains)}\")\n",
    "\n",
    "    # çµ±è¨ˆï¼†ç‰¹å¾´é‡ä»˜ä¸ï¼ˆConfigã®é–¾å€¤ãƒ»æ—¥æ•°ã§ï¼‰\n",
    "    fn_features_df = false_negatives_df.copy()\n",
    "    if 'domain' in fn_features_df.columns:\n",
    "        fn_features_df['has_certificate'] = fn_features_df['domain'].map(lambda d: cert_full_info_map.get(d,{}).get('has_certificate', False))\n",
    "        fn_features_df['cert_age_days'] = fn_features_df['domain'].map(lambda d: cert_full_info_map.get(d,{}).get('cert_age_days', -1))\n",
    "        fn_features_df['is_free_ca']    = fn_features_df['domain'].map(lambda d: cert_full_info_map.get(d,{}).get('is_free_ca', False))\n",
    "        fn_features_df['san_count']     = fn_features_df['domain'].map(lambda d: cert_full_info_map.get(d,{}).get('san_count', 0))\n",
    "        fn_features_df['is_wildcard']   = fn_features_df['domain'].map(lambda d: cert_full_info_map.get(d,{}).get('is_wildcard', False))\n",
    "        fn_features_df['is_self_signed']= fn_features_df['domain'].map(lambda d: cert_full_info_map.get(d,{}).get('is_self_signed', False))\n",
    "        fn_features_df['has_organization']= fn_features_df['domain'].map(lambda d: cert_full_info_map.get(d,{}).get('has_organization', False))\n",
    "\n",
    "        print(\"\\nğŸ¯ ä½ç¢ºç‡é ˜åŸŸï¼ˆConfigï¼‰ã§ã®çµ±è¨ˆ:\")\n",
    "        if 'prediction_proba' in fn_features_df.columns:\n",
    "            low_df = fn_features_df[fn_features_df['prediction_proba'] < LOW_TH]\n",
    "            print(f\"  - ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(low_df)}ä»¶\")\n",
    "            if len(low_df):\n",
    "                print(f\"  - è¨¼æ˜æ›¸ã‚ã‚Š: {low_df['has_certificate'].sum()}ä»¶ ({low_df['has_certificate'].mean()*100:.1f}%)\")\n",
    "                print(f\"  - ç„¡æ–™CA: {low_df['is_free_ca'].sum()}ä»¶ ({low_df['is_free_ca'].mean()*100:.1f}%)\")\n",
    "                nc = low_df[low_df['cert_age_days'] >= 0]\n",
    "                if len(nc):\n",
    "                    v7 = (nc['cert_age_days'] < NEW7).sum()\n",
    "                    v30= (nc['cert_age_days'] < NEW30).sum()\n",
    "                    print(f\"  - æ–°è¦ï¼ˆ{NEW7}æ—¥ä»¥å†…ï¼‰: {v7}ä»¶ / ï¼ˆ{NEW30}æ—¥ä»¥å†…ï¼‰: {v30}ä»¶\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¿å­˜\n",
    "globals()['cert_full_info_map'] = cert_full_info_map if 'cert_full_info_map' in locals() else {}\n",
    "globals()['fn_features_df'] = fn_features_df\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆå®‰å…¨ã«è¨ˆç®—ï¼‰\n",
    "try:\n",
    "    map_size_mb = (sys.getsizeof(globals()['cert_full_info_map']) / 1024 / 1024)\n",
    "except Exception:\n",
    "    map_size_mb = 0.0\n",
    "try:\n",
    "    import pandas as _pd\n",
    "    df_size_mb = (fn_features_df.memory_usage(deep=True).sum() / 1024 / 1024) if isinstance(fn_features_df, _pd.DataFrame) else 0.0\n",
    "except Exception:\n",
    "    df_size_mb = 0.0\n",
    "\n",
    "print(f\"\\nãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: cert_full_info_map={map_size_mb:.2f} MB, fn_features_df={df_size_mb:.2f} MB\")\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ… è¨¼æ˜æ›¸ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†ï¼ˆConfigå¯¾å¿œ/ãƒãƒ¼ãƒ‰ãƒ•ã‚§ã‚¤ãƒ«åˆ‡æ›¿/ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å¯¾å¿œï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74760bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ã‚»ãƒ«7: Controller APIé–¢æ•° ===\n",
    "from typing import Tuple, Dict, Any\n",
    "import os, joblib\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "\n",
    "def agent_enhance(session_id: str, cfg: Dict[str, Any]) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Controller ã‹ã‚‰å‘¼ã³å‡ºã™API\n",
    "    æˆ»ã‚Šå€¤:\n",
    "      - ReturnCode: \"OK\" | \"NOT_FOUND\" | \"INVALID_INPUT\" | \"ERROR\"\n",
    "      - Paths: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¾æ›¸\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1) è¨­å®šã®æ¤œè¨¼ã¨ãƒãƒ¼ã‚¸\n",
    "        try:\n",
    "            _cfg = load_configuration(cfg_override=cfg)\n",
    "        except Exception as e:\n",
    "            return \"INVALID_INPUT\", {\"error\": f\"config error: {e}\"}\n",
    "        if not bool(_cfg.get(\"system\",{}).get(\"cert_only_mode\", True)):\n",
    "            return \"INVALID_INPUT\", {\"error\": \"cert_only_mode must be True\"}\n",
    "\n",
    "        # 2) handoffã®èª­ã¿è¾¼ã¿ï¼ˆPart1â†’Part2é–“ï¼‰\n",
    "        global handoff\n",
    "        if 'handoff' not in globals() or not isinstance(handoff, dict):\n",
    "            candidates = []\n",
    "            if 'HANDOFF_DIR' in globals():\n",
    "                p = os.path.join(HANDOFF_DIR, \"03_ai_agent_analysis_part1.pkl\")\n",
    "                if os.path.exists(p): candidates.append(p)\n",
    "            if not candidates:\n",
    "                from pathlib import Path\n",
    "                arts = Path(\"artifacts\")\n",
    "                if arts.exists():\n",
    "                    found = sorted(arts.glob(\"*/handoff/03_ai_agent_analysis_part1.pkl\"))\n",
    "                    candidates.extend([str(p) for p in found])\n",
    "            if not candidates:\n",
    "                return \"NOT_FOUND\", {\"error\": \"handoff(part1) not found\"}\n",
    "            handoff = joblib.load(candidates[0])\n",
    "\n",
    "        required = [\"false_negatives_df\"]\n",
    "        missing = [k for k in required if k not in handoff]\n",
    "        if missing:\n",
    "            return \"NOT_FOUND\", {\"error\": f\"handoff missing keys: {missing}\"}\n",
    "\n",
    "        # 3) è¨¼æ˜æ›¸å–å¾—å‡¦ç†ã®ç¢ºèª\n",
    "        if 'cert_full_info_map' not in globals() or 'fn_features_df' not in globals():\n",
    "            return \"NOT_FOUND\", {\"error\": \"certificate mapping not prepared (run cell 5)\"}        \n",
    "        if len(cert_full_info_map) == 0:\n",
    "            return \"NOT_FOUND\", {\"error\": \"no certificates fetched\"}\n",
    "\n",
    "        # 5) å‡ºåŠ›ãƒ‘ã‚¹è¾æ›¸ã®ä½œæˆ\n",
    "        run_base = Path(globals().get('RESULTS_DIR', 'results')) / str(session_id)\n",
    "        logs_base = Path(globals().get('LOGS_DIR', 'logs')) / \"cert_analysis\"\n",
    "        run_base.mkdir(parents=True, exist_ok=True)\n",
    "        logs_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        p_cert_map = run_base / \"cert_full_info_map.pkl\"\n",
    "        p_fn_feat  = run_base / \"fn_features_df.pkl\"\n",
    "        p_handoff  = Path(globals().get('HANDOFF_DIR', 'handoff')) / \"03_ai_agent_analysis_part2.pkl\"\n",
    "        p_logs     = logs_base / \"cert_analysis.log\"\n",
    "\n",
    "        out_handoff = dict(handoff)\n",
    "        out_handoff.update({\n",
    "            \"cert_full_info_map\": cert_full_info_map,\n",
    "            \"fn_features_df\": fn_features_df\n",
    "        })\n",
    "        joblib.dump(cert_full_info_map, p_cert_map)\n",
    "        joblib.dump(fn_features_df, p_fn_feat)\n",
    "        joblib.dump(out_handoff, p_handoff)\n",
    "\n",
    "        Paths = {\n",
    "            \"cert_full_info_map\": str(p_cert_map),\n",
    "            \"fn_features_df\": str(p_fn_feat),\n",
    "            \"handoff\": str(p_handoff),\n",
    "            \"logs\": str(p_logs)\n",
    "        }\n",
    "        return \"OK\", Paths\n",
    "\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc(limit=3)\n",
    "        return \"ERROR\", {\"error\": str(e), \"trace\": tb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9571de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚»ãƒ«è‡ªå·±è¨ºæ–­:\n",
      "âœ” cfgã¨agent_enhanceã®åŸºæœ¬å­˜åœ¨OKï¼ˆå®Ÿè¡Œã¯ã‚»ãƒ«é †ä¾å­˜ï¼‰\n"
     ]
    }
   ],
   "source": [
    "print('ã‚»ãƒ«è‡ªå·±è¨ºæ–­:')\n",
    "try:\n",
    "    assert 'cfg' in globals()\n",
    "    assert callable(agent_enhance)\n",
    "    print('âœ” cfgã¨agent_enhanceã®åŸºæœ¬å­˜åœ¨OKï¼ˆå®Ÿè¡Œã¯ã‚»ãƒ«é †ä¾å­˜ï¼‰')\n",
    "except Exception as e:\n",
    "    print('â„¹ï¸ æ³¨æ„: åˆå›å®Ÿè¡Œé †ã«ã‚ˆã‚Šæœªå®šç¾©ã®å ´åˆãŒã‚ã‚Šã¾ã™:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d04782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell HF-02A: test_async_connection (restored minimal) ===\n",
    "import asyncio, time\n",
    "from typing import Dict, Any\n",
    "\n",
    "async def test_async_connection() -> Dict[str, Any]:\n",
    "    t0 = time.perf_counter()\n",
    "    try:\n",
    "        async def _work(i: int):\n",
    "            await asyncio.sleep(0.001)\n",
    "            return i\n",
    "        results = await asyncio.gather(*[_work(i) for i in range(3)])\n",
    "        elapsed_ms = (time.perf_counter() - t0) * 1000.0\n",
    "        try:\n",
    "            policy = type(asyncio.get_event_loop_policy()).__name__\n",
    "        except Exception:\n",
    "            policy = \"unknown\"\n",
    "        return {\n",
    "            \"ok\": True,\n",
    "            \"elapsed_ms\": round(elapsed_ms, 3),\n",
    "            \"loop_policy\": policy,\n",
    "            \"details\": {\"results\": results},\n",
    "        }\n",
    "    except Exception as e:\n",
    "        elapsed_ms = (time.perf_counter() - t0) * 1000.0\n",
    "        return {\"ok\": False, \"error\": str(e), \"elapsed_ms\": round(elapsed_ms, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da178c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HF] test_async_connection: {\"ok\": true, \"elapsed_ms\": 1.607, \"loop_policy\": \"_UnixDefaultEventLoopPolicy\", \"details\": {\"results\": [0, 1, 2]}}\n",
      "[HF] PASS\n"
     ]
    }
   ],
   "source": [
    "# === Cell HF-02B: test_async_connection (self-check) ===\n",
    "import asyncio, json\n",
    "try:\n",
    "    _ = asyncio.get_running_loop()\n",
    "    res = await test_async_connection()\n",
    "except RuntimeError:\n",
    "    res = asyncio.run(test_async_connection())\n",
    "print(\"[HF] test_async_connection:\", json.dumps(res, ensure_ascii=False))\n",
    "assert isinstance(res, dict) and res.get(\"ok\") is True, \"Async connection test failed\"\n",
    "print(\"[HF] PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65050a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell HF-03A: test_async_functions (restored minimal) ===\n",
    "# Purpose: Validate async behaviors (concurrency cap, cancel/timeout handling, result shape) without external I/O.\n",
    "import asyncio, time, json\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "async def test_async_functions(max_concurrent: int = 8) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Runs a suite of lightweight async checks.\n",
    "    Returns:\n",
    "        {\n",
    "          \"ok\": bool,\n",
    "          \"elapsed_ms\": float,\n",
    "          \"checks\": {\n",
    "            \"concurrency_limit_ok\": bool,\n",
    "            \"cancellation_ok\": bool,\n",
    "            \"timeout_ok\": bool,\n",
    "            \"wrapper_shape_ok\": bool\n",
    "          },\n",
    "          \"details\": {...}\n",
    "        }\n",
    "    \"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    checks = {\"concurrency_limit_ok\": False, \"cancellation_ok\": False, \"timeout_ok\": False, \"wrapper_shape_ok\": False}\n",
    "    details: Dict[str, Any] = {}\n",
    "    try:\n",
    "        # ---- A) Concurrency-cap test using local evaluator ----\n",
    "        current = 0\n",
    "        peak = 0\n",
    "        lock = asyncio.Lock()\n",
    "        sem = asyncio.Semaphore(max_concurrent if max_concurrent and max_concurrent > 0 else 8)\n",
    "\n",
    "        async def local_evaluate_async(domain: str, prob: float) -> Dict[str, Any]:\n",
    "            nonlocal current, peak\n",
    "            async with sem:\n",
    "                async with lock:\n",
    "                    current += 1\n",
    "                    peak = max(peak, current)\n",
    "                try:\n",
    "                    await asyncio.sleep(0.003)  # emulate small work\n",
    "                    score = min(1.0, 0.3 * float(prob or 0.0))\n",
    "                    return {\n",
    "                        \"domain\": domain, \"ml_probability\": prob,\n",
    "                        \"is_phishing\": score >= 0.5, \"confidence\": score,\n",
    "                        \"risk_level\": \"high\" if score>=0.75 else \"medium\" if score>=0.5 else \"low\",\n",
    "                        \"risk_score\": score, \"final_label\": \"phishing\" if score>=0.5 else \"benign\",\n",
    "                        \"reasoning\": \"local-eval (mock)\"\n",
    "                    }\n",
    "                finally:\n",
    "                    async with lock:\n",
    "                        current -= 1\n",
    "\n",
    "        domains = [f\"example-{i}.test\" for i in range(25)]\n",
    "        probs = [i/25.0 for i in range(25)]\n",
    "        tasks = [asyncio.create_task(local_evaluate_async(d, p)) for d, p in zip(domains, probs)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        checks[\"concurrency_limit_ok\"] = (peak <= (max_concurrent if max_concurrent and max_concurrent>0 else 8))\n",
    "        details[\"peak_concurrency\"] = peak\n",
    "\n",
    "        # ---- B) Cancellation handling ----\n",
    "        async def long_sleep():\n",
    "            await asyncio.sleep(5)\n",
    "            return \"done\"\n",
    "        t = asyncio.create_task(long_sleep())\n",
    "        await asyncio.sleep(0.005)\n",
    "        t.cancel()\n",
    "        try:\n",
    "            await t\n",
    "            cancellation_ok = False\n",
    "        except asyncio.CancelledError:\n",
    "            cancellation_ok = True\n",
    "        checks[\"cancellation_ok\"] = cancellation_ok\n",
    "\n",
    "        # ---- C) Timeout handling ----\n",
    "        async def slightly_long():\n",
    "            await asyncio.sleep(0.02)\n",
    "            return \"ok\"\n",
    "        try:\n",
    "            await asyncio.wait_for(slightly_long(), timeout=0.005)\n",
    "            timeout_ok = False\n",
    "        except asyncio.TimeoutError:\n",
    "            timeout_ok = True\n",
    "        checks[\"timeout_ok\"] = timeout_ok\n",
    "\n",
    "        # ---- D) Wrapper-shape check (prefer real evaluate_async if present) ----\n",
    "        async def _probe_wrapper(domain: str, p: float) -> Dict[str, Any]:\n",
    "            if \"evaluate_async\" in globals():\n",
    "                try:\n",
    "                    # try with a small semaphore if signature accepts it\n",
    "                    try:\n",
    "                        return await globals()[\"evaluate_async\"](domain, p, asyncio.Semaphore(2), 0, None)\n",
    "                    except TypeError:\n",
    "                        return await globals()[\"evaluate_async\"](domain, p)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # fallback to local\n",
    "            return await local_evaluate_async(domain, p)\n",
    "\n",
    "        sample = await _probe_wrapper(\"wrapper-sample.test\", 0.42)\n",
    "        wrapper_shape_keys = {\"domain\",\"ml_probability\",\"is_phishing\",\"confidence\",\"risk_level\",\"risk_score\",\"final_label\",\"reasoning\"}\n",
    "        checks[\"wrapper_shape_ok\"] = wrapper_shape_keys.issubset(set(sample.keys()))\n",
    "        details[\"wrapper_sample\"] = sample\n",
    "\n",
    "        ok = all(checks.values())\n",
    "        elapsed_ms = round((time.perf_counter() - t0) * 1000.0, 3)\n",
    "        return {\"ok\": ok, \"elapsed_ms\": elapsed_ms, \"checks\": checks, \"details\": details}\n",
    "    except Exception as e:\n",
    "        elapsed_ms = round((time.perf_counter() - t0) * 1000.0, 3)\n",
    "        return {\"ok\": False, \"error\": str(e), \"elapsed_ms\": elapsed_ms, \"checks\": checks, \"details\": details}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f89b6789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HF] test_async_functions: {\"ok\": true, \"elapsed_ms\": 30.564, \"checks\": {\"concurrency_limit_ok\": true, \"cancellation_ok\": true, \"timeout_ok\": true, \"wrapper_shape_ok\": true}, \"details\": {\"peak_concurrency\": 6, \"wrapper_sample\": {\"domain\": \"wrapper-sample.test\", \"ml_probability\": 0.42, \"is_phishing\": false, \"confidence\": 0.126, \"risk_level\": \"low\", \"risk_score\": 0.126, \"final_label\": \"benign\", \"reasoning\": \"local-eval (mock)\"}}}\n",
      "[HF] PASS\n"
     ]
    }
   ],
   "source": [
    "# === Cell HF-03B: test_async_functions (self-check) ===\n",
    "import asyncio, json\n",
    "try:\n",
    "    _ = asyncio.get_running_loop()\n",
    "    res = await test_async_functions(max_concurrent=6)\n",
    "except RuntimeError:\n",
    "    res = asyncio.run(test_async_functions(max_concurrent=6))\n",
    "\n",
    "print(\"[HF] test_async_functions:\", json.dumps(res, ensure_ascii=False))\n",
    "assert isinstance(res, dict) and res.get(\"ok\") is True, \"test_async_functions failed\"\n",
    "print(\"[HF] PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e944db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell HF-04A: quick_test_process_domains (restored minimal) ===\n",
    "import pandas as pd, random, math\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "def quick_test_process_domains(max_items: int = 10,\n",
    "                               mock: bool = True,\n",
    "                               seed: int = 42,\n",
    "                               max_concurrent: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimal E2E smoke test for batch domain evaluation.\n",
    "    - Prefers 04-4's process_domains_batch_fixed / process_domains_batch\n",
    "    - Falls back to local stub if not available (no external I/O)\n",
    "    Returns: pd.DataFrame with columns at least\n",
    "      ['domain','ml_probability','is_phishing','confidence','risk_level','risk_score','final_label','reasoning']\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    samples = [\n",
    "        (\"paypal-login.example\",            0.91),\n",
    "        (\"docs-google-secure-auth.example\", 0.76),\n",
    "        (\"benign-example.org\",              0.05),\n",
    "        (\"my-ledger-secure.com\",            0.16),\n",
    "        (\"secure-verification-center.top\",  0.67),\n",
    "        (\"update-billing-info.click\",       0.55),\n",
    "        (\"support-wallet-verify.xyz\",       0.73),\n",
    "        (\"images.cdn-static.net\",           0.09),\n",
    "        (\"account-security-alert.work\",     0.48),\n",
    "        (\"intranet.localdomain\",            0.02),\n",
    "    ]\n",
    "    if max_items and len(samples) > max_items:\n",
    "        samples = samples[:max_items]\n",
    "\n",
    "    domains = [d for d, _ in samples]\n",
    "    probs   = [p for _, p in samples]\n",
    "\n",
    "    # Prefer real batch evaluator from 04-4\n",
    "    g = globals()\n",
    "    evaluator = g.get(\"process_domains_batch_fixed\") or g.get(\"process_domains_batch\")\n",
    "    if callable(evaluator):\n",
    "        try:\n",
    "            rows = evaluator(domains, probs, cfg=None, max_concurrent=max_concurrent, agent=None, mock=mock)\n",
    "            df = pd.DataFrame(rows)\n",
    "        except TypeError:\n",
    "            # older signature\n",
    "            rows = evaluator(domains, probs)\n",
    "            df = pd.DataFrame(rows)\n",
    "    else:\n",
    "        # Local lightweight stub (heuristic, no external I/O)\n",
    "        SUSPICIOUS = {\"secure\",\"login\",\"verify\",\"update\",\"account\",\"wallet\",\"support\",\"confirm\",\"bank\",\"invoice\",\"payment\",\"auth\",\"google\",\"paypal\"}\n",
    "        def _score(domain: str, p: float) -> Dict[str, Any]:\n",
    "            lower = (domain or \"\").lower()\n",
    "            score = 0.0\n",
    "            kw = [k for k in SUSPICIOUS if k in lower]\n",
    "            if kw: score += 0.4 + 0.05 * min(len(kw), 3)\n",
    "            if \"-\" in lower: score += 0.05\n",
    "            if any(ch.isdigit() for ch in lower): score += 0.05\n",
    "            score += float(p or 0.0) * 0.3\n",
    "            score = max(0.0, min(0.99, score))\n",
    "            return {\n",
    "                \"domain\": domain,\n",
    "                \"ml_probability\": float(p or 0.0),\n",
    "                \"is_phishing\": score >= 0.5,\n",
    "                \"confidence\": score,\n",
    "                \"risk_level\": (\"high\" if score>=0.75 else \"medium\" if score>=0.5 else \"low\"),\n",
    "                \"risk_score\": score,\n",
    "                \"final_label\": (\"phishing\" if score>=0.5 else \"benign\"),\n",
    "                \"reasoning\": \"local stub (no external I/O)\",\n",
    "            }\n",
    "        df = pd.DataFrame([_score(d, p) for d, p in samples])\n",
    "\n",
    "    # Ensure minimal columns\n",
    "    must_cols = [\"domain\",\"ml_probability\",\"is_phishing\",\"confidence\",\"risk_level\",\"risk_score\",\"final_label\",\"reasoning\"]\n",
    "    for c in must_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "    return df[must_cols + [c for c in df.columns if c not in must_cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6938aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            domain  ml_probability  is_phishing  confidence  \\\n",
      "0             paypal-login.example            0.91         True       0.823   \n",
      "1  docs-google-secure-auth.example            0.76         True       0.828   \n",
      "2               benign-example.org            0.05        False       0.065   \n",
      "3             my-ledger-secure.com            0.16         True       0.548   \n",
      "4   secure-verification-center.top            0.67         True       0.701   \n",
      "\n",
      "  risk_level  risk_score final_label                     reasoning  \n",
      "0       high       0.823    phishing  local stub (no external I/O)  \n",
      "1       high       0.828    phishing  local stub (no external I/O)  \n",
      "2        low       0.065      benign  local stub (no external I/O)  \n",
      "3     medium       0.548    phishing  local stub (no external I/O)  \n",
      "4     medium       0.701    phishing  local stub (no external I/O)  \n",
      "[HF] PASS\n"
     ]
    }
   ],
   "source": [
    "# === Cell HF-04B: quick_test_process_domains (self-check) ===\n",
    "import pandas as pd\n",
    "df = quick_test_process_domains(max_items=6, mock=True, seed=7, max_concurrent=3)\n",
    "print(df.head())\n",
    "assert isinstance(df, pd.DataFrame) and not df.empty, \"quick_test_process_domains returned empty\"\n",
    "need = {\"domain\",\"ml_probability\",\"is_phishing\",\"confidence\",\"risk_level\",\"risk_score\",\"final_label\",\"reasoning\"}\n",
    "assert need.issubset(set(df.columns)), f\"missing columns: {need - set(df.columns)}\"\n",
    "print(\"[HF] PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77b875bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell HF-05A: run_async_test (restored minimal, sync wrapper) ===\n",
    "import pandas as pd, random\n",
    "from typing import Dict, Any, Optional, List\n",
    "\n",
    "def run_async_test(sample_size: int = 12,\n",
    "                   max_concurrent: int = 6,\n",
    "                   mock: bool = True,\n",
    "                   seed: int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Small end-to-end batch test, provided as a SYNC function (no top-level await required).\n",
    "    Priority:\n",
    "      1) quick_test_process_domains (preferred)\n",
    "      2) process_domains_batch_fixed / process_domains_batch\n",
    "      3) local stub (no external I/O)\n",
    "    Returns: pd.DataFrame with at least the following columns:\n",
    "      ['domain','ml_probability','is_phishing','confidence','risk_level','risk_score','final_label','reasoning']\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    # Preferred path: use quick_test_process_domains if available\n",
    "    if \"quick_test_process_domains\" in globals():\n",
    "        try:\n",
    "            df = globals()[\"quick_test_process_domains\"](max_items=sample_size, mock=mock, seed=seed, max_concurrent=max_concurrent)\n",
    "            return _ensure_columns(df)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback to batch evaluator\n",
    "    evaluator = globals().get(\"process_domains_batch_fixed\") or globals().get(\"process_domains_batch\")\n",
    "    if callable(evaluator):\n",
    "        samples = _make_default_samples(sample_size)\n",
    "        domains = [d for d, _ in samples]\n",
    "        probs   = [p for _, p in samples]\n",
    "        try:\n",
    "            rows = evaluator(domains, probs, cfg=None, max_concurrent=max_concurrent, agent=None, mock=mock)\n",
    "        except TypeError:\n",
    "            rows = evaluator(domains, probs)\n",
    "        df = pd.DataFrame(rows)\n",
    "        return _ensure_columns(df)\n",
    "\n",
    "    # Local stub as last resort\n",
    "    samples = _make_default_samples(sample_size)\n",
    "    def _score(domain: str, p: float) -> Dict[str, Any]:\n",
    "        lower = (domain or \"\").lower()\n",
    "        score = 0.0\n",
    "        SUSPICIOUS = {\"secure\",\"login\",\"verify\",\"update\",\"account\",\"wallet\",\"support\",\"confirm\",\"bank\",\"invoice\",\"payment\",\"auth\",\"google\",\"paypal\"}\n",
    "        kw = [k for k in SUSPICIOUS if k in lower]\n",
    "        if kw: score += 0.4 + 0.05 * min(len(kw), 3)\n",
    "        if \"-\" in lower: score += 0.05\n",
    "        if any(ch.isdigit() for ch in lower): score += 0.05\n",
    "        score += float(p or 0.0) * 0.3\n",
    "        score = max(0.0, min(0.99, score))\n",
    "        return {\n",
    "            \"domain\": domain,\n",
    "            \"ml_probability\": float(p or 0.0),\n",
    "            \"is_phishing\": score >= 0.5,\n",
    "            \"confidence\": score,\n",
    "            \"risk_level\": (\"high\" if score>=0.75 else \"medium\" if score>=0.5 else \"low\"),\n",
    "            \"risk_score\": score,\n",
    "            \"final_label\": (\"phishing\" if score>=0.5 else \"benign\"),\n",
    "            \"reasoning\": \"local stub (no external I/O)\",\n",
    "        }\n",
    "    df = pd.DataFrame([_score(d, p) for d, p in samples])\n",
    "    return _ensure_columns(df)\n",
    "\n",
    "def _make_default_samples(n: int):\n",
    "    base = [\n",
    "        (\"paypal-login.example\",            0.91),\n",
    "        (\"docs-google-secure-auth.example\", 0.76),\n",
    "        (\"benign-example.org\",              0.05),\n",
    "        (\"my-ledger-secure.com\",            0.16),\n",
    "        (\"secure-verification-center.top\",  0.67),\n",
    "        (\"update-billing-info.click\",       0.55),\n",
    "        (\"support-wallet-verify.xyz\",       0.73),\n",
    "        (\"images.cdn-static.net\",           0.09),\n",
    "        (\"account-security-alert.work\",     0.48),\n",
    "        (\"intranet.localdomain\",            0.02),\n",
    "        (\"apple-id-auth.center\",            0.62),\n",
    "        (\"billing-confirm-pay.co\",          0.58),\n",
    "        (\"bank-update-info.net\",            0.64),\n",
    "        (\"secure-docs-storage.site\",        0.42),\n",
    "        (\"help-center-support.app\",         0.28),\n",
    "    ]\n",
    "    return base[:max(1, min(n, len(base)))]\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    must = [\"domain\",\"ml_probability\",\"is_phishing\",\"confidence\",\"risk_level\",\"risk_score\",\"final_label\",\"reasoning\"]\n",
    "    out = df.copy()\n",
    "    for c in must:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    return out[must + [c for c in out.columns if c not in must]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aa2b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            domain  ml_probability  is_phishing  confidence  \\\n",
      "0             paypal-login.example            0.91         True       0.823   \n",
      "1  docs-google-secure-auth.example            0.76         True       0.828   \n",
      "2               benign-example.org            0.05        False       0.065   \n",
      "3             my-ledger-secure.com            0.16         True       0.548   \n",
      "4   secure-verification-center.top            0.67         True       0.701   \n",
      "\n",
      "  risk_level  risk_score final_label                     reasoning  \n",
      "0       high       0.823    phishing  local stub (no external I/O)  \n",
      "1       high       0.828    phishing  local stub (no external I/O)  \n",
      "2        low       0.065      benign  local stub (no external I/O)  \n",
      "3     medium       0.548    phishing  local stub (no external I/O)  \n",
      "4     medium       0.701    phishing  local stub (no external I/O)  \n",
      "[HF] PASS\n"
     ]
    }
   ],
   "source": [
    "# === Cell HF-05B: run_async_test (self-check) ===\n",
    "import pandas as pd\n",
    "df = run_async_test(sample_size=8, max_concurrent=4, mock=True, seed=11)\n",
    "print(df.head())\n",
    "assert isinstance(df, pd.DataFrame) and not df.empty, \"run_async_test returned empty\"\n",
    "need = {\"domain\",\"ml_probability\",\"is_phishing\",\"confidence\",\"risk_level\",\"risk_score\",\"final_label\",\"reasoning\"}\n",
    "assert need.issubset(set(df.columns)), f\"missing columns: {need - set(df.columns)}\"\n",
    "print(\"[HF] PASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40bcc6-61a3-4bd4-b1c7-b943cfa2535b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
