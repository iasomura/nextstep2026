{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9ce7ee",
   "metadata": {},
   "source": [
    "# 03_ai_agent_analysis_part1_fixed â€” Config & API æ‹¡å¼µï¼ˆçµ±åˆç‰ˆï¼‰\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**å…ƒã‚³ãƒ¼ãƒ‰ã‚’æ¸©å­˜**ã—ãŸã¾ã¾ã€ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’**è¿½è¨˜**ã—ã¦æ©Ÿèƒ½æ‹¡å¼µã—ã¾ã™ï¼š\n",
    "- ã‚»ãƒ«0: Configèª­ã¿è¾¼ã¿ã¨ãƒãƒ¼ã‚¸ (`load_configuration`)\n",
    "- ã‚»ãƒ«3(ãƒ‘ãƒƒãƒ): DB/LLMè¨­å®šã®Configåæ˜  + GPUè‡ªå‹•æ¤œå‡º + vLLMâ†’Ollamaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "- ã‚»ãƒ«X: å½é™°æ€§åˆ†æã®é–¾å€¤ã‚’Configã‹ã‚‰å–å¾—ï¼ˆä½ç¢ºç‡é ˜åŸŸåˆ†æã¯å¿…ãšå®Ÿè¡Œï¼‰\n",
    "- æœ€çµ‚ã‚»ãƒ«: Controller API (`agent_minimal`)\n",
    "â€» å…ƒã‚»ãƒ«ã¯å¤‰æ›´ãƒ»å‰Šé™¤ã›ãšã€**å®Ÿè¡Œé †åºã§ä¸Šæ›¸ãé©ç”¨**ã—ã¾ã™ï¼ˆå‰¯ä½œç”¨å›é¿ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a064f7-7b70-473f-a396-9a2200a17578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T15:41:44.739580Z",
     "iopub.status.busy": "2025-10-29T15:41:44.738947Z",
     "iopub.status.idle": "2025-10-29T15:41:44.877660Z",
     "shell.execute_reply": "2025-10-29T15:41:44.876994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NX] RUN_ID = 20251030_004055 | paths.RUN_ID = 20251030_004055\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0 (02ä»¥é™ å…±é€š): ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰è§£æ±ºã—ã¦ paths ã‚’èª­ã‚€ ===\n",
    "import run_id_registry as runreg\n",
    "rid = runreg.bootstrap()  # envâ†’ãƒ•ã‚¡ã‚¤ãƒ«(artifacts/_current/run_id.txt)â†’Part3â†’latestâ†’æ–°è¦ ã®é †ã§è§£æ±º\n",
    "\n",
    "import importlib\n",
    "import _compat.paths as paths\n",
    "importlib.reload(paths)\n",
    "importlib.reload(paths)\n",
    "print(\"[NX] RUN_ID =\", rid, \"| paths.RUN_ID =\", paths.RUN_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a8c1e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T15:41:44.880396Z",
     "iopub.status.busy": "2025-10-29T15:41:44.880193Z",
     "iopub.status.idle": "2025-10-29T15:41:44.893147Z",
     "shell.execute_reply": "2025-10-29T15:41:44.892350Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === ã‚»ãƒ«0: Configèª­ã¿è¾¼ã¿ã¨ãƒãƒ¼ã‚¸ (è¿½è¨˜) ===\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import yaml  # optional\n",
    "except Exception:\n",
    "    yaml = None\n",
    "\n",
    "def _deep_update(base: dict, override: dict) -> dict:\n",
    "    base = dict(base or {})\n",
    "    for k, v in (override or {}).items():\n",
    "        if isinstance(v, dict) and isinstance(base.get(k), dict):\n",
    "            base[k] = _deep_update(base[k], v)\n",
    "        else:\n",
    "            base[k] = v\n",
    "    return base\n",
    "\n",
    "def load_configuration(\n",
    "    config_path: Optional[str] = None,\n",
    "    cfg_override: Optional[Dict[str, Any]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    defaults = {\n",
    "        \"system\": {\n",
    "            \"cert_only_mode\": False,\n",
    "            \"enable_brand_keywords\": True,\n",
    "            \"gpu_auto_detect\": True,\n",
    "            \"development_mode\": False,\n",
    "            \"seed\": 42,\n",
    "        },\n",
    "        \"llm\": {\n",
    "            \"provider\": \"vllm\",  # \"vllm\" | \"ollama\"\n",
    "            \"vllm_base_url\": \"http://192.168.100.71:30000/v1\",\n",
    "            \"vllm_model\": \"Qwen/Qwen3-14B-FP8\",\n",
    "            \"ollama_base_url\": \"http://localhost:11434/v1\",\n",
    "            \"ollama_model\": \"qwen3:14b\",\n",
    "            \"fallback_enabled\": True,\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 2000,\n",
    "        },\n",
    "        \"db\": {\n",
    "            \"dbname\": \"rapids_data\",\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"asomura\",\n",
    "            \"host\": \"localhost\",\n",
    "            \"port\": \"5432\",\n",
    "            \"timeout_s\": 30,\n",
    "            \"read_only\": True,\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"fn_threshold_low\": 0.2,\n",
    "            \"fn_threshold_high\": 0.7,\n",
    "            \"enable_detailed_stats\": True,\n",
    "        },\n",
    "    }\n",
    "    cfg = dict(defaults)\n",
    "\n",
    "    # 2) config file (JSON/YAML) if exists\n",
    "    def _read_config_file(p: str) -> dict:\n",
    "        p = str(p)\n",
    "        if not p or not os.path.exists(p):\n",
    "            return {}\n",
    "        try:\n",
    "            text = Path(p).read_text(encoding=\"utf-8\")\n",
    "            if p.endswith((\".yml\",\".yaml\")) and yaml is not None:\n",
    "                return yaml.safe_load(text) or {}\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã«å¤±æ•—: {p} ({e})\")\n",
    "            return {}\n",
    "    if config_path:\n",
    "        cfg = _deep_update(cfg, _read_config_file(config_path))\n",
    "\n",
    "    # 3) env vars\n",
    "    env_map = {\n",
    "        \"DEV_MODE\": (\"system\", \"development_mode\"),\n",
    "        \"LLM_TYPE\": (\"llm\", \"provider\"),\n",
    "        \"VLLM_BASE_URL\": (\"llm\", \"vllm_base_url\"),\n",
    "        \"PGDATABASE\": (\"db\", \"dbname\"),\n",
    "        \"PGUSER\": (\"db\", \"user\"),\n",
    "        \"PGPASSWORD\": (\"db\", \"password\"),\n",
    "    }\n",
    "    for env, path in env_map.items():\n",
    "        if env in os.environ and os.environ[env] != \"\":\n",
    "            val = os.environ[env]\n",
    "            low = val.lower()\n",
    "            if low in (\"true\",\"false\"):\n",
    "                val = (low == \"true\")\n",
    "            node = cfg\n",
    "            for key in path[:-1]:\n",
    "                node = node.setdefault(key, {})\n",
    "            node[path[-1]] = val\n",
    "\n",
    "    # 4) override\n",
    "    if cfg_override:\n",
    "        cfg = _deep_update(cfg, cfg_override)\n",
    "\n",
    "    # 5) validation & warnings\n",
    "    if cfg[\"system\"].get(\"cert_only_mode\", False):\n",
    "        print(\"âš ï¸ æ³¨æ„: cert_only_mode=True ã§ã™ãŒã€ãƒ–ãƒ©ãƒ³ãƒ‰å½è£…æ¤œå‡ºã¯ç ”ç©¶ã®æ ¸å¿ƒæ©Ÿèƒ½ã®ãŸã‚å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "        cfg[\"system\"][\"enable_brand_keywords\"] = True\n",
    "\n",
    "    provider = str(cfg[\"llm\"].get(\"provider\",\"vllm\")).lower()\n",
    "    if provider not in (\"vllm\",\"ollama\"):\n",
    "        print(f\"âš ï¸ æœªçŸ¥ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{provider}' -> 'vllm' ã«çŸ¯æ­£ã—ã¾ã™\")\n",
    "        cfg[\"llm\"][\"provider\"] = \"vllm\"\n",
    "\n",
    "    for req in (\"dbname\",\"user\",\"host\",\"port\"):\n",
    "        if not str(cfg[\"db\"].get(req,\"\")).strip():\n",
    "            raise ValueError(f\"DBæ¥ç¶šæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™: db.{req}\")\n",
    "\n",
    "    globals()[\"cfg\"] = cfg\n",
    "    print(\"âœ… Config loaded (provider={provider}, gpu_auto_detect={gpu})\".format(\n",
    "        provider=cfg[\"llm\"][\"provider\"],\n",
    "        gpu=cfg[\"system\"][\"gpu_auto_detect\"],\n",
    "    ))\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cce460a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T15:41:44.896836Z",
     "iopub.status.busy": "2025-10-29T15:41:44.896467Z",
     "iopub.status.idle": "2025-10-29T15:41:45.715599Z",
     "shell.execute_reply": "2025-10-29T15:41:45.714196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… IO guard ready -> artifacts/20251030_004055/handoff\n",
      "âœ… 02ä»•æ§˜ã«åŸºã¥ãæœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è‡ªå‹•é¸æŠ:\n",
      "  session_id  : 2025-10-22_082121\n",
      "  model_path  : models/2025-10-22_082121/xgboost_model.pkl\n",
      "  scaler_path : models/2025-10-22_082121/scaler.pkl\n",
      "  feature_order_path : models/2025-10-22_082121/feature_order.json \n",
      "  brand_keywords_path: models/2025-10-22_082121/brand_keywords.json \n",
      "  results FN pkl: results/2025-10-22_082121/false_negatives.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ãƒ­ãƒ¼ã‚«ãƒ«GPUæ¤œå‡º: XGBoostãƒ¢ãƒ‡ãƒ«ã§GPUåˆ©ç”¨å¯èƒ½\n",
      "\n",
      "ğŸ¤– LLMè¨­å®š:\n",
      "  - LLMå®Ÿè¡Œç’°å¢ƒ: ãƒªãƒ¢ãƒ¼ãƒˆvLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆhttp://192.168.100.71:30000/v1ï¼‰\n",
      "  - LLMãƒ¢ãƒ‡ãƒ«: Qwen/Qwen3-14B-FP8ï¼ˆ14Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\n",
      "  - æ³¨: LLMã¯ãƒªãƒ¢ãƒ¼ãƒˆã‚µãƒ¼ãƒãƒ¼ã®GPUã§å®Ÿè¡Œã•ã‚Œã¾ã™\n",
      "\n",
      "ğŸ”§ ç’°å¢ƒè¨­å®šã¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
      "================================================================================\n",
      "âœ… é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: False\n",
      "âœ… models ã‹ã‚‰ 02å½¢å¼ã®æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º: 2025-10-22_082121\n",
      "  model_path : models/2025-10-22_082121/xgboost_model.pkl\n",
      "  scaler_path: models/2025-10-22_082121/scaler.pkl\n",
      "  feature_order_path : models/2025-10-22_082121/feature_order.json \n",
      "  brand_keywords_path: models/2025-10-22_082121/brand_keywords.json \n",
      "âœ… XGBoostãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: models/2025-10-22_082121/xgboost_model.pkl\n",
      "âœ… XGBoostãƒ¢ãƒ‡ãƒ«: ãƒ­ãƒ¼ã‚«ãƒ«GPUã‚’ä½¿ç”¨\n",
      "âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†: models/2025-10-22_082121/scaler.pkl\n",
      "âœ… ç‰¹å¾´é‡é †åºèª­ã¿è¾¼ã¿å®Œäº†: 20å€‹ã®ç‰¹å¾´é‡\n",
      "âœ… ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿å®Œäº†: 100å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰\n",
      "   ãƒ–ãƒ©ãƒ³ãƒ‰ä¾‹: ['allegro', 'facebook', 'microsoft', 'at&t', 'adobe']\n",
      "âœ… å½é™°æ€§ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: results/2025-10-22_082121/false_negatives.pkl\n",
      "\n",
      "ğŸ“Š å½é™°æ€§ï¼ˆFNï¼‰ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ:\n",
      "  - ç·FNæ•°: 4,132ä»¶\n",
      "  - å¹³å‡äºˆæ¸¬ç¢ºç‡: 0.2086\n",
      "  - ä¸­å¤®å€¤: 0.1762\n",
      "  - æœ€å°äºˆæ¸¬ç¢ºç‡: 0.0005\n",
      "  - æœ€å¤§äºˆæ¸¬ç¢ºç‡: 0.4998\n",
      "\n",
      "ğŸ¯ ä½ç¢ºç‡é ˜åŸŸã®FNåˆ†å¸ƒï¼ˆå•é¡Œã®æ ¸å¿ƒï¼‰:\n",
      "  - MLç¢ºç‡ < 0.2: 2,241ä»¶ (54.2%)\n",
      "  - MLç¢ºç‡ 0.2-0.4: 1,315ä»¶ (31.8%)\n",
      "  - MLç¢ºç‡ >= 0.7: 0ä»¶ (0.0%)\n",
      "\n",
      "ã‚½ãƒ¼ã‚¹åˆ¥å†…è¨³:\n",
      "  - jpcert: 1,980ä»¶ (47.9%)\n",
      "  - certificates: 1,112ä»¶ (26.9%)\n",
      "  - phishtank: 1,040ä»¶ (25.2%)\n",
      "âœ… resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: results/ai_agent_qwen3/\n",
      "âœ… logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: logs/ai_agent_qwen3/\n",
      "âœ… analysisãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: analysis/ai_agent_qwen3/\n",
      "\n",
      "âœ… åˆæœŸè¨­å®šå®Œäº†\n",
      "  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: 2025-10-22_082121\n",
      "  - LLMå®Ÿè¡Œ: ãƒªãƒ¢ãƒ¼ãƒˆvLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆGPUä½¿ç”¨ï¼‰\n",
      "  - LLMãƒ¢ãƒ‡ãƒ«: Qwen/Qwen3-14B-FP8\n",
      "  - XGBoost: ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆGPUä½¿ç”¨ï¼‰\n",
      "  - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: 20251030_004145\n",
      "  - å½é™°æ€§ä»¶æ•°: 4132\n",
      "  - ãƒ–ãƒ©ãƒ³ãƒ‰æ•°: 100\n",
      "================================================================================\n",
      "\n",
      "ğŸ”§ å¤‰æ•°åã®çµ±ä¸€å®Œäº†:\n",
      "  - ai_session_id: 2025-10-22_082121\n",
      "  - brand_keywords: 100å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’åˆ©ç”¨å¯èƒ½\n",
      "  - LLM_TYPE: vllm\n",
      "  - DEVELOPMENT_MODE: False\n",
      "âœ… Saved handoff â†’ artifacts/20251030_004055/handoff/03_ai_agent_analysis_part1.pkl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Part 1 â€” MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "# \n",
    "# â€»æœ¬ãƒ‘ãƒ¼ãƒˆã¯åŸæ–‡ã®ã‚»ãƒ«ã‚’**ä¸€å­—ä¸€å¥å¤‰æ›´ã›ãš**åéŒ²ã—ã¦ã„ã¾ã™ã€‚è¿½åŠ ã¯ã“ã®è¦‹å‡ºã—ã¨å…¥å‡ºåŠ›ã‚¹ãƒ­ãƒƒãƒˆã®ã¿ã§ã™ã€‚\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# === IO PATHS guard (define RUN_ID, HANDOFF_DIR, etc.) ===\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "RUN_ID = os.environ.get(\"RUN_ID\") or datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "ARTIFACTS = Path(\"artifacts\") / RUN_ID\n",
    "RAW = ARTIFACTS / \"raw\"; PROCESSED = ARTIFACTS / \"processed\"; MODELS = ARTIFACTS / \"models\"\n",
    "RESULTS = ARTIFACTS / \"results\"; HANDOFF = ARTIFACTS / \"handoff\"; LOGS = ARTIFACTS / \"logs\"; TRACES = ARTIFACTS / \"traces\"\n",
    "for _p in [RAW, PROCESSED, MODELS, RESULTS, HANDOFF, LOGS, TRACES]:\n",
    "    _p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# æ–‡å­—åˆ—ãƒ‘ã‚¹ï¼ˆäº’æ›ï¼‰\n",
    "RAW_DIR=str(RAW); PROCESSED_DIR=str(PROCESSED); MODELS_DIR=str(MODELS)\n",
    "RESULTS_DIR=str(RESULTS); HANDOFF_DIR=str(HANDOFF); LOGS_DIR=str(LOGS); TRACES_DIR=str(TRACES)\n",
    "\n",
    "print(\"âœ… IO guard ready ->\", HANDOFF_DIR)\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# === 02ã®ä»•æ§˜ã«åŸºã¥ãã€Œæœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³è‡ªå‹•é¸æŠã€ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆè¿½åŠ ã‚»ãƒ«ï¼‰ ===\n",
    "from pathlib import Path\n",
    "import re, json\n",
    "from datetime import datetime\n",
    "\n",
    "def _parse_session_id(name: str):\n",
    "    # 02ã¯ \"%Y-%m-%d_%H%M%S\" å½¢å¼ã‚’æ¡ç”¨\n",
    "    m = re.match(r\"^(\\d{4})-(\\d{2})-(\\d{2})_(\\d{6})$\", name)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(name, \"%Y-%m-%d_%H%M%S\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _exists_all(sess_dir: Path):\n",
    "    # 02ã®æˆæœç‰©ï¼ˆæœ€ä½é™ï¼‰\n",
    "    need = [\"xgboost_model.pkl\", \"scaler.pkl\"]\n",
    "    missing = [n for n in need if not (sess_dir / n).exists()]\n",
    "    return (len(missing) == 0), missing\n",
    "\n",
    "models_root = Path(\"models\")\n",
    "if not models_root.exists():\n",
    "    raise FileNotFoundError(\"models/ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã¾ãš 02_xgboost_training_evaluation.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "candidates = []\n",
    "for d in models_root.glob(\"*\"):\n",
    "    if d.is_dir():\n",
    "        ts = _parse_session_id(d.name)\n",
    "        if ts:\n",
    "            ok, missing = _exists_all(d)\n",
    "            if ok:\n",
    "                # xgboost_model.pkl ã® mtime ã‚‚å‚è€ƒã«\n",
    "                model_pkl = d / \"xgboost_model.pkl\"\n",
    "                candidates.append((ts, model_pkl.stat().st_mtime, d))\n",
    "\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(\"02ã®ä»•æ§˜ã«åˆè‡´ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\\n\"\n",
    "                            \"models/YYYY-MM-DD_HHMMSS/{xgboost_model.pkl, scaler.pkl} ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# æ—¥æ™‚â†’mtime ã®å„ªå…ˆé †ã§æœ€æ–°ã‚’é¸æŠ\n",
    "candidates.sort(key=lambda t: (t[0], t[1]))\n",
    "latest_dir = candidates[-1][2]\n",
    "session_id = latest_dir.name  # ã“ã“ã§ 03 ã®å¾Œç¶šã‚»ãƒ«ãŒä½¿ã† session_id ã‚’ä¸Šæ›¸ã\n",
    "\n",
    "# 03 ãŒå‚ç…§ã™ã‚‹ã§ã‚ã‚ã†ãƒ‘ã‚¹ã‚’ä¸Šæ›¸ãå®šç¾©ï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¯ç·©ãã€‚ç„¡ã„ã‚‚ã®ã¯å¾Œæ®µã®åŸæ–‡ãƒ­ã‚¸ãƒƒã‚¯ã«å§”ã­ã‚‹ï¼‰\n",
    "model_path = str(latest_dir / \"xgboost_model.pkl\")\n",
    "scaler_path = str(latest_dir / \"scaler.pkl\")\n",
    "feature_order_path = str(latest_dir / \"feature_order.json\")  # ã‚ã‚‹å ´åˆæƒ³å®š\n",
    "brand_keywords_path = str(latest_dir / \"brand_keywords.json\")  # ã‚ã‚‹å ´åˆæƒ³å®š\n",
    "\n",
    "print(\"âœ… 02ä»•æ§˜ã«åŸºã¥ãæœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è‡ªå‹•é¸æŠ:\")\n",
    "print(\"  session_id  :\", session_id)\n",
    "print(\"  model_path  :\", model_path)\n",
    "print(\"  scaler_path :\", scaler_path)\n",
    "print(\"  feature_order_path :\", feature_order_path, (\"(å­˜åœ¨ã—ã¾ã›ã‚“)\" if not Path(feature_order_path).exists() else \"\"))\n",
    "print(\"  brand_keywords_path:\", brand_keywords_path, (\"(å­˜åœ¨ã—ã¾ã›ã‚“)\" if not Path(brand_keywords_path).exists() else \"\"))\n",
    "\n",
    "# å‚è€ƒ: å½é™°æ€§ãƒ”ã‚¯ãƒ«ï¼ˆå¿…è¦ãªã‚‰å¾Œæ®µã§å‚ç…§ï¼‰\n",
    "results_pkl = Path(\"results\") / session_id / \"false_negatives.pkl\"\n",
    "if results_pkl.exists():\n",
    "    print(\"  results FN pkl:\", results_pkl)\n",
    "else:\n",
    "    print(\"  results FN pkl: (è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)\")\n",
    "\n",
    "# ä»¥é™ã¯ 03 ã®åŸæ–‡ã‚»ãƒ«ï¼ˆjoblib.load ç­‰ï¼‰ã‚’ãã®ã¾ã¾å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒ å: 03_ai_agent_analysis.ipynb\n",
    "æ¦‚è¦: MLãƒ¢ãƒ‡ãƒ«ãŒè¦‹é€ƒã—ãŸãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã‚µã‚¤ãƒˆã‚’AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆQwen3-14Bï¼‰ã§æ¤œå‡º\n",
    "ç›®çš„: å›½éš›å­¦ä¼šè«–æ–‡ã§ã®ç™ºè¡¨ï¼ˆMLã¨LLMã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè¨¼ï¼‰\n",
    "ä½œæˆæ—¥: 2025-01-29\n",
    "\n",
    "é‡è¦ãªæ”¹å–„ç‚¹:\n",
    "- ä½ç¢ºç‡é ˜åŸŸï¼ˆML < 0.2ï¼‰ã«FNã®52%ãŒå­˜åœ¨ã™ã‚‹å•é¡Œã«å¯¾å¿œ\n",
    "- å‹•çš„ãƒ–ãƒ©ãƒ³ãƒ‰ãƒªã‚¹ãƒˆï¼ˆ62å€‹ï¼‰ã‚’æ´»ç”¨ã—ãŸæ¤œå‡º\n",
    "- LangGraphã¨Structured Outputã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’é©ç”¨\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ã‚»ãƒ«ç•ªå·: 01\n",
    "æ¦‚è¦: ç’°å¢ƒè¨­å®šã¨MLãƒ¢ãƒ‡ãƒ«çµæœã®èª­ã¿è¾¼ã¿ï¼ˆå½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã®å†æ§‹ç¯‰ã‚’å«ã‚€ï¼‰\n",
    "å…¥åŠ›: 02_xgboost_training_evaluation.pyã®å‡ºåŠ›ï¼ˆãƒ¢ãƒ‡ãƒ«ã€å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã€ãƒ–ãƒ©ãƒ³ãƒ‰ãƒªã‚¹ãƒˆï¼‰\n",
    "å‡ºåŠ›: model, scaler, false_negatives_df, brand_keywordsç­‰ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import joblib  # 02_xgboost_training_evaluation.pyã¯joblibã§ä¿å­˜\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "# XGBoostã®GPUè­¦å‘Šã‚’æŠ‘åˆ¶\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# ãƒ­ãƒ¼ã‚«ãƒ«GPUã®ç¢ºèªï¼ˆXGBoostãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰\n",
    "GPU_AVAILABLE = True\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… ãƒ­ãƒ¼ã‚«ãƒ«GPUæ¤œå‡º: XGBoostãƒ¢ãƒ‡ãƒ«ã§GPUåˆ©ç”¨å¯èƒ½\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«GPUæœªæ¤œå‡º: XGBoostã¯CPUå‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™\")\n",
    "        GPU_AVAILABLE = False\n",
    "except:\n",
    "    print(\"âš ï¸ nvidia-smiåˆ©ç”¨ä¸å¯: XGBoostã¯CPUå‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™\")\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "# LLMè¨­å®šï¼ˆãƒªãƒ¢ãƒ¼ãƒˆvLLMã‚µãƒ¼ãƒãƒ¼ã‚’ä½¿ç”¨ï¼‰\n",
    "print(\"\\nğŸ¤– LLMè¨­å®š:\")\n",
    "LLM_TYPE = \"vllm\"  # \"ollama\" or \"vllm\"\n",
    "VLLM_BASE_URL = \"http://192.168.100.71:30000/v1\"  # ãƒªãƒ¢ãƒ¼ãƒˆGPUã‚µãƒ¼ãƒãƒ¼\n",
    "VLLM_MODEL = \"Qwen/Qwen3-14B-FP8\"  # 14Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆã‚µãƒ¼ãƒãƒ¼å´ã®GPUã§å®Ÿè¡Œï¼‰\n",
    "OLLAMA_MODEL = 'qwen3:14b'  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨\n",
    "\n",
    "print(f\"  - LLMå®Ÿè¡Œç’°å¢ƒ: ãƒªãƒ¢ãƒ¼ãƒˆvLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆ{VLLM_BASE_URL}ï¼‰\")\n",
    "print(f\"  - LLMãƒ¢ãƒ‡ãƒ«: {VLLM_MODEL}ï¼ˆ14Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\")\n",
    "print(f\"  - æ³¨: LLMã¯ãƒªãƒ¢ãƒ¼ãƒˆã‚µãƒ¼ãƒãƒ¼ã®GPUã§å®Ÿè¡Œã•ã‚Œã¾ã™\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š\n",
    "DB_CONFIG = {\n",
    "    'dbname': 'rapids_data',\n",
    "    'user': 'postgres',\n",
    "    'password': 'asomura',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ”§ ç’°å¢ƒè¨­å®šã¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ç¢ºèª\n",
    "DEVELOPMENT_MODE = os.environ.get('DEV_MODE', 'false').lower() == 'true'\n",
    "print(f\"âœ… é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: {DEVELOPMENT_MODE}\")\n",
    "\n",
    "# ===== ï¼ˆå·®ã—æ›¿ãˆï¼‰æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®è‡ªå‹•æ¤œå‡ºï¼š02ã®ä»•æ§˜ï¼ˆ%Y-%m-%d_%H%M%Sï¼‰ã«å¿ å®Ÿ =====\n",
    "session_id = None\n",
    "\n",
    "def _parse_session_id(name: str):\n",
    "    m = re.match(r\"^(\\d{4})-(\\d{2})-(\\d{2})_(\\d{6})$\", name)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(name, \"%Y-%m-%d_%H%M%S\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "models_dir = Path('models')\n",
    "candidates = []\n",
    "\n",
    "# ã¾ãš 02 ã®æ­£è¦å½¢å¼ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¢ç´¢ï¼ˆxgboost_model.pkl ã¨ scaler.pkl ãŒæƒã£ã¦ã„ã‚‹ã‚‚ã®ã®ã¿ï¼‰\n",
    "if models_dir.exists():\n",
    "    for d in models_dir.iterdir():\n",
    "        if d.is_dir():\n",
    "            ts = _parse_session_id(d.name)\n",
    "            if ts and (d / \"xgboost_model.pkl\").exists() and (d / \"scaler.pkl\").exists():\n",
    "                candidates.append((ts, d))\n",
    "\n",
    "if candidates:\n",
    "    candidates.sort(key=lambda x: x[0])\n",
    "    latest_session_dir = candidates[-1][1]\n",
    "    session_id = latest_session_dir.name\n",
    "    print(f\"âœ… models ã‹ã‚‰ 02å½¢å¼ã®æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º: {session_id}\")\n",
    "\n",
    "# äº’æ›ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1ï¼šmodels/session_*ï¼ˆæ—§æµå„€ï¼‰\n",
    "if not session_id and models_dir.exists():\n",
    "    session_dirs = [d for d in models_dir.iterdir() if d.is_dir() and d.name.startswith('session_')]\n",
    "    if session_dirs:\n",
    "        latest_session = max(session_dirs, key=lambda x: x.name)\n",
    "        session_id = latest_session.name\n",
    "        print(f\"âš ï¸ äº’æ›: models/session_* ã‹ã‚‰æ¤œå‡º: {session_id}\")\n",
    "\n",
    "# äº’æ›ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯2ï¼šresults/20*ï¼ˆçµæœãƒ•ã‚©ãƒ«ãƒ€åã‹ã‚‰æ¨å®šï¼‰\n",
    "if not session_id:\n",
    "    result_dirs = sorted(glob.glob('results/20*'))\n",
    "    if result_dirs:\n",
    "        latest_result_dir = result_dirs[-1]\n",
    "        session_id = latest_result_dir.split('/')[-1]\n",
    "        print(f\"âš ï¸ äº’æ›: results ã‹ã‚‰æ¤œå‡º: {session_id}\")\n",
    "\n",
    "if not session_id:\n",
    "    raise ValueError(\"ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚02_xgboost_training_evaluation.pyã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å®šç¾©ï¼ˆä»¥é™ã¯åŸæ–‡ã©ãŠã‚Šã®èª­ã¿è¾¼ã¿ãƒ­ã‚¸ãƒƒã‚¯ã«å§”ã­ã‚‹ï¼‰\n",
    "model_path = f\"models/{session_id}/xgboost_model.pkl\"\n",
    "scaler_path = f\"models/{session_id}/scaler.pkl\"\n",
    "feature_order_path = f\"models/{session_id}/feature_order.json\"\n",
    "brand_keywords_path = f\"models/{session_id}/brand_keywords.json\"\n",
    "\n",
    "print(\"  model_path :\", model_path)\n",
    "print(\"  scaler_path:\", scaler_path)\n",
    "print(\"  feature_order_path :\", feature_order_path, (\"(å­˜åœ¨ã—ã¾ã›ã‚“)\" if not Path(feature_order_path).exists() else \"\"))\n",
    "print(\"  brand_keywords_path:\", brand_keywords_path, (\"(å­˜åœ¨ã—ã¾ã›ã‚“)\" if not Path(brand_keywords_path).exists() else \"\"))\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆjoblibå½¢å¼ï¼‰\n",
    "try:\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"âœ… XGBoostãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "    raise\n",
    "\n",
    "# XGBoostãƒ¢ãƒ‡ãƒ«ã®GPUè¨­å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã€åˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n",
    "if GPU_AVAILABLE:\n",
    "    try:\n",
    "        if hasattr(model, 'set_param'):\n",
    "            model.set_param({'device': 'cuda:0'})\n",
    "        elif hasattr(model, 'get_booster'):\n",
    "            model.get_booster().set_param({'device': 'cuda:0'})\n",
    "            if hasattr(model, 'set_params'):\n",
    "                model.set_params(device='cuda:0', tree_method='hist')\n",
    "        print(f\"âœ… XGBoostãƒ¢ãƒ‡ãƒ«: ãƒ­ãƒ¼ã‚«ãƒ«GPUã‚’ä½¿ç”¨\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ GPUè¨­å®šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"   XGBoostã¯CPUå‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™\")\n",
    "\n",
    "# ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆjoblibå½¢å¼ï¼‰\n",
    "try:\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(f\"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†: {scaler_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {scaler_path}\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "    raise\n",
    "\n",
    "# ç‰¹å¾´é‡é †åºã®èª­ã¿è¾¼ã¿ï¼ˆJSONå½¢å¼ï¼‰\n",
    "try:\n",
    "    with open(feature_order_path, 'r') as f:\n",
    "        feature_order = json.load(f)\n",
    "    print(f\"âœ… ç‰¹å¾´é‡é †åºèª­ã¿è¾¼ã¿å®Œäº†: {len(feature_order)}å€‹ã®ç‰¹å¾´é‡\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ç‰¹å¾´é‡é †åºãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {feature_order_path}\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "    raise\n",
    "\n",
    "# ğŸ”¥ é‡è¦: å‹•çš„ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿ï¼ˆJSONå½¢å¼ï¼‰\n",
    "try:\n",
    "    with open(brand_keywords_path, 'r', encoding='utf-8') as f:\n",
    "        brand_keywords = json.load(f)\n",
    "    print(f\"âœ… ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿å®Œäº†: {len(brand_keywords)}å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰\")\n",
    "    print(f\"   ãƒ–ãƒ©ãƒ³ãƒ‰ä¾‹: {brand_keywords[:5]}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {brand_keywords_path}\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "    print(\"   ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯GPT-4o miniã§å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹é‡è¦ãªãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚\")\n",
    "    raise\n",
    "\n",
    "# å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "false_negatives_df = None\n",
    "\n",
    "# æ–°å½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è©¦ã™\n",
    "fn_paths = [\n",
    "    f\"models/{session_id}/false_negatives_reconstructed.pkl\",\n",
    "    f\"results/{session_id}/false_negatives_reconstructed.pkl\",\n",
    "    f\"results/{session_id}/false_negatives.pkl\"\n",
    "]\n",
    "\n",
    "for fn_path in fn_paths:\n",
    "    if os.path.exists(fn_path):\n",
    "        try:\n",
    "            # joblibã§èª­ã¿è¾¼ã¿ï¼ˆ02_xgboost_training_evaluation.pyã®å½¢å¼ï¼‰\n",
    "            fn_data = joblib.load(fn_path)\n",
    "            \n",
    "            # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®åˆ¤å®š\n",
    "            if isinstance(fn_data, pd.DataFrame):\n",
    "                false_negatives_df = fn_data\n",
    "            elif isinstance(fn_data, dict):\n",
    "                if 'analysis_df' in fn_data:\n",
    "                    false_negatives_df = fn_data['analysis_df']\n",
    "                elif 'domains' in fn_data and 'predictions' in fn_data:\n",
    "                    # æ—§å½¢å¼ã®å ´åˆã¯å†æ§‹ç¯‰\n",
    "                    false_negatives_df = pd.DataFrame({\n",
    "                        'domain': fn_data['domains'],\n",
    "                        'prediction_proba': fn_data['predictions'],\n",
    "                        'source': fn_data.get('sources', ['unknown'] * len(fn_data['domains']))\n",
    "                    })\n",
    "            \n",
    "            if false_negatives_df is not None:\n",
    "                print(f\"âœ… å½é™°æ€§ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {fn_path}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ {fn_path}ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}\")\n",
    "            continue\n",
    "\n",
    "if false_negatives_df is None:\n",
    "    print(\"âŒ å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "    raise FileNotFoundError(\"å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆè¡¨ç¤º\n",
    "print(f\"\\nğŸ“Š å½é™°æ€§ï¼ˆFNï¼‰ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ:\")\n",
    "print(f\"  - ç·FNæ•°: {len(false_negatives_df):,}ä»¶\")\n",
    "\n",
    "if 'prediction_proba' in false_negatives_df.columns:\n",
    "    print(f\"  - å¹³å‡äºˆæ¸¬ç¢ºç‡: {false_negatives_df['prediction_proba'].mean():.4f}\")\n",
    "    print(f\"  - ä¸­å¤®å€¤: {false_negatives_df['prediction_proba'].median():.4f}\")\n",
    "    print(f\"  - æœ€å°äºˆæ¸¬ç¢ºç‡: {false_negatives_df['prediction_proba'].min():.4f}\")\n",
    "    print(f\"  - æœ€å¤§äºˆæ¸¬ç¢ºç‡: {false_negatives_df['prediction_proba'].max():.4f}\")\n",
    "    \n",
    "    # ğŸ”¥ ä½ç¢ºç‡é ˜åŸŸã®FNåˆ†å¸ƒï¼ˆå•é¡Œã®æ ¸å¿ƒï¼‰\n",
    "    fn_low = false_negatives_df[false_negatives_df['prediction_proba'] < 0.2]\n",
    "    fn_mid = false_negatives_df[(false_negatives_df['prediction_proba'] >= 0.2) & \n",
    "                                 (false_negatives_df['prediction_proba'] < 0.4)]\n",
    "    fn_high = false_negatives_df[false_negatives_df['prediction_proba'] >= 0.7]\n",
    "    print(f\"\\nğŸ¯ ä½ç¢ºç‡é ˜åŸŸã®FNåˆ†å¸ƒï¼ˆå•é¡Œã®æ ¸å¿ƒï¼‰:\")\n",
    "    print(f\"  - MLç¢ºç‡ < 0.2: {len(fn_low):,}ä»¶ ({len(fn_low)/len(false_negatives_df)*100:.1f}%)\")\n",
    "    print(f\"  - MLç¢ºç‡ 0.2-0.4: {len(fn_mid):,}ä»¶ ({len(fn_mid)/len(false_negatives_df)*100:.1f}%)\")\n",
    "    print(f\"  - MLç¢ºç‡ >= 0.7: {len(fn_high):,}ä»¶ ({len(fn_high)/len(false_negatives_df)*100:.1f}%)\")\n",
    "\n",
    "# ã‚½ãƒ¼ã‚¹åˆ¥ã®çµ±è¨ˆ\n",
    "if 'source' in false_negatives_df.columns:\n",
    "    print(f\"\\nã‚½ãƒ¼ã‚¹åˆ¥å†…è¨³:\")\n",
    "    source_counts = false_negatives_df['source'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  - {source}: {count:,}ä»¶ ({count/len(false_negatives_df)*100:.1f}%)\")\n",
    "\n",
    "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "output_dirs = {\n",
    "    'results': 'results/ai_agent_qwen3/',\n",
    "    'logs': 'logs/ai_agent_qwen3/',\n",
    "    'analysis': 'analysis/ai_agent_qwen3/'\n",
    "}\n",
    "\n",
    "for dir_name, dir_path in output_dirs.items():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"âœ… {dir_name}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {dir_path}\")\n",
    "\n",
    "# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆå¾Œã®ã‚»ãƒ«ã‹ã‚‰å‚ç…§ï¼‰\n",
    "cert_full_info_map = {}  # ã‚»ãƒ«02ã§ä½œæˆã•ã‚Œã‚‹\n",
    "fn_features_df = None    # ã‚»ãƒ«02ã§ä½œæˆã•ã‚Œã‚‹\n",
    "\n",
    "print(f\"\\nâœ… åˆæœŸè¨­å®šå®Œäº†\")\n",
    "print(f\"  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {session_id}\")\n",
    "print(f\"  - LLMå®Ÿè¡Œ: ãƒªãƒ¢ãƒ¼ãƒˆvLLMã‚µãƒ¼ãƒãƒ¼ï¼ˆGPUä½¿ç”¨ï¼‰\")\n",
    "print(f\"  - LLMãƒ¢ãƒ‡ãƒ«: {VLLM_MODEL if LLM_TYPE == 'vllm' else OLLAMA_MODEL}\")\n",
    "print(f\"  - XGBoost: ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆ{'GPU' if GPU_AVAILABLE else 'CPU'}ä½¿ç”¨ï¼‰\")\n",
    "print(f\"  - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {timestamp}\")\n",
    "print(f\"  - å½é™°æ€§ä»¶æ•°: {len(false_negatives_df)}\")\n",
    "print(f\"  - ãƒ–ãƒ©ãƒ³ãƒ‰æ•°: {len(brand_keywords)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# å¤‰æ•°åã®çµ±ä¸€ï¼ˆã‚»ãƒ«07, 08ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰\n",
    "ai_session_id = session_id\n",
    "globals()['ai_session_id'] = ai_session_id\n",
    "globals()['LLM_TYPE'] = LLM_TYPE\n",
    "globals()['output_dirs'] = output_dirs\n",
    "globals()['DEVELOPMENT_MODE'] = DEVELOPMENT_MODE\n",
    "globals()['brand_keywords'] = brand_keywords  # ğŸ”¥ é‡è¦: ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦è¨­å®š\n",
    "\n",
    "print(f\"\\nğŸ”§ å¤‰æ•°åã®çµ±ä¸€å®Œäº†:\")\n",
    "print(f\"  - ai_session_id: {ai_session_id}\")\n",
    "print(f\"  - brand_keywords: {len(brand_keywords)}å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’åˆ©ç”¨å¯èƒ½\")\n",
    "print(f\"  - LLM_TYPE: {LLM_TYPE}\")\n",
    "print(f\"  - DEVELOPMENT_MODE: {DEVELOPMENT_MODE}\")\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "# === handoff (auto-added) ===\n",
    "# ã“ã®ã‚»ãƒ«ã¯è‡ªå‹•è¿½åŠ : æ¬¡ãƒ‘ãƒ¼ãƒˆãŒå¿…è¦ã¨ã™ã‚‹æœ€å°é›†åˆã®ã¿ã‚’ä¿å­˜ã—ã¾ã™ï¼ˆåŸæ–‡ã‚»ãƒ«ã¯ç„¡æ”¹å¤‰ï¼‰ã€‚\n",
    "import os, joblib\n",
    "os.makedirs(HANDOFF_DIR, exist_ok=True)\n",
    "handoff = {}\n",
    "# --- keys: 'false_negatives_df', 'DB_CONFIG', 'brand_keywords'\n",
    "handoff['false_negatives_df'] = false_negatives_df\n",
    "handoff['DB_CONFIG'] = DB_CONFIG\n",
    "handoff['brand_keywords'] = brand_keywords\n",
    "joblib.dump(handoff, os.path.join(HANDOFF_DIR, \"03_ai_agent_analysis_part1.pkl\"))\n",
    "print(f\"âœ… Saved handoff â†’ {os.path.join(HANDOFF_DIR, '03_ai_agent_analysis_part1.pkl')}\")\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ca7cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T15:41:45.719597Z",
     "iopub.status.busy": "2025-10-29T15:41:45.719134Z",
     "iopub.status.idle": "2025-10-29T15:41:45.735051Z",
     "shell.execute_reply": "2025-10-29T15:41:45.733916Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === ã‚»ãƒ«3(ãƒ‘ãƒƒãƒ): DB/LLMè¨­å®šã‚’Configã‹ã‚‰å–å¾— & GPUåˆ¶å¾¡ (è¿½è¨˜) ===\n",
    "from typing import Dict, Any, Optional\n",
    "import subprocess\n",
    "\n",
    "def _summarize_db(db: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {k: db.get(k) for k in (\"dbname\",\"user\",\"host\",\"port\")}\n",
    "\n",
    "def apply_config_to_runtime(cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # --- DB ---\n",
    "    db = cfg.get(\"db\", {})\n",
    "    timeout_s = int(db.get(\"timeout_s\", 30))\n",
    "    DB_CONFIG = {\n",
    "        \"dbname\": str(db.get(\"dbname\")),\n",
    "        \"user\": str(db.get(\"user\")),\n",
    "        \"password\": str(db.get(\"password\",\"\")),\n",
    "        \"host\": str(db.get(\"host\")),\n",
    "        \"port\": str(db.get(\"port\")),\n",
    "    }\n",
    "    globals()[\"DB_CONFIG\"] = DB_CONFIG\n",
    "    globals()[\"DB_CONNECT_TIMEOUT_S\"] = timeout_s\n",
    "    print(\"ğŸ”§ DBè¨­å®š:\", _summarize_db(DB_CONFIG), f\"(timeout_s={timeout_s})\")\n",
    "\n",
    "    # --- GPU detect ---\n",
    "    GPU_AVAILABLE = False\n",
    "    if cfg.get(\"system\", {}).get(\"gpu_auto_detect\", True):\n",
    "        try:\n",
    "            r = subprocess.run([\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            GPU_AVAILABLE = (r.returncode == 0)\n",
    "            print(\"âœ… GPUæ¤œå‡º: åˆ©ç”¨å¯èƒ½\" if GPU_AVAILABLE else \"âš ï¸ GPUæœªæ¤œå‡º: CPUã§å®Ÿè¡Œã—ã¾ã™\")\n",
    "        except Exception:\n",
    "            print(\"âš ï¸ nvidia-smi å®Ÿè¡Œä¸å¯: CPUã§å®Ÿè¡Œã—ã¾ã™\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ GPUè‡ªå‹•æ¤œå‡ºã¯ç„¡åŠ¹åŒ– (cfg.system.gpu_auto_detect=False)\")\n",
    "    globals()[\"GPU_AVAILABLE\"] = GPU_AVAILABLE\n",
    "\n",
    "    # --- LLM ---\n",
    "    try:\n",
    "        from openai import OpenAI  # OpenAI SDK äº’æ› (vLLM/Ollama)\n",
    "    except Exception:\n",
    "        OpenAI = None\n",
    "\n",
    "    llm_cfg = cfg.get(\"llm\", {})\n",
    "    provider = str(llm_cfg.get(\"provider\",\"vllm\")).lower()\n",
    "    DEFAULT_MODEL = None\n",
    "    client = None\n",
    "\n",
    "    def _try_client(base_url: str, model: str, key: str = \"EMPTY\"):\n",
    "        if OpenAI is None:\n",
    "            return None\n",
    "        try:\n",
    "            cli = OpenAI(base_url=base_url, api_key=key or \"EMPTY\")\n",
    "            _ = cli.chat.completions.create(model=model, messages=[{\"role\":\"user\",\"content\":\"ping\"}], max_tokens=1, temperature=0.0)\n",
    "            return cli\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    if provider == \"vllm\":\n",
    "        base = str(llm_cfg.get(\"vllm_base_url\") or \"http://192.168.100.71:30000/v1\")\n",
    "        model = str(llm_cfg.get(\"vllm_model\") or \"Qwen/Qwen3-14B-FP8\")\n",
    "        client = _try_client(base, model)\n",
    "        if client is None and llm_cfg.get(\"fallback_enabled\", True):\n",
    "            print(\"âš ï¸ vLLMæ¥ç¶šã«å¤±æ•—ã€‚Ollamaã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦ã¿ã¾ã™\")\n",
    "            provider = \"ollama\"\n",
    "        else:\n",
    "            DEFAULT_MODEL = model\n",
    "\n",
    "    if provider == \"ollama\":\n",
    "        base = str(llm_cfg.get(\"ollama_base_url\") or \"http://localhost:11434/v1\")\n",
    "        model = str(llm_cfg.get(\"ollama_model\") or \"qwen3:14b\")\n",
    "        if client is None:\n",
    "            client = _try_client(base, model, key=\"ollama\")\n",
    "        DEFAULT_MODEL = model\n",
    "\n",
    "    LLM_READY = (client is not None) if OpenAI is not None else False\n",
    "    if OpenAI is None:\n",
    "        print(\"â„¹ï¸ openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœªå°å…¥: LLMã¯ç„¡åŠ¹åŒ–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ç¶™ç¶šï¼‰\")\n",
    "\n",
    "    globals()[\"LLM_CLIENT\"] = client\n",
    "    globals()[\"LLM_READY\"] = LLM_READY\n",
    "    globals()[\"LLM_TYPE\"]  = provider\n",
    "    globals()[\"DEFAULT_MODEL\"] = DEFAULT_MODEL\n",
    "    globals()[\"VLLM_BASE_URL\"] = llm_cfg.get(\"vllm_base_url\")\n",
    "    globals()[\"OLLAMA_BASE_URL\"] = llm_cfg.get(\"ollama_base_url\")\n",
    "    print(f\"ğŸ¤– LLM: type={provider}, ready={LLM_READY}, model={DEFAULT_MODEL}\")\n",
    "\n",
    "    return {\n",
    "        \"DB_CONFIG\": DB_CONFIG,\n",
    "        \"GPU_AVAILABLE\": GPU_AVAILABLE,\n",
    "        \"LLM_TYPE\": provider,\n",
    "        \"LLM_READY\": LLM_READY,\n",
    "        \"DEFAULT_MODEL\": DEFAULT_MODEL,\n",
    "    }\n",
    "\n",
    "# ï¼ˆå…ƒã‚»ãƒ«3ã®å¾Œã§ã‚‚ç¢ºå®Ÿã«é©ç”¨ã™ã‚‹ãŸã‚ã®å†é©ç”¨ãƒ•ãƒƒã‚¯ï¼‰\n",
    "if \"cfg\" in globals() and isinstance(cfg, dict):\n",
    "    _ = apply_config_to_runtime(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719f4132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T15:41:45.738389Z",
     "iopub.status.busy": "2025-10-29T15:41:45.738166Z",
     "iopub.status.idle": "2025-10-29T15:41:45.749177Z",
     "shell.execute_reply": "2025-10-29T15:41:45.747897Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === ã‚»ãƒ«X: å½é™°æ€§åˆ†æã®é–¾å€¤ã‚’Configã‹ã‚‰å–å¾— (è¿½è¨˜) ===\n",
    "from typing import Dict, Any\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "def _get_thresholds(cfg: Dict[str, Any]) -> Dict[str, float]:\n",
    "    a = cfg.get(\"analysis\", {})\n",
    "    low = float(a.get(\"fn_threshold_low\", 0.2))\n",
    "    high = float(a.get(\"fn_threshold_high\", 0.7))\n",
    "    detailed = bool(a.get(\"enable_detailed_stats\", True))\n",
    "    return {\"low\": low, \"high\": high, \"detailed\": detailed}\n",
    "\n",
    "def analyze_low_prob_region(false_negatives_df: pd.DataFrame, cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    th = _get_thresholds(cfg)\n",
    "    low = th[\"low\"]; high = th[\"high\"]; detailed = th[\"detailed\"]\n",
    "    if false_negatives_df is None or \"prediction_proba\" not in false_negatives_df.columns:\n",
    "        print(\"âš ï¸ false_negatives_df æœªæº–å‚™ã€ã¾ãŸã¯ prediction_proba åˆ—ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return {\"count\": 0, \"low_threshold\": low, \"high_threshold\": high}\n",
    "\n",
    "    df = false_negatives_df\n",
    "    low_df = df[df[\"prediction_proba\"] < low]\n",
    "    mid_df = df[(df[\"prediction_proba\"] >= low) & (df[\"prediction_proba\"] < high)]\n",
    "    high_df = df[df[\"prediction_proba\"] >= high]\n",
    "\n",
    "    print(f\"ğŸ¯ ä½ç¢ºç‡é ˜åŸŸåˆ†æï¼ˆã—ãã„å€¤ low={low}, high={high}ï¼‰\")\n",
    "    print(f\"  - MLç¢ºç‡ < {low:.2f}: {len(low_df):,}ä»¶\")\n",
    "    if detailed:\n",
    "        print(f\"  - {low:.2f} <= MLç¢ºç‡ < {high:.2f}: {len(mid_df):,}ä»¶\")\n",
    "        print(f\"  - MLç¢ºç‡ >= {high:.2f}: {len(high_df):,}ä»¶\")\n",
    "\n",
    "    stats = {\n",
    "        \"low_region_count\": int(len(low_df)),\n",
    "        \"mid_region_count\": int(len(mid_df)),\n",
    "        \"high_region_count\": int(len(high_df)),\n",
    "        \"low_threshold\": float(low),\n",
    "        \"high_threshold\": float(high),\n",
    "    }\n",
    "    if detailed and len(low_df) > 0:\n",
    "        stats.update({\n",
    "            \"low_mean\": float(low_df[\"prediction_proba\"].mean()),\n",
    "            \"low_median\": float(low_df[\"prediction_proba\"].median()),\n",
    "        })\n",
    "    return stats\n",
    "\n",
    "# é–¾å€¤ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å…¬é–‹\n",
    "if \"cfg\" in globals():\n",
    "    __th = _get_thresholds(cfg)\n",
    "    FN_THRESHOLD_LOW = __th[\"low\"]; FN_THRESHOLD_HIGH = __th[\"high\"]; ENABLE_DETAILED_STATS = __th[\"detailed\"]\n",
    "    globals().update({\"FN_THRESHOLD_LOW\": FN_THRESHOLD_LOW, \"FN_THRESHOLD_HIGH\": FN_THRESHOLD_HIGH, \"ENABLE_DETAILED_STATS\": ENABLE_DETAILED_STATS})\n",
    "    print(f\"ğŸ” é–¾å€¤ã‚’è¨­å®š: low={FN_THRESHOLD_LOW}, high={FN_THRESHOLD_HIGH}, detailed={ENABLE_DETAILED_STATS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2195d3a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T15:41:45.753702Z",
     "iopub.status.busy": "2025-10-29T15:41:45.753440Z",
     "iopub.status.idle": "2025-10-29T15:41:45.782385Z",
     "shell.execute_reply": "2025-10-29T15:41:45.780608Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === æœ€çµ‚ã‚»ãƒ«: Controller APIé–¢æ•° (agent_minimal) (è¿½è¨˜) ===\n",
    "from typing import Tuple, Dict, Any, Optional, List\n",
    "import os, json, joblib, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "def _find_first(*candidates: str) -> Optional[str]:\n",
    "    for p in candidates:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _load_false_negatives(session_id: str):\n",
    "    import pandas as pd\n",
    "    cands = [\n",
    "        f\"results/{session_id}/false_negatives_reconstructed.pkl\",\n",
    "        f\"models/{session_id}/false_negatives_reconstructed.pkl\",\n",
    "        f\"results/{session_id}/false_negatives.pkl\",\n",
    "    ]\n",
    "    p = _find_first(*cands)\n",
    "    if not p:\n",
    "        return None\n",
    "    try:\n",
    "        obj = joblib.load(p)\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            return obj\n",
    "        if isinstance(obj, dict):\n",
    "            if \"analysis_df\" in obj and isinstance(obj[\"analysis_df\"], pd.DataFrame):\n",
    "                return obj[\"analysis_df\"]\n",
    "            if \"domains\" in obj and \"predictions\" in obj:\n",
    "                return pd.DataFrame({\"domain\": obj[\"domains\"], \"prediction_proba\": obj[\"predictions\"], \"source\": obj.get(\"sources\")})\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def _load_brand_keywords(session_id: str) -> List[str]:\n",
    "    p1 = f\"models/{session_id}/brand_keywords.json\"\n",
    "    if os.path.exists(p1):\n",
    "        try:\n",
    "            return json.loads(Path(p1).read_text(encoding='utf-8'))\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        models_dir = Path(\"models\")\n",
    "        dirs = sorted([d for d in models_dir.glob(\"*\") if d.is_dir()])\n",
    "        for d in reversed(dirs):\n",
    "            p = d / \"brand_keywords.json\"\n",
    "            if p.exists():\n",
    "                return json.loads(p.read_text(encoding='utf-8'))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "def _ensure_dirs(base: Path) -> Dict[str, str]:\n",
    "    results_dir = base / \"results\"\n",
    "    handoff_dir = base / \"handoff\"\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    handoff_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return {\"results_dir\": str(results_dir), \"handoff_dir\": str(handoff_dir)}\n",
    "\n",
    "def _simple_risk_correction(df, brand_keywords: List[str], low_th: float):\n",
    "    import pandas as pd\n",
    "    df = df.copy()\n",
    "    if \"prediction_proba\" not in df.columns or \"domain\" not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"domain\",\"prediction_proba\",\"corrected\",\"reason\"])\n",
    "    brands = [b.lower() for b in (brand_keywords or [])]\n",
    "    def _has_brand(d: str) -> bool:\n",
    "        d = str(d).lower()\n",
    "        return any(b and b in d for b in brands[:200])\n",
    "    def _tld(d: str) -> str:\n",
    "        parts = str(d).lower().split(\".\")\n",
    "        return parts[-1] if len(parts) > 1 else \"\"\n",
    "    def _short(d: str) -> bool:\n",
    "        left = str(d).split(\".\")[0]\n",
    "        return len(left) < 10\n",
    "    corrected, reasons = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        ml = float(row.get(\"prediction_proba\", 1.0)); d = str(row.get(\"domain\",\"\"))\n",
    "        c = False; reason = []\n",
    "        if ml < low_th:\n",
    "            if _has_brand(d): c=True; reason.append(\"brand+lowML\")\n",
    "            if _short(d):     c=True; reason.append(\"short+lowML\")\n",
    "            tl = _tld(d)\n",
    "            if tl in (\"tk\",\"gq\",\"ml\",\"cf\",\"ga\"):\n",
    "                c=True; reason.append(f\"tld({tl})+lowML\")\n",
    "        corrected.append(c); reasons.append(\",\".join(reason))\n",
    "    df[\"corrected\"] = corrected; df[\"reason\"] = reasons\n",
    "    return df\n",
    "\n",
    "def agent_minimal(session_id: str, inputs: Dict[str, Any] = {\"mode\":\"fn\"}, cfg: Dict[str, Any] = None) -> Tuple[str, Dict[str, str]]:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    try:\n",
    "        if not isinstance(session_id, str) or not session_id.strip():\n",
    "            return \"INVALID_INPUT\", {\"error\": \"session_idã¯å¿…é ˆã§ã™\"}\n",
    "\n",
    "        local_cfg = load_configuration(cfg_override=(cfg or {}))\n",
    "        _ = apply_config_to_runtime(local_cfg)\n",
    "        if local_cfg.get(\"system\", {}).get(\"cert_only_mode\", False):\n",
    "            print(\"âš ï¸ æ³¨æ„: cert_only_modeã§ã™ãŒã€ãƒ–ãƒ©ãƒ³ãƒ‰å½è£…æ¤œå‡ºã¯ç ”ç©¶ã®æ ¸å¿ƒæ©Ÿèƒ½ã®ãŸã‚å®Ÿè¡Œã—ã¾ã™\")\n",
    "\n",
    "        base = Path(\"artifacts\") / session_id\n",
    "        dirs = _ensure_dirs(base)\n",
    "        results_dir = Path(dirs[\"results_dir\"]); handoff_dir = Path(dirs[\"handoff_dir\"])\n",
    "\n",
    "        brand_keywords = _load_brand_keywords(session_id)\n",
    "        if len(brand_keywords) < 1:\n",
    "            print(\"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬åˆ†æã®ã¿å®Ÿè¡Œã—ã¾ã™\")\n",
    "        elif len(brand_keywords) < 62:\n",
    "            print(f\"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ{len(brand_keywords)}ä»¶ (>=62ãŒæ¨å¥¨)\")\n",
    "\n",
    "        fn_df = _load_false_negatives(session_id)\n",
    "        if fn_df is None or fn_df.empty:\n",
    "            return \"NOT_FOUND\", {\"error\": \"å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\", \"session_id\": session_id}\n",
    "\n",
    "        low_th = float(local_cfg.get(\"analysis\", {}).get(\"fn_threshold_low\", 0.2))\n",
    "        corr_df = _simple_risk_correction(fn_df, brand_keywords, low_th)\n",
    "\n",
    "        low_df = fn_df[fn_df[\"prediction_proba\"] < low_th]\n",
    "        corrected_low = corr_df[(corr_df[\"prediction_proba\"] < low_th) & (corr_df[\"corrected\"] == True)]\n",
    "        improvement_rate = (len(corrected_low) / max(1, len(low_df))) * 100.0\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        bars = [len(low_df)-len(corrected_low), len(corrected_low)]\n",
    "        plt.bar([\"æœªè£œæ­£\",\"è£œæ­£\"], bars)\n",
    "        plt.title(f\"ä½ç¢ºç‡é ˜åŸŸ(<{low_th:.2f})ã®è£œæ­£æ•° (æ”¹å–„ç‡ {improvement_rate:.1f}%)\")\n",
    "        plt.ylabel(\"ä»¶æ•°\")\n",
    "        out_improve = Path(results_dir) / \"fn_detection_improvement.png\"\n",
    "        plt.tight_layout(); plt.savefig(out_improve, dpi=150); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        fn_df[\"prediction_proba\"].hist(bins=30)\n",
    "        try:\n",
    "            plt.axvline(x=low_th, color=\"red\", linestyle=\"--\", label=f\"low={low_th}\")\n",
    "            plt.legend()\n",
    "        except Exception:\n",
    "            pass\n",
    "        plt.title(\"å½é™°æ€§ã®äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ\")\n",
    "        out_dist = Path(results_dir) / \"ml_probability_distribution.png\"\n",
    "        plt.tight_layout(); plt.savefig(out_dist, dpi=150); plt.close()\n",
    "\n",
    "        if \"corrected\" in corr_df.columns:\n",
    "            top_reasons = corr_df[corr_df[\"corrected\"]==True][\"reason\"].value_counts()[:5]\n",
    "            plt.figure(figsize=(6,4))\n",
    "            top_reasons.plot(kind=\"bar\")\n",
    "            plt.title(\"è£œæ­£ã®ä¸»å›  (ä¸Šä½)\")\n",
    "            out_brand = Path(results_dir) / \"brand_detection_results.png\"\n",
    "            plt.tight_layout(); plt.savefig(out_brand, dpi=150); plt.close()\n",
    "        else:\n",
    "            out_brand = Path(results_dir) / \"brand_detection_results.png\"; out_brand.touch()\n",
    "\n",
    "        report = {\n",
    "            \"session_id\": session_id,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"metrics\": {\n",
    "                \"low_threshold\": low_th,\n",
    "                \"low_region_count\": int(len(low_df)),\n",
    "                \"corrected_low_count\": int(len(corrected_low)),\n",
    "                \"improvement_rate_pct\": float(improvement_rate),\n",
    "            },\n",
    "            \"notes\": [\n",
    "                \"LangGraphæœªä½¿ç”¨ã®æœ€å°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\",\n",
    "                \"vLLMâ†’Ollama ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ apply_config_to_runtime ã§å®Ÿæ–½\",\n",
    "            ],\n",
    "        }\n",
    "        out_json = Path(results_dir) / \"agent_performance_report.json\"\n",
    "        out_json.write_text(json.dumps(report, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "        out_corr = Path(results_dir) / \"false_negatives_corrected.csv\"\n",
    "        corr_df.to_csv(out_corr, index=False, encoding=\"utf-8\")\n",
    "\n",
    "        handoff_obj = {\"false_negatives_df\": fn_df, \"corrected_df\": corr_df, \"brand_keywords\": brand_keywords, \"cfg\": local_cfg}\n",
    "        out_handoff = Path(dirs[\"handoff_dir\"]) / \"03_ai_agent_analysis_part1.pkl\"\n",
    "        joblib.dump(handoff_obj, out_handoff)\n",
    "\n",
    "        Paths = {\n",
    "            \"fn_detection_improvement\": str(out_improve),\n",
    "            \"ml_probability_distribution\": str(out_dist),\n",
    "            \"brand_detection_results\": str(out_brand),\n",
    "            \"agent_performance_report\": str(out_json),\n",
    "            \"false_negatives_corrected\": str(out_corr),\n",
    "            \"handoff\": str(out_handoff),\n",
    "        }\n",
    "        return \"OK\", Paths\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc(limit=2)\n",
    "        return \"ERROR\", {\"error\": str(e), \"traceback\": tb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd11aa-8055-4ba8-8597-bb3dda5707d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
