{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9ce7ee",
   "metadata": {},
   "source": [
    "# 03_ai_agent_analysis_part1_fixed â€” Config & API æ‹¡å¼µï¼ˆçµ±åˆç‰ˆï¼‰\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**å…ƒã‚³ãƒ¼ãƒ‰ã‚’æ¸©å­˜**ã—ãŸã¾ã¾ã€ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’**è¿½è¨˜**ã—ã¦æ©Ÿèƒ½æ‹¡å¼µã—ã¾ã™ï¼š\n",
    "- ã‚»ãƒ«0: Configèª­ã¿è¾¼ã¿ã¨ãƒãƒ¼ã‚¸ (`load_configuration`)\n",
    "- ã‚»ãƒ«3(ãƒ‘ãƒƒãƒ): DB/LLMè¨­å®šã®Configåæ˜  + GPUè‡ªå‹•æ¤œå‡º + vLLMâ†’Ollamaãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "- ã‚»ãƒ«X: å½é™°æ€§åˆ†æã®é–¾å€¤ã‚’Configã‹ã‚‰å–å¾—ï¼ˆä½ç¢ºç‡é ˜åŸŸåˆ†æã¯å¿…ãšå®Ÿè¡Œï¼‰\n",
    "- æœ€çµ‚ã‚»ãƒ«: Controller API (`agent_minimal`)\n",
    "â€» å…ƒã‚»ãƒ«ã¯å¤‰æ›´ãƒ»å‰Šé™¤ã›ãšã€**å®Ÿè¡Œé †åºã§ä¸Šæ›¸ãé©ç”¨**ã—ã¾ã™ï¼ˆå‰¯ä½œç”¨å›é¿ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4e13d4-1a9d-403b-a034-fee8fcb5c8a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:18:22.643291Z",
     "iopub.status.busy": "2026-02-02T13:18:22.643142Z",
     "iopub.status.idle": "2026-02-02T13:18:22.874187Z",
     "shell.execute_reply": "2026-02-02T13:18:22.873345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using candidate: Noto Sans CJK JP\n",
      "Resolved path: /usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import font_manager as fm\n",
    "\n",
    "# ã©ã®ãƒ•ã‚©ãƒ³ãƒˆãŒé¸ã°ã‚Œã‚‹ã‹ã¯ rcParams ã§æ±ºã¾ã‚‹ã®ã§ã€æœ€å„ªå…ˆã« Noto ã‚’ç½®ã\n",
    "mpl.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "mpl.rcParams[\"font.sans-serif\"] = [\"Noto Sans CJK JP\", \"DejaVu Sans\"]\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# å¿µã®ãŸã‚ã€Œæœ¬å½“ã«è¦‹ãˆã¦ã‚‹ã‹ã€ã‚‚ç¢ºèª\n",
    "print(\"Using candidate:\", mpl.rcParams[\"font.sans-serif\"][0])\n",
    "print(\"Resolved path:\", fm.findfont(\"Noto Sans CJK JP\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a064f7-7b70-473f-a396-9a2200a17578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:18:22.879233Z",
     "iopub.status.busy": "2026-02-02T13:18:22.878880Z",
     "iopub.status.idle": "2026-02-02T13:18:22.930996Z",
     "shell.execute_reply": "2026-02-02T13:18:22.929877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NX] RUN_ID = 2026-02-02_220431 | paths.RUN_ID = 2026-02-02_220431\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0 (02ä»¥é™ å…±é€š): ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰è§£æ±ºã—ã¦ paths ã‚’èª­ã‚€ ===\n",
    "import run_id_registry as runreg\n",
    "rid = runreg.bootstrap()  # envâ†’ãƒ•ã‚¡ã‚¤ãƒ«(artifacts/_current/run_id.txt)â†’Part3â†’latestâ†’æ–°è¦ ã®é †ã§è§£æ±º\n",
    "\n",
    "import importlib\n",
    "import _compat.paths as paths\n",
    "importlib.reload(paths)\n",
    "importlib.reload(paths)\n",
    "print(\"[NX] RUN_ID =\", rid, \"| paths.RUN_ID =\", paths.RUN_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a8c1e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:18:22.934598Z",
     "iopub.status.busy": "2026-02-02T13:18:22.933983Z",
     "iopub.status.idle": "2026-02-02T13:18:22.949618Z",
     "shell.execute_reply": "2026-02-02T13:18:22.948438Z"
    }
   },
   "outputs": [],
   "source": [
    "# === ã‚»ãƒ«0: Configèª­ã¿è¾¼ã¿ã¨ãƒãƒ¼ã‚¸ (å³æ ¼ç‰ˆ) ===\n",
    "from __future__ import annotations\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import yaml  # optional\n",
    "except Exception:\n",
    "    yaml = None\n",
    "\n",
    "def _deep_update(base: dict, override: dict) -> dict:\n",
    "    base = dict(base or {})\n",
    "    for k, v in (override or {}).items():\n",
    "        if isinstance(v, dict) and isinstance(base.get(k), dict):\n",
    "            base[k] = _deep_update(base[k], v)\n",
    "        else:\n",
    "            base[k] = v\n",
    "    return base\n",
    "\n",
    "def load_configuration(\n",
    "    config_path: Optional[str] = None,\n",
    "    cfg_override: Optional[Dict[str, Any]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \n",
    "    # 1) ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œãƒ•ãƒ©ã‚°ã®ã¿ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’æŒã¤ (ç’°å¢ƒä¾å­˜å€¤ã¯ä¸€åˆ‡ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã—ãªã„)\n",
    "    defaults = {\n",
    "        \"system\": {\n",
    "            \"cert_only_mode\": False,\n",
    "            \"enable_brand_keywords\": True,\n",
    "            \"gpu_auto_detect\": True,\n",
    "            \"development_mode\": False,\n",
    "            \"seed\": 42,\n",
    "        },\n",
    "        \"llm\": {\n",
    "            \"provider\": \"vllm\", \n",
    "            \"fallback_enabled\": False, # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ç„¡åŠ¹åŒ–\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 2000,\n",
    "            # URLã‚„ãƒ¢ãƒ‡ãƒ«åã¯Configãƒ•ã‚¡ã‚¤ãƒ«å¿…é ˆã¨ã™ã‚‹\n",
    "        },\n",
    "        \"db\": {\n",
    "            \"timeout_s\": 30,\n",
    "            \"read_only\": True,\n",
    "            # DBæ¥ç¶šæƒ…å ±ã‚‚Configãƒ•ã‚¡ã‚¤ãƒ«å¿…é ˆ\n",
    "        },\n",
    "        \"analysis\": {\n",
    "            \"fn_threshold_low\": 0.2,\n",
    "            \"fn_threshold_high\": 0.7,\n",
    "            \"enable_detailed_stats\": True,\n",
    "        },\n",
    "    }\n",
    "    cfg = dict(defaults)\n",
    "\n",
    "    # 2) Configãƒ•ã‚¡ã‚¤ãƒ«ã®æ¢ç´¢ (æŒ‡å®šãƒ‘ã‚¹ -> ã‚«ãƒ¬ãƒ³ãƒˆ -> _compat -> è¦ª)\n",
    "    search_paths = []\n",
    "    if config_path:\n",
    "        search_paths.append(config_path)\n",
    "    \n",
    "    # è‡ªå‹•æ¢ç´¢ãƒ‘ã‚¹\n",
    "    search_paths.extend([\"config.json\", \"_compat/config.json\", \"../config.json\"])\n",
    "    \n",
    "    loaded_path = None\n",
    "    file_cfg = {}\n",
    "\n",
    "    for p in search_paths:\n",
    "        if p and os.path.exists(p):\n",
    "            try:\n",
    "                text = Path(p).read_text(encoding=\"utf-8\")\n",
    "                if p.endswith((\".yml\", \".yaml\")) and yaml is not None:\n",
    "                    file_cfg = yaml.safe_load(text) or {}\n",
    "                else:\n",
    "                    file_cfg = json.loads(text)\n",
    "                \n",
    "                loaded_path = p\n",
    "                print(f\"ğŸ“– è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {loaded_path}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ« {p} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "                # èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã¯è‡´å‘½çš„ã¨ã—ã¦æ‰±ã†å ´åˆã¯ã“ã“ã§raiseã—ã¦ã‚‚è‰¯ã„ãŒã€\n",
    "                # æ¬¡ã®å€™è£œã‚’æ¢ã™ãŸã‚ã«continueã™ã‚‹\n",
    "                continue\n",
    "\n",
    "    # â˜…å¤‰æ›´ç‚¹: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å³ã‚¨ãƒ©ãƒ¼ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—)\n",
    "    if not loaded_path:\n",
    "        raise FileNotFoundError(\n",
    "            \"âŒ å¿…é ˆã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (config.json) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\\n\"\n",
    "            \"   å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€ã¾ãŸã¯ _compat/ é…ä¸‹ã« config.json ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚\"\n",
    "        )\n",
    "\n",
    "    # è¨­å®šã®ãƒãƒ¼ã‚¸\n",
    "    cfg = _deep_update(cfg, file_cfg)\n",
    "\n",
    "    # 3) ç’°å¢ƒå¤‰æ•° (Env vars) ã§ã®ä¸Šæ›¸ãã¯åˆ©ä¾¿æ€§ã®ãŸã‚æ®‹ã™\n",
    "    env_map = {\n",
    "        \"DEV_MODE\": (\"system\", \"development_mode\"),\n",
    "        \"LLM_TYPE\": (\"llm\", \"provider\"),\n",
    "        \"VLLM_BASE_URL\": (\"llm\", \"vllm_base_url\"),\n",
    "        \"PGDATABASE\": (\"db\", \"dbname\"),\n",
    "        \"PGUSER\": (\"db\", \"user\"),\n",
    "        \"PGPASSWORD\": (\"db\", \"password\"),\n",
    "    }\n",
    "    for env, path in env_map.items():\n",
    "        if env in os.environ and os.environ[env] != \"\":\n",
    "            val = os.environ[env]\n",
    "            low = val.lower()\n",
    "            if low in (\"true\",\"false\"):\n",
    "                val = (low == \"true\")\n",
    "            node = cfg\n",
    "            for key in path[:-1]:\n",
    "                node = node.setdefault(key, {})\n",
    "            node[path[-1]] = val\n",
    "\n",
    "    # 4) å¼•æ•°ã§ã®Override\n",
    "    if cfg_override:\n",
    "        cfg = _deep_update(cfg, cfg_override)\n",
    "\n",
    "    # 5) å¿…é ˆé …ç›®ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "    # LLMè¨­å®šãƒã‚§ãƒƒã‚¯\n",
    "    llm_conf = cfg.get(\"llm\", {})\n",
    "    provider = str(llm_conf.get(\"provider\", \"vllm\")).lower()\n",
    "    \n",
    "    if provider == \"vllm\":\n",
    "        if not llm_conf.get(\"vllm_base_url\"):\n",
    "            raise ValueError(f\"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {loaded_path} ã« 'llm.vllm_base_url' ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
    "    elif provider == \"ollama\":\n",
    "        if not llm_conf.get(\"ollama_base_url\"):\n",
    "            raise ValueError(f\"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {loaded_path} ã« 'llm.ollama_base_url' ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
    "\n",
    "    # DBè¨­å®šãƒã‚§ãƒƒã‚¯\n",
    "    db_conf = cfg.get(\"db\", {})\n",
    "    required_db_keys = [\"dbname\", \"user\", \"host\", \"port\"]\n",
    "    missing_db_keys = [k for k in required_db_keys if not db_conf.get(k)]\n",
    "    if missing_db_keys:\n",
    "        raise ValueError(f\"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {loaded_path} ã«DBæ¥ç¶šæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_db_keys}\")\n",
    "\n",
    "    globals()[\"cfg\"] = cfg\n",
    "    print(\"âœ… Config loaded successfully (provider={provider}, gpu_auto_detect={gpu})\".format(\n",
    "        provider=provider,\n",
    "        gpu=cfg[\"system\"][\"gpu_auto_detect\"],\n",
    "    ))\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6022b5-ef30-4553-8423-29f96c617264",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:18:22.952158Z",
     "iopub.status.busy": "2026-02-02T13:18:22.951983Z",
     "iopub.status.idle": "2026-02-02T13:18:35.310768Z",
     "shell.execute_reply": "2026-02-02T13:18:35.309919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: _compat/config.json\n",
      "âœ… é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ãƒ­ãƒ¼ã‚«ãƒ«GPUæ¤œå‡º: XGBoostãƒ¢ãƒ‡ãƒ«ã§GPUåˆ©ç”¨å¯èƒ½\n",
      "\n",
      "ğŸ¤– LLMè¨­å®š:\n",
      "  - LLMå®Ÿè¡Œç’°å¢ƒ: vllm\n",
      "  - LLMãƒ¢ãƒ‡ãƒ«: JunHowie/Qwen3-4B-Thinking-2507-GPTQ-Int8\n",
      "  - Base URL: http://localhost:8000/v1\n",
      "  - æ³¨: LLMã¯ãƒªãƒ¢ãƒ¼ãƒˆã‚µãƒ¼ãƒãƒ¼ã®GPUã§å®Ÿè¡Œã•ã‚Œã¾ã™\n",
      "\n",
      "ğŸ”§ ç’°å¢ƒè¨­å®šã¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\n",
      "================================================================================\n",
      "âš ï¸ artifacts/_current/run_id.txt ã¯è¦‹ã¤ã‹ã£ãŸãŒã€å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒæƒã£ã¦ã„ã¾ã›ã‚“: 2026-02-02_220431\n",
      "âœ… artifacts ã‹ã‚‰æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º: 2026-01-24_213326\n",
      "  model_path : artifacts/2026-01-24_213326/models/xgboost_model.pkl\n",
      "  scaler_path: artifacts/2026-01-24_213326/models/scaler.pkl\n",
      "  feature_order_path : artifacts/2026-01-24_213326/models/feature_order.json \n",
      "  brand_keywords_path: artifacts/2026-01-24_213326/models/brand_keywords.json \n",
      "âœ… XGBoostãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: artifacts/2026-01-24_213326/models/xgboost_model.pkl\n",
      "âœ… XGBoostãƒ¢ãƒ‡ãƒ«: ãƒ­ãƒ¼ã‚«ãƒ«GPUã‚’ä½¿ç”¨\n",
      "âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†: artifacts/2026-01-24_213326/models/scaler.pkl\n",
      "âœ… ç‰¹å¾´é‡é †åºèª­ã¿è¾¼ã¿å®Œäº†: 42å€‹ã®ç‰¹å¾´é‡\n",
      "âœ… ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿å®Œäº†: 223å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰\n",
      "   ãƒ–ãƒ©ãƒ³ãƒ‰ä¾‹: ['aeonbank', 'aeoncard', 'amazon', 'amex', 'apple']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å½é™°æ€§ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: artifacts/2026-01-24_213326/results/false_negatives_reconstructed.pkl\n",
      "\n",
      "ğŸ“Š å½é™°æ€§ï¼ˆFNï¼‰ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ:\n",
      "  - ç·FNæ•°: 15,670ä»¶\n",
      "  - å¹³å‡äºˆæ¸¬ç¢ºç‡: 0.1736\n",
      "  - ä¸­å¤®å€¤: 0.0579\n",
      "  - æœ€å°äºˆæ¸¬ç¢ºç‡: 0.0036\n",
      "  - æœ€å¤§äºˆæ¸¬ç¢ºç‡: 0.9610\n",
      "\n",
      "ğŸ¯ ä½ç¢ºç‡é ˜åŸŸã®FNåˆ†å¸ƒï¼ˆå•é¡Œã®æ ¸å¿ƒï¼‰:\n",
      "  - MLç¢ºç‡ < 0.2: 12,196ä»¶ (77.8%)\n",
      "  - MLç¢ºç‡ 0.2-0.4: 1,318ä»¶ (8.4%)\n",
      "  - MLç¢ºç‡ >= 0.7: 1,310ä»¶ (8.4%)\n",
      "\n",
      "ã‚½ãƒ¼ã‚¹åˆ¥å†…è¨³:\n",
      "  - trusted: 12,931ä»¶ (82.5%)\n",
      "  - jpcert: 1,240ä»¶ (7.9%)\n",
      "  - certificates: 775ä»¶ (4.9%)\n",
      "  - phishtank: 724ä»¶ (4.6%)\n",
      "âœ… analysisãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: artifacts/2026-01-24_213326/analysis/03_analysis/\n",
      "âœ… logsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: artifacts/2026-01-24_213326/logs/03_analysis/\n",
      "âœ… tracesãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: artifacts/2026-01-24_213326/traces/03_analysis/\n",
      "âœ… resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: artifacts/2026-01-24_213326/analysis/03_analysis/\n",
      "\n",
      "âœ… åˆæœŸè¨­å®šå®Œäº†\n",
      "  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: 2026-01-24_213326\n",
      "  - LLMå®Ÿè¡Œ: vllm\n",
      "  - LLMãƒ¢ãƒ‡ãƒ«: JunHowie/Qwen3-4B-Thinking-2507-GPTQ-Int8\n",
      "  - XGBoost: ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆGPUä½¿ç”¨ï¼‰\n",
      "  - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: 20260202_221835\n",
      "  - å½é™°æ€§ä»¶æ•°: 15670\n",
      "  - ãƒ–ãƒ©ãƒ³ãƒ‰æ•°: 223\n",
      "================================================================================\n",
      "\n",
      "ğŸ”§ å¤‰æ•°åã®çµ±ä¸€å®Œäº†:\n",
      "  - ai_session_id: 2026-01-24_213326\n",
      "  - brand_keywords: 223å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’åˆ©ç”¨å¯èƒ½\n",
      "  - LLM_TYPE: vllm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "import joblib  # 02_xgboost_training_evaluation.pyã¯joblibã§ä¿å­˜\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "# === CHANGELOG ===\n",
    "# 2025-12-28: artifacts/{session_id}/ é…ä¸‹ã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã«åˆã‚ã›ã¦å…¥å‡ºåŠ›ãƒ‘ã‚¹ã‚’å…¨é¢çš„ã«æ›´æ–°ï¼ˆ03_analysisã«çµ±ä¸€ï¼‰\n",
    "\n",
    "# === CONFIG LOADING (STRICT MODE) ===\n",
    "CONFIG_PATH = Path(\"_compat/config.json\")\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"âŒ å¿…é ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {CONFIG_PATH.absolute()}\\n\"\n",
    "                            \"   ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã¯å»ƒæ­¢ã•ã‚Œã¾ã—ãŸã€‚å¿…ãšconfig.jsonã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "        APP_CONFIG = json.load(f)\n",
    "    print(f\"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {CONFIG_PATH}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    raise ValueError(f\"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®JSONå½¢å¼ãŒä¸æ­£ã§ã™: {e}\")\n",
    "\n",
    "# XGBoostã®GPUè­¦å‘Šã‚’æŠ‘åˆ¶\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "# === è¨­å®šå€¤ã®é©ç”¨ï¼ˆã‚­ãƒ¼ä¸è¶³æ™‚ã¯KeyErrorã§å³åœæ­¢ï¼‰ ===\n",
    "\n",
    "# 1. Systemè¨­å®š (GPUæ¤œå‡ºãƒ•ãƒ©ã‚°ãªã©)\n",
    "try:\n",
    "    system_conf = APP_CONFIG[\"system\"]\n",
    "    gpu_auto_detect = system_conf[\"gpu_auto_detect\"]\n",
    "    DEVELOPMENT_MODE = system_conf[\"development_mode\"]\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"âŒ config.jsonã® 'system' ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å¿…é ˆã‚­ãƒ¼ {e} ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "\n",
    "print(f\"âœ… é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: {DEVELOPMENT_MODE}\")\n",
    "\n",
    "# ãƒ­ãƒ¼ã‚«ãƒ«GPUã®ç¢ºèªï¼ˆXGBoostãƒ¢ãƒ‡ãƒ«ç”¨ï¼‰\n",
    "GPU_AVAILABLE = True\n",
    "if gpu_auto_detect:\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… ãƒ­ãƒ¼ã‚«ãƒ«GPUæ¤œå‡º: XGBoostãƒ¢ãƒ‡ãƒ«ã§GPUåˆ©ç”¨å¯èƒ½\")\n",
    "        else:\n",
    "            print(\"âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«GPUæœªæ¤œå‡º: XGBoostã¯CPUå‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™\")\n",
    "            GPU_AVAILABLE = False\n",
    "    except:\n",
    "        print(\"âš ï¸ nvidia-smiåˆ©ç”¨ä¸å¯: XGBoostã¯CPUå‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™\")\n",
    "        GPU_AVAILABLE = False\n",
    "else:\n",
    "    print(\"â„¹ï¸ GPUè‡ªå‹•æ¤œå‡ºã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚\")\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "\n",
    "# 2. LLMè¨­å®š\n",
    "print(\"\\nğŸ¤– LLMè¨­å®š:\")\n",
    "try:\n",
    "    llm_conf = APP_CONFIG[\"llm\"]\n",
    "    # ã‚­ãƒ¼åã®ãƒãƒƒãƒ”ãƒ³ã‚°ä¿®æ­£: \"type\" -> \"provider\"\n",
    "    LLM_TYPE = llm_conf[\"provider\"]\n",
    "    VLLM_BASE_URL = llm_conf[\"vllm_base_url\"]\n",
    "    VLLM_MODEL = llm_conf[\"vllm_model\"]\n",
    "    OLLAMA_MODEL = llm_conf[\"ollama_model\"]\n",
    "    # å¿µã®ãŸã‚Ollamaã®URLã‚‚å–å¾—ï¼ˆä½¿ã†å ´åˆï¼‰\n",
    "    OLLAMA_BASE_URL = llm_conf.get(\"ollama_base_url\")\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"âŒ config.jsonã® 'llm' ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å¿…é ˆã‚­ãƒ¼ {e} ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "\n",
    "print(f\"  - LLMå®Ÿè¡Œç’°å¢ƒ: {LLM_TYPE}\")\n",
    "print(f\"  - LLMãƒ¢ãƒ‡ãƒ«: {VLLM_MODEL if LLM_TYPE == 'vllm' else OLLAMA_MODEL}\")\n",
    "if LLM_TYPE == 'vllm':\n",
    "    print(f\"  - Base URL: {VLLM_BASE_URL}\")\n",
    "    print(f\"  - æ³¨: LLMã¯ãƒªãƒ¢ãƒ¼ãƒˆã‚µãƒ¼ãƒãƒ¼ã®GPUã§å®Ÿè¡Œã•ã‚Œã¾ã™\")\n",
    "\n",
    "\n",
    "# 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š\n",
    "try:\n",
    "    # \"database\" ã§ã¯ãªã \"db\" ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨\n",
    "    db_source = APP_CONFIG[\"db\"]\n",
    "\n",
    "    # æ¥ç¶šã«å¿…è¦ãªã‚­ãƒ¼ã‚’æŠ½å‡ºã—ã¦ DB_CONFIG ã‚’æ§‹ç¯‰\n",
    "    DB_CONFIG = {\n",
    "        \"dbname\": db_source[\"dbname\"],\n",
    "        \"user\": db_source[\"user\"],\n",
    "        \"password\": db_source[\"password\"],\n",
    "        \"host\": db_source[\"host\"],\n",
    "        \"port\": db_source[\"port\"]\n",
    "    }\n",
    "except KeyError as e:\n",
    "    raise KeyError(f\"âŒ config.jsonã® 'db' ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ä¸å‚™ãŒã‚ã‚Šã¾ã™: {e}\")\n",
    "\n",
    "print(\"\\nğŸ”§ ç’°å¢ƒè¨­å®šã¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== Session ID ã®æ¤œå‡ºï¼ˆartifacts/{session_id}/... æ§‹æˆã«åˆã‚ã›ã‚‹ï¼‰=====\n",
    "session_id = None\n",
    "artifacts_dir = Path(\"artifacts\")\n",
    "current_run_id_path = artifacts_dir / \"_current\" / \"run_id.txt\"\n",
    "\n",
    "def _parse_session_id(name: str):\n",
    "    m = re.match(r\"^(\\d{4})-(\\d{2})-(\\d{2})_(\\d{6})$\", name)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(name, \"%Y-%m-%d_%H%M%S\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def _has_required_models(run_dir: Path) -> bool:\n",
    "    return (\n",
    "        (run_dir / \"models\" / \"xgboost_model.pkl\").exists()\n",
    "        and (run_dir / \"models\" / \"scaler.pkl\").exists()\n",
    "        and (run_dir / \"models\" / \"feature_order.json\").exists()\n",
    "        and (run_dir / \"models\" / \"brand_keywords.json\").exists()\n",
    "    )\n",
    "\n",
    "# 1) ã¾ãš _current/run_id.txt ã‚’å„ªå…ˆ\n",
    "if current_run_id_path.exists():\n",
    "    rid = current_run_id_path.read_text(encoding=\"utf-8\").strip()\n",
    "    if rid:\n",
    "        candidate_dir = artifacts_dir / rid\n",
    "        if candidate_dir.exists() and candidate_dir.is_dir() and _has_required_models(candidate_dir):\n",
    "            session_id = rid\n",
    "            print(f\"âœ… artifacts/_current ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³IDæ¤œå‡º: {session_id}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ artifacts/_current/run_id.txt ã¯è¦‹ã¤ã‹ã£ãŸãŒã€å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒæƒã£ã¦ã„ã¾ã›ã‚“: {rid}\")\n",
    "\n",
    "# 2) ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šartifacts/ é…ä¸‹ã‚’èµ°æŸ»ã—ã¦æœ€æ–°ã‚’æ‹¾ã†\n",
    "if not session_id:\n",
    "    candidates = []\n",
    "    if artifacts_dir.exists():\n",
    "        for d in artifacts_dir.iterdir():\n",
    "            if d.is_dir():\n",
    "                ts = _parse_session_id(d.name)\n",
    "                if ts and _has_required_models(d):\n",
    "                    candidates.append((ts, d.name))\n",
    "    if candidates:\n",
    "        candidates.sort(key=lambda x: x[0])\n",
    "        session_id = candidates[-1][1]\n",
    "        print(f\"âœ… artifacts ã‹ã‚‰æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º: {session_id}\")\n",
    "\n",
    "if not session_id:\n",
    "    raise ValueError(\"ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚artifacts/_current/run_id.txt ã‚’ç¢ºèªã™ã‚‹ã‹ã€02ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# ===== ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å®šç¾©ï¼ˆartifacts/{session_id}/...ï¼‰=====\n",
    "model_path = f\"artifacts/{session_id}/models/xgboost_model.pkl\"\n",
    "scaler_path = f\"artifacts/{session_id}/models/scaler.pkl\"\n",
    "feature_order_path = f\"artifacts/{session_id}/models/feature_order.json\"\n",
    "brand_keywords_path = f\"artifacts/{session_id}/models/brand_keywords.json\"\n",
    "\n",
    "print(\"  model_path :\", model_path)\n",
    "print(\"  scaler_path:\", scaler_path)\n",
    "print(\"  feature_order_path :\", feature_order_path, (\"(å­˜åœ¨ã—ã¾ã›ã‚“)\" if not Path(feature_order_path).exists() else \"\"))\n",
    "print(\"  brand_keywords_path:\", brand_keywords_path, (\"(å­˜åœ¨ã—ã¾ã›ã‚“)\" if not Path(brand_keywords_path).exists() else \"\"))\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆjoblibå½¢å¼ï¼‰\n",
    "try:\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"âœ… XGBoostãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "    raise\n",
    "\n",
    "# XGBoostãƒ¢ãƒ‡ãƒ«ã®GPUè¨­å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã€åˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰\n",
    "if GPU_AVAILABLE:\n",
    "    try:\n",
    "        if hasattr(model, 'set_param'):\n",
    "            model.set_param({'device': 'cuda:0'})\n",
    "        elif hasattr(model, 'get_booster'):\n",
    "            model.get_booster().set_param({'device': 'cuda:0'})\n",
    "            if hasattr(model, 'set_params'):\n",
    "                model.set_params(device='cuda:0', tree_method='hist')\n",
    "        print(f\"âœ… XGBoostãƒ¢ãƒ‡ãƒ«: ãƒ­ãƒ¼ã‚«ãƒ«GPUã‚’ä½¿ç”¨\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ GPUè¨­å®šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"   XGBoostã¯CPUå‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™\")\n",
    "\n",
    "# ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®èª­ã¿è¾¼ã¿ï¼ˆjoblibå½¢å¼ï¼‰\n",
    "try:\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    print(f\"âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†: {scaler_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {scaler_path}\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "    raise\n",
    "\n",
    "# ç‰¹å¾´é‡é †åºã®èª­ã¿è¾¼ã¿ï¼ˆJSONå½¢å¼ï¼‰\n",
    "try:\n",
    "    with open(feature_order_path, 'r') as f:\n",
    "        feature_order = json.load(f)\n",
    "    print(f\"âœ… ç‰¹å¾´é‡é †åºèª­ã¿è¾¼ã¿å®Œäº†: {len(feature_order)}å€‹ã®ç‰¹å¾´é‡\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ç‰¹å¾´é‡é †åºãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {feature_order_path}\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "    raise\n",
    "\n",
    "# ğŸ”¥ é‡è¦: å‹•çš„ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿ï¼ˆJSONå½¢å¼ï¼‰\n",
    "try:\n",
    "    with open(brand_keywords_path, 'r', encoding='utf-8') as f:\n",
    "        brand_keywords = json.load(f)\n",
    "    print(f\"âœ… ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿å®Œäº†: {len(brand_keywords)}å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰\")\n",
    "    print(f\"   ãƒ–ãƒ©ãƒ³ãƒ‰ä¾‹: {brand_keywords[:5]}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {brand_keywords_path}\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "    print(\"   ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯GPT-4o miniã§å‹•çš„ã«ç”Ÿæˆã•ã‚Œã‚‹é‡è¦ãªãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚\")\n",
    "    raise\n",
    "\n",
    "# å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "false_negatives_df = None\n",
    "\n",
    "# artifacts/{session_id}/results/ ã‚’å„ªå…ˆ\n",
    "fn_paths = [\n",
    "    f\"artifacts/{session_id}/results/false_negatives_reconstructed.pkl\",\n",
    "    f\"artifacts/{session_id}/results/false_negatives.pkl\",\n",
    "]\n",
    "\n",
    "for fn_path in fn_paths:\n",
    "    if os.path.exists(fn_path):\n",
    "        try:\n",
    "            # joblibã§èª­ã¿è¾¼ã¿ï¼ˆ02_xgboost_training_evaluation.pyã®å½¢å¼ï¼‰\n",
    "            fn_data = joblib.load(fn_path)\n",
    "\n",
    "            # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®åˆ¤å®š\n",
    "            if isinstance(fn_data, pd.DataFrame):\n",
    "                false_negatives_df = fn_data\n",
    "            elif isinstance(fn_data, dict):\n",
    "                if 'analysis_df' in fn_data:\n",
    "                    false_negatives_df = fn_data['analysis_df']\n",
    "                elif 'domains' in fn_data and 'predictions' in fn_data:\n",
    "                    # æ—§å½¢å¼ã®å ´åˆã¯å†æ§‹ç¯‰\n",
    "                    false_negatives_df = pd.DataFrame({\n",
    "                        'domain': fn_data['domains'],\n",
    "                        'prediction_proba': fn_data['predictions'],\n",
    "                        'source': fn_data.get('sources', ['unknown'] * len(fn_data['domains']))\n",
    "                    })\n",
    "\n",
    "            if false_negatives_df is not None:\n",
    "                print(f\"âœ… å½é™°æ€§ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {fn_path}\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ {fn_path}ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}\")\n",
    "            continue\n",
    "\n",
    "if false_negatives_df is None:\n",
    "    print(\"âŒ å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "    print(\"   02_xgboost_training_evaluation.pyã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\")\n",
    "    raise FileNotFoundError(\"å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "# å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆè¡¨ç¤º\n",
    "print(f\"\\nğŸ“Š å½é™°æ€§ï¼ˆFNï¼‰ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ:\")\n",
    "print(f\"  - ç·FNæ•°: {len(false_negatives_df):,}ä»¶\")\n",
    "\n",
    "if 'prediction_proba' in false_negatives_df.columns:\n",
    "    print(f\"  - å¹³å‡äºˆæ¸¬ç¢ºç‡: {false_negatives_df['prediction_proba'].mean():.4f}\")\n",
    "    print(f\"  - ä¸­å¤®å€¤: {false_negatives_df['prediction_proba'].median():.4f}\")\n",
    "    print(f\"  - æœ€å°äºˆæ¸¬ç¢ºç‡: {false_negatives_df['prediction_proba'].min():.4f}\")\n",
    "    print(f\"  - æœ€å¤§äºˆæ¸¬ç¢ºç‡: {false_negatives_df['prediction_proba'].max():.4f}\")\n",
    "\n",
    "    # ğŸ”¥ ä½ç¢ºç‡é ˜åŸŸã®FNåˆ†å¸ƒï¼ˆå•é¡Œã®æ ¸å¿ƒï¼‰\n",
    "    fn_low = false_negatives_df[false_negatives_df['prediction_proba'] < 0.2]\n",
    "    fn_mid = false_negatives_df[(false_negatives_df['prediction_proba'] >= 0.2) &\n",
    "                                 (false_negatives_df['prediction_proba'] < 0.4)]\n",
    "    fn_high = false_negatives_df[false_negatives_df['prediction_proba'] >= 0.7]\n",
    "    print(f\"\\nğŸ¯ ä½ç¢ºç‡é ˜åŸŸã®FNåˆ†å¸ƒï¼ˆå•é¡Œã®æ ¸å¿ƒï¼‰:\")\n",
    "    print(f\"  - MLç¢ºç‡ < 0.2: {len(fn_low):,}ä»¶ ({len(fn_low)/len(false_negatives_df)*100:.1f}%)\")\n",
    "    print(f\"  - MLç¢ºç‡ 0.2-0.4: {len(fn_mid):,}ä»¶ ({len(fn_mid)/len(false_negatives_df)*100:.1f}%)\")\n",
    "    print(f\"  - MLç¢ºç‡ >= 0.7: {len(fn_high):,}ä»¶ ({len(fn_high)/len(false_negatives_df)*100:.1f}%)\")\n",
    "\n",
    "# ã‚½ãƒ¼ã‚¹åˆ¥ã®çµ±è¨ˆ\n",
    "if 'source' in false_negatives_df.columns:\n",
    "    print(f\"\\nã‚½ãƒ¼ã‚¹åˆ¥å†…è¨³:\")\n",
    "    source_counts = false_negatives_df['source'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"  - {source}: {count:,}ä»¶ ({count/len(false_negatives_df)*100:.1f}%)\")\n",
    "\n",
    "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆï¼ˆ03_analysis ã«çµ±ä¸€ï¼‰\n",
    "output_dirs = {\n",
    "    \"analysis\": f\"artifacts/{session_id}/analysis/03_analysis/\",\n",
    "    \"logs\":    f\"artifacts/{session_id}/logs/03_analysis/\",\n",
    "    \"traces\":  f\"artifacts/{session_id}/traces/03_analysis/\",\n",
    "}\n",
    "# äº’æ›æ€§ã®ãŸã‚ï¼ˆæ—¢å­˜ã‚»ãƒ«ãŒ output_dirs['results'] ã‚’å‚ç…§ã—ã¦ã‚‚å´©ã‚Œãªã„ã‚ˆã†ã«ï¼‰\n",
    "output_dirs[\"results\"] = output_dirs[\"analysis\"]\n",
    "\n",
    "for dir_name, dir_path in output_dirs.items():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"âœ… {dir_name}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {dir_path}\")\n",
    "\n",
    "# ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆå¾Œã®ã‚»ãƒ«ã‹ã‚‰å‚ç…§ï¼‰\n",
    "cert_full_info_map = {}  # ã‚»ãƒ«02ã§ä½œæˆã•ã‚Œã‚‹\n",
    "fn_features_df = None    # ã‚»ãƒ«02ã§ä½œæˆã•ã‚Œã‚‹\n",
    "\n",
    "print(f\"\\nâœ… åˆæœŸè¨­å®šå®Œäº†\")\n",
    "print(f\"  - ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {session_id}\")\n",
    "print(f\"  - LLMå®Ÿè¡Œ: {LLM_TYPE}\")\n",
    "print(f\"  - LLMãƒ¢ãƒ‡ãƒ«: {VLLM_MODEL if LLM_TYPE == 'vllm' else OLLAMA_MODEL}\")\n",
    "print(f\"  - XGBoost: ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆ{'GPU' if GPU_AVAILABLE else 'CPU'}ä½¿ç”¨ï¼‰\")\n",
    "print(f\"  - ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {timestamp}\")\n",
    "print(f\"  - å½é™°æ€§ä»¶æ•°: {len(false_negatives_df)}\")\n",
    "print(f\"  - ãƒ–ãƒ©ãƒ³ãƒ‰æ•°: {len(brand_keywords)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# å¤‰æ•°åã®çµ±ä¸€ï¼ˆã‚»ãƒ«07, 08ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰\n",
    "ai_session_id = session_id\n",
    "globals()['ai_session_id'] = ai_session_id\n",
    "globals()['LLM_TYPE'] = LLM_TYPE\n",
    "globals()['output_dirs'] = output_dirs\n",
    "globals()['DEVELOPMENT_MODE'] = DEVELOPMENT_MODE\n",
    "globals()['brand_keywords'] = brand_keywords  # ğŸ”¥ é‡è¦: ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦è¨­å®š\n",
    "\n",
    "print(f\"\\nğŸ”§ å¤‰æ•°åã®çµ±ä¸€å®Œäº†:\")\n",
    "print(f\"  - ai_session_id: {ai_session_id}\")\n",
    "print(f\"  - brand_keywords: {len(brand_keywords)}å€‹ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’åˆ©ç”¨å¯èƒ½\")\n",
    "print(f\"  - LLM_TYPE: {LLM_TYPE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ca7cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:18:35.312973Z",
     "iopub.status.busy": "2026-02-02T13:18:35.312738Z",
     "iopub.status.idle": "2026-02-02T13:18:35.320785Z",
     "shell.execute_reply": "2026-02-02T13:18:35.320282Z"
    }
   },
   "outputs": [],
   "source": [
    "# === ã‚»ãƒ«3(ãƒ‘ãƒƒãƒ): DB/LLMè¨­å®šã‚’Configã‹ã‚‰å–å¾— & GPUåˆ¶å¾¡ (å³æ ¼ç‰ˆ: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—) ===\n",
    "from typing import Dict, Any, Optional\n",
    "import subprocess\n",
    "\n",
    "def _summarize_db(db: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    return {k: db.get(k) for k in (\"dbname\",\"user\",\"host\",\"port\")}\n",
    "\n",
    "def apply_config_to_runtime(cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # --- DB ---\n",
    "    db = cfg.get(\"db\", {})\n",
    "    timeout_s = int(db.get(\"timeout_s\", 30))\n",
    "    DB_CONFIG = {\n",
    "        \"dbname\": str(db.get(\"dbname\")),\n",
    "        \"user\": str(db.get(\"user\")),\n",
    "        \"password\": str(db.get(\"password\",\"\")),\n",
    "        \"host\": str(db.get(\"host\")),\n",
    "        \"port\": str(db.get(\"port\")),\n",
    "    }\n",
    "    globals()[\"DB_CONFIG\"] = DB_CONFIG\n",
    "    globals()[\"DB_CONNECT_TIMEOUT_S\"] = timeout_s\n",
    "    print(\"ğŸ”§ DBè¨­å®š:\", _summarize_db(DB_CONFIG), f\"(timeout_s={timeout_s})\")\n",
    "\n",
    "    # --- GPU detect ---\n",
    "    GPU_AVAILABLE = False\n",
    "    if cfg.get(\"system\", {}).get(\"gpu_auto_detect\", True):\n",
    "        try:\n",
    "            r = subprocess.run([\"nvidia-smi\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            GPU_AVAILABLE = (r.returncode == 0)\n",
    "            print(\"âœ… GPUæ¤œå‡º: åˆ©ç”¨å¯èƒ½\" if GPU_AVAILABLE else \"âš ï¸ GPUæœªæ¤œå‡º: CPUã§å®Ÿè¡Œã—ã¾ã™\")\n",
    "        except Exception:\n",
    "            print(\"âš ï¸ nvidia-smi å®Ÿè¡Œä¸å¯: CPUã§å®Ÿè¡Œã—ã¾ã™\")\n",
    "    else:\n",
    "        print(\"â„¹ï¸ GPUè‡ªå‹•æ¤œå‡ºã¯ç„¡åŠ¹åŒ– (cfg.system.gpu_auto_detect=False)\")\n",
    "    globals()[\"GPU_AVAILABLE\"] = GPU_AVAILABLE\n",
    "\n",
    "    # --- LLM ---\n",
    "    try:\n",
    "        from openai import OpenAI  # OpenAI SDK äº’æ› (vLLM/Ollama)\n",
    "    except Exception:\n",
    "        OpenAI = None\n",
    "\n",
    "    llm_cfg = cfg.get(\"llm\", {})\n",
    "    provider = str(llm_cfg.get(\"provider\", \"vllm\")).lower()\n",
    "    \n",
    "    DEFAULT_MODEL = None\n",
    "    client = None\n",
    "    base_url = \"\"\n",
    "\n",
    "    def _try_client(base_url: str, model: str, key: str = \"EMPTY\"):\n",
    "        if OpenAI is None:\n",
    "            return None\n",
    "        try:\n",
    "            # æ¥ç¶šç¢ºèªç”¨ã®è»½é‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ\n",
    "            cli = OpenAI(base_url=base_url, api_key=key or \"EMPTY\")\n",
    "            _ = cli.chat.completions.create(\n",
    "                model=model, \n",
    "                messages=[{\"role\":\"user\",\"content\":\"ping\"}], \n",
    "                max_tokens=1, \n",
    "                temperature=0.0\n",
    "            )\n",
    "            return cli\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ LLMæ¥ç¶šãƒã‚§ãƒƒã‚¯å¤±æ•— ({base_url}): {e}\")\n",
    "            return None\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®è¨­å®šèª­ã¿è¾¼ã¿ (ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ãªã—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãªã—)\n",
    "    if provider == \"vllm\":\n",
    "        base_url = str(llm_cfg.get(\"vllm_base_url\", \"\"))\n",
    "        model = str(llm_cfg.get(\"vllm_model\", \"\"))\n",
    "        if base_url and model:\n",
    "            client = _try_client(base_url, model)\n",
    "            if client:\n",
    "                DEFAULT_MODEL = model\n",
    "        else:\n",
    "            print(\"âš ï¸ vLLMã®è¨­å®š(url/model)ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "\n",
    "    elif provider == \"ollama\":\n",
    "        base_url = str(llm_cfg.get(\"ollama_base_url\", \"\"))\n",
    "        model = str(llm_cfg.get(\"ollama_model\", \"\"))\n",
    "        if base_url and model:\n",
    "            client = _try_client(base_url, model, key=\"ollama\")\n",
    "            if client:\n",
    "                DEFAULT_MODEL = model\n",
    "        else:\n",
    "            print(\"âš ï¸ Ollamaã®è¨­å®š(url/model)ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "\n",
    "    else:\n",
    "        print(f\"âš ï¸ æœªå¯¾å¿œã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã™: {provider}\")\n",
    "\n",
    "    LLM_READY = (client is not None) if OpenAI is not None else False\n",
    "    \n",
    "    if OpenAI is None:\n",
    "        print(\"â„¹ï¸ openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœªå°å…¥: LLMæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "    elif not LLM_READY:\n",
    "        print(f\"âŒ LLMæ¥ç¶šä¸å¯ (provider={provider}, url={base_url})\")\n",
    "\n",
    "    globals()[\"LLM_CLIENT\"] = client\n",
    "    globals()[\"LLM_READY\"] = LLM_READY\n",
    "    globals()[\"LLM_TYPE\"]  = provider\n",
    "    globals()[\"DEFAULT_MODEL\"] = DEFAULT_MODEL\n",
    "    globals()[\"VLLM_BASE_URL\"] = llm_cfg.get(\"vllm_base_url\")\n",
    "    globals()[\"OLLAMA_BASE_URL\"] = llm_cfg.get(\"ollama_base_url\")\n",
    "    \n",
    "    if LLM_READY:\n",
    "        print(f\"ğŸ¤– LLM: type={provider}, ready=True, model={DEFAULT_MODEL}\")\n",
    "\n",
    "    return {\n",
    "        \"DB_CONFIG\": DB_CONFIG,\n",
    "        \"GPU_AVAILABLE\": GPU_AVAILABLE,\n",
    "        \"LLM_TYPE\": provider,\n",
    "        \"LLM_READY\": LLM_READY,\n",
    "        \"DEFAULT_MODEL\": DEFAULT_MODEL,\n",
    "    }\n",
    "\n",
    "# ï¼ˆå…ƒã‚»ãƒ«3ã®å¾Œã§ã‚‚ç¢ºå®Ÿã«é©ç”¨ã™ã‚‹ãŸã‚ã®å†é©ç”¨ãƒ•ãƒƒã‚¯ï¼‰\n",
    "if \"cfg\" in globals() and isinstance(cfg, dict):\n",
    "    _ = apply_config_to_runtime(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719f4132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:18:35.322585Z",
     "iopub.status.busy": "2026-02-02T13:18:35.322473Z",
     "iopub.status.idle": "2026-02-02T13:18:35.327724Z",
     "shell.execute_reply": "2026-02-02T13:18:35.327222Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === ã‚»ãƒ«X: å½é™°æ€§åˆ†æã®é–¾å€¤ã‚’Configã‹ã‚‰å–å¾— (è¿½è¨˜) ===\n",
    "from typing import Dict, Any\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "def _get_thresholds(cfg: Dict[str, Any]) -> Dict[str, float]:\n",
    "    a = cfg.get(\"analysis\", {})\n",
    "    low = float(a.get(\"fn_threshold_low\", 0.2))\n",
    "    high = float(a.get(\"fn_threshold_high\", 0.7))\n",
    "    detailed = bool(a.get(\"enable_detailed_stats\", True))\n",
    "    return {\"low\": low, \"high\": high, \"detailed\": detailed}\n",
    "\n",
    "def analyze_low_prob_region(false_negatives_df: pd.DataFrame, cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    th = _get_thresholds(cfg)\n",
    "    low = th[\"low\"]; high = th[\"high\"]; detailed = th[\"detailed\"]\n",
    "    if false_negatives_df is None or \"prediction_proba\" not in false_negatives_df.columns:\n",
    "        print(\"âš ï¸ false_negatives_df æœªæº–å‚™ã€ã¾ãŸã¯ prediction_proba åˆ—ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "        return {\"count\": 0, \"low_threshold\": low, \"high_threshold\": high}\n",
    "\n",
    "    df = false_negatives_df\n",
    "    low_df = df[df[\"prediction_proba\"] < low]\n",
    "    mid_df = df[(df[\"prediction_proba\"] >= low) & (df[\"prediction_proba\"] < high)]\n",
    "    high_df = df[df[\"prediction_proba\"] >= high]\n",
    "\n",
    "    print(f\"ğŸ¯ ä½ç¢ºç‡é ˜åŸŸåˆ†æï¼ˆã—ãã„å€¤ low={low}, high={high}ï¼‰\")\n",
    "    print(f\"  - MLç¢ºç‡ < {low:.2f}: {len(low_df):,}ä»¶\")\n",
    "    if detailed:\n",
    "        print(f\"  - {low:.2f} <= MLç¢ºç‡ < {high:.2f}: {len(mid_df):,}ä»¶\")\n",
    "        print(f\"  - MLç¢ºç‡ >= {high:.2f}: {len(high_df):,}ä»¶\")\n",
    "\n",
    "    stats = {\n",
    "        \"low_region_count\": int(len(low_df)),\n",
    "        \"mid_region_count\": int(len(mid_df)),\n",
    "        \"high_region_count\": int(len(high_df)),\n",
    "        \"low_threshold\": float(low),\n",
    "        \"high_threshold\": float(high),\n",
    "    }\n",
    "    if detailed and len(low_df) > 0:\n",
    "        stats.update({\n",
    "            \"low_mean\": float(low_df[\"prediction_proba\"].mean()),\n",
    "            \"low_median\": float(low_df[\"prediction_proba\"].median()),\n",
    "        })\n",
    "    return stats\n",
    "\n",
    "# é–¾å€¤ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å…¬é–‹\n",
    "if \"cfg\" in globals():\n",
    "    __th = _get_thresholds(cfg)\n",
    "    FN_THRESHOLD_LOW = __th[\"low\"]; FN_THRESHOLD_HIGH = __th[\"high\"]; ENABLE_DETAILED_STATS = __th[\"detailed\"]\n",
    "    globals().update({\"FN_THRESHOLD_LOW\": FN_THRESHOLD_LOW, \"FN_THRESHOLD_HIGH\": FN_THRESHOLD_HIGH, \"ENABLE_DETAILED_STATS\": ENABLE_DETAILED_STATS})\n",
    "    print(f\"ğŸ” é–¾å€¤ã‚’è¨­å®š: low={FN_THRESHOLD_LOW}, high={FN_THRESHOLD_HIGH}, detailed={ENABLE_DETAILED_STATS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2195d3a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:18:35.329477Z",
     "iopub.status.busy": "2026-02-02T13:18:35.329362Z",
     "iopub.status.idle": "2026-02-02T13:18:35.345587Z",
     "shell.execute_reply": "2026-02-02T13:18:35.345200Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# === æœ€çµ‚ã‚»ãƒ«: Controller APIé–¢æ•° (agent_minimal) (è¿½è¨˜) ===\n",
    "from typing import Tuple, Dict, Any, Optional, List\n",
    "import os, json, joblib, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "# === CHANGELOG ===\n",
    "# 2025-12-28: artifacts/{session_id}/... æ§‹æˆã«åˆã‚ã›ã€brand_keywords ã¨ false_negatives ã®æ¢ç´¢ãƒ‘ã‚¹ã‚’æ›´æ–°\n",
    "\n",
    "def _find_first(*candidates: str) -> Optional[str]:\n",
    "    for p in candidates:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _load_false_negatives(session_id: str):\n",
    "    import pandas as pd\n",
    "    # å„ªå…ˆ: artifacts/{session_id}/resultsï¼ˆç¾è¡Œã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆï¼‰\n",
    "    cands = [\n",
    "        f\"artifacts/{session_id}/results/false_negatives_reconstructed.pkl\",\n",
    "        f\"artifacts/{session_id}/results/false_negatives.pkl\",\n",
    "        # äº’æ›ï¼ˆæ—§æ§‹æˆï¼‰\n",
    "        f\"results/{session_id}/false_negatives_reconstructed.pkl\",\n",
    "        f\"models/{session_id}/false_negatives_reconstructed.pkl\",\n",
    "        f\"results/{session_id}/false_negatives.pkl\",\n",
    "    ]\n",
    "    p = _find_first(*cands)\n",
    "    if not p:\n",
    "        return None\n",
    "    try:\n",
    "        obj = joblib.load(p)\n",
    "        if isinstance(obj, pd.DataFrame):\n",
    "            return obj\n",
    "        if isinstance(obj, dict):\n",
    "            if \"analysis_df\" in obj and isinstance(obj[\"analysis_df\"], pd.DataFrame):\n",
    "                return obj[\"analysis_df\"]\n",
    "            if \"domains\" in obj and \"predictions\" in obj:\n",
    "                return pd.DataFrame({\n",
    "                    \"domain\": obj[\"domains\"],\n",
    "                    \"prediction_proba\": obj[\"predictions\"],\n",
    "                    \"source\": obj.get(\"sources\"),\n",
    "                })\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def _normalize_fn_df(fn_df):\n",
    "    \"\"\"\n",
    "    Normalize FN dataframe schema across versions.\n",
    "    Ensures:\n",
    "      - \"prediction_proba\" column exists (float) if any known probability column exists.\n",
    "      - \"domain\" column exists if any known domain-like column exists.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    if fn_df is None:\n",
    "        return None\n",
    "    if not isinstance(fn_df, pd.DataFrame):\n",
    "        return fn_df\n",
    "    df = fn_df.copy()\n",
    "\n",
    "    # Domain column normalization\n",
    "    if \"domain\" not in df.columns:\n",
    "        for c in (\"hostname\", \"host\", \"url\", \"fqdn\", \"domain_name\"):\n",
    "            if c in df.columns:\n",
    "                df[\"domain\"] = df[c].astype(str)\n",
    "                break\n",
    "\n",
    "    # Probability column normalization\n",
    "    if \"prediction_proba\" not in df.columns:\n",
    "        for c in (\n",
    "            \"ml_probability\", \"ml_prob\", \"proba\", \"pred_proba\", \"xgb_proba\",\n",
    "            \"xgb_probability\", \"prediction_probability\", \"score\"\n",
    "        ):\n",
    "            if c in df.columns:\n",
    "                df[\"prediction_proba\"] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "                break\n",
    "    # Final fallback\n",
    "    if \"prediction_proba\" not in df.columns and \"prediction\" in df.columns:\n",
    "        df[\"prediction_proba\"] = pd.to_numeric(df[\"prediction\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _load_brand_keywords(session_id: str) -> List[str]:\n",
    "    # å„ªå…ˆ: artifacts/{session_id}/modelsï¼ˆç¾è¡Œã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆï¼‰\n",
    "    p1 = f\"artifacts/{session_id}/models/brand_keywords.json\"\n",
    "    if os.path.exists(p1):\n",
    "        try:\n",
    "            return json.loads(Path(p1).read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # äº’æ›: æ—§æ§‹æˆ\n",
    "    p2 = f\"models/{session_id}/brand_keywords.json\"\n",
    "    if os.path.exists(p2):\n",
    "        try:\n",
    "            return json.loads(Path(p2).read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: artifacts é…ä¸‹ã®æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ¢ç´¢ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‚‚ã®ï¼‰\n",
    "    try:\n",
    "        artifacts_dir = Path(\"artifacts\")\n",
    "        dirs = sorted([d for d in artifacts_dir.glob(\"*\") if d.is_dir()])\n",
    "        for d in reversed(dirs):\n",
    "            p = d / \"models\" / \"brand_keywords.json\"\n",
    "            if p.exists():\n",
    "                return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—§æ§‹æˆï¼ˆmodels/*ï¼‰\n",
    "    try:\n",
    "        models_dir = Path(\"models\")\n",
    "        dirs = sorted([d for d in models_dir.glob(\"*\") if d.is_dir()])\n",
    "        for d in reversed(dirs):\n",
    "            p = d / \"brand_keywords.json\"\n",
    "            if p.exists():\n",
    "                return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return []\n",
    "\n",
    "def _ensure_dirs(base: Path) -> Dict[str, str]:\n",
    "    results_dir = base / \"results\"\n",
    "    handoff_dir = base / \"handoff\"\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    handoff_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return {\"results_dir\": str(results_dir), \"handoff_dir\": str(handoff_dir)}\n",
    "\n",
    "def _simple_risk_correction(df, brand_keywords: List[str], low_th: float):\n",
    "    import pandas as pd\n",
    "    df = df.copy()\n",
    "    if \"prediction_proba\" not in df.columns or \"domain\" not in df.columns:\n",
    "        return pd.DataFrame(columns=[\"domain\",\"prediction_proba\",\"corrected\",\"reason\"])\n",
    "    brands = [b.lower() for b in (brand_keywords or [])]\n",
    "    def _has_brand(d: str) -> bool:\n",
    "        d = str(d).lower()\n",
    "        return any(b and b in d for b in brands[:200])\n",
    "    def _tld(d: str) -> str:\n",
    "        parts = str(d).lower().split(\".\")\n",
    "        return parts[-1] if len(parts) > 1 else \"\"\n",
    "    def _short(d: str) -> bool:\n",
    "        left = str(d).split(\".\")[0]\n",
    "        return len(left) < 10\n",
    "    corrected, reasons = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        ml = float(row.get(\"prediction_proba\", 1.0)); d = str(row.get(\"domain\",\"\"))\n",
    "        c = False; reason = []\n",
    "        if ml < low_th:\n",
    "            if _has_brand(d): c=True; reason.append(\"brand+lowML\")\n",
    "            if _short(d):     c=True; reason.append(\"short+lowML\")\n",
    "            tl = _tld(d)\n",
    "            if tl in (\"tk\",\"gq\",\"ml\",\"cf\",\"ga\"):\n",
    "                c=True; reason.append(f\"tld({tl})+lowML\")\n",
    "        corrected.append(c); reasons.append(\",\".join(reason))\n",
    "    df[\"corrected\"] = corrected; df[\"reason\"] = reasons\n",
    "    return df\n",
    "\n",
    "def agent_minimal(session_id: str, inputs: Dict[str, Any] = {\"mode\":\"fn\"}, cfg: Dict[str, Any] = None) -> Tuple[str, Dict[str, str]]:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    try:\n",
    "        if not isinstance(session_id, str) or not session_id.strip():\n",
    "            return \"INVALID_INPUT\", {\"error\": \"session_idã¯å¿…é ˆã§ã™\"}\n",
    "\n",
    "        local_cfg = load_configuration(cfg_override=(cfg or {}))\n",
    "        _ = apply_config_to_runtime(local_cfg)\n",
    "        if local_cfg.get(\"system\", {}).get(\"cert_only_mode\", False):\n",
    "            print(\"âš ï¸ æ³¨æ„: cert_only_modeã§ã™ãŒã€ãƒ–ãƒ©ãƒ³ãƒ‰å½è£…æ¤œå‡ºã¯ç ”ç©¶ã®æ ¸å¿ƒæ©Ÿèƒ½ã®ãŸã‚å®Ÿè¡Œã—ã¾ã™\")\n",
    "\n",
    "        base = Path(\"artifacts\") / session_id\n",
    "        dirs = _ensure_dirs(base)\n",
    "        results_dir = Path(dirs[\"results_dir\"]); handoff_dir = Path(dirs[\"handoff_dir\"])\n",
    "\n",
    "        brand_keywords = _load_brand_keywords(session_id)\n",
    "        if len(brand_keywords) < 1:\n",
    "            print(\"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬åˆ†æã®ã¿å®Ÿè¡Œã—ã¾ã™\")\n",
    "        elif len(brand_keywords) < 62:\n",
    "            print(f\"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ{len(brand_keywords)}ä»¶ (>=62ãŒæ¨å¥¨)\")\n",
    "\n",
    "        fn_df = _load_false_negatives(session_id)\n",
    "        fn_df = _normalize_fn_df(fn_df)\n",
    "        if fn_df is None or fn_df.empty:\n",
    "            return \"NOT_FOUND\", {\"error\": \"å½é™°æ€§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\", \"session_id\": session_id}\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        if \"prediction_proba\" not in fn_df.columns:\n",
    "            return \"ERROR\", {\"error\": \"FNãƒ‡ãƒ¼ã‚¿ã«ç¢ºç‡åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆprediction_proba/ml_probabilityç­‰ï¼‰ã€‚\", \"columns\": list(fn_df.columns), \"session_id\": session_id}\n",
    "        low_th = float(local_cfg.get(\"analysis\", {}).get(\"fn_threshold_low\", 0.2))\n",
    "        corr_df = _simple_risk_correction(fn_df, brand_keywords, low_th)\n",
    "\n",
    "        low_df = fn_df[fn_df[\"prediction_proba\"] < low_th]\n",
    "        corrected_low = corr_df[(corr_df[\"prediction_proba\"] < low_th) & (corr_df[\"corrected\"] == True)]\n",
    "        improvement_rate = (len(corrected_low) / max(1, len(low_df))) * 100.0\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        bars = [len(low_df)-len(corrected_low), len(corrected_low)]\n",
    "        plt.bar([\"æœªè£œæ­£\",\"è£œæ­£\"], bars)\n",
    "        plt.title(f\"ä½ç¢ºç‡é ˜åŸŸ(<{low_th:.2f})ã®è£œæ­£æ•° (æ”¹å–„ç‡ {improvement_rate:.1f}%)\")\n",
    "        plt.ylabel(\"ä»¶æ•°\")\n",
    "        out_improve = Path(results_dir) / \"fn_detection_improvement.png\"\n",
    "        plt.tight_layout(); plt.savefig(out_improve, dpi=150); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(6,4))\n",
    "        fn_df[\"prediction_proba\"].hist(bins=30)\n",
    "        try:\n",
    "            plt.axvline(x=low_th, color=\"red\", linestyle=\"--\", label=f\"low={low_th}\")\n",
    "            plt.legend()\n",
    "        except Exception:\n",
    "            pass\n",
    "        plt.title(\"å½é™°æ€§ã®äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒ\")\n",
    "        out_dist = Path(results_dir) / \"ml_probability_distribution.png\"\n",
    "        plt.tight_layout(); plt.savefig(out_dist, dpi=150); plt.close()\n",
    "\n",
    "        if \"corrected\" in corr_df.columns:\n",
    "            top_reasons = corr_df[corr_df[\"corrected\"]==True][\"reason\"].value_counts()[:5]\n",
    "            plt.figure(figsize=(6,4))\n",
    "            top_reasons.plot(kind=\"bar\")\n",
    "            plt.title(\"è£œæ­£ã®ä¸»å›  (ä¸Šä½)\")\n",
    "            out_brand = Path(results_dir) / \"brand_detection_results.png\"\n",
    "            plt.tight_layout(); plt.savefig(out_brand, dpi=150); plt.close()\n",
    "        else:\n",
    "            out_brand = Path(results_dir) / \"brand_detection_results.png\"; out_brand.touch()\n",
    "\n",
    "        report = {\n",
    "            \"session_id\": session_id,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"metrics\": {\n",
    "                \"low_threshold\": low_th,\n",
    "                \"low_region_count\": int(len(low_df)),\n",
    "                \"corrected_low_count\": int(len(corrected_low)),\n",
    "                \"improvement_rate_pct\": float(improvement_rate),\n",
    "            },\n",
    "            \"notes\": [\n",
    "                \"LangGraphæœªä½¿ç”¨ã®æœ€å°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\",\n",
    "                \"vLLMâ†’Ollama ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ apply_config_to_runtime ã§å®Ÿæ–½\",\n",
    "            ],\n",
    "        }\n",
    "        out_json = Path(results_dir) / \"agent_performance_report.json\"\n",
    "        out_json.write_text(json.dumps(report, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "        out_corr = Path(results_dir) / \"false_negatives_corrected.csv\"\n",
    "        corr_df.to_csv(out_corr, index=False, encoding=\"utf-8\")\n",
    "\n",
    "        handoff_obj = {\"false_negatives_df\": fn_df, \"corrected_df\": corr_df, \"brand_keywords\": brand_keywords, \"cfg\": local_cfg}\n",
    "        out_handoff = Path(dirs[\"handoff_dir\"]) / \"03_ai_agent_analysis_part1.pkl\"\n",
    "        joblib.dump(handoff_obj, out_handoff)\n",
    "\n",
    "        Paths = {\n",
    "            \"fn_detection_improvement\": str(out_improve),\n",
    "            \"ml_probability_distribution\": str(out_dist),\n",
    "            \"brand_detection_results\": str(out_brand),\n",
    "            \"agent_performance_report\": str(out_json),\n",
    "            \"false_negatives_corrected\": str(out_corr),\n",
    "            \"handoff\": str(out_handoff),\n",
    "        }\n",
    "        return \"OK\", Paths\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc(limit=2)\n",
    "        return \"ERROR\", {\"error\": str(e), \"traceback\": tb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdbd11aa-8055-4ba8-8597-bb3dda5707d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:18:35.348304Z",
     "iopub.status.busy": "2026-02-02T13:18:35.348188Z",
     "iopub.status.idle": "2026-02-02T13:18:49.740044Z",
     "shell.execute_reply": "2026-02-02T13:18:49.739369Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ†æ(Part 1)ã‚’é–‹å§‹ã—ã¾ã™... (Target Session ID: 2026-01-24_213326)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: config.json\n",
      "âœ… Config loaded successfully (provider=vllm, gpu_auto_detect=True)\n",
      "ğŸ”§ DBè¨­å®š: {'dbname': 'rapids_data', 'user': 'postgres', 'host': 'localhost', 'port': '5432'} (timeout_s=30)\n",
      "âœ… GPUæ¤œå‡º: åˆ©ç”¨å¯èƒ½\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ LLMæ¥ç¶šãƒã‚§ãƒƒã‚¯å¤±æ•— (http://localhost:8000/v1): Connection error.\n",
      "âŒ LLMæ¥ç¶šä¸å¯ (provider=vllm, url=http://localhost:8000/v1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Execution Result] Status: OK\n",
      "âœ… å‡¦ç†å®Œäº†ã€‚ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\n",
      " - fn_detection_improvement: artifacts/2026-01-24_213326/results/fn_detection_improvement.png\n",
      " - ml_probability_distribution: artifacts/2026-01-24_213326/results/ml_probability_distribution.png\n",
      " - brand_detection_results: artifacts/2026-01-24_213326/results/brand_detection_results.png\n",
      " - agent_performance_report: artifacts/2026-01-24_213326/results/agent_performance_report.json\n",
      " - false_negatives_corrected: artifacts/2026-01-24_213326/results/false_negatives_corrected.csv\n",
      " - handoff: artifacts/2026-01-24_213326/handoff/03_ai_agent_analysis_part1.pkl\n"
     ]
    }
   ],
   "source": [
    "# === ã“ã®ã‚»ãƒ«ã‚’è¿½åŠ ã—ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼ˆå³æ ¼ãƒ¢ãƒ¼ãƒ‰ï¼‰ ===\n",
    "import sys\n",
    "import _compat.paths as paths\n",
    "\n",
    "# 1. å®Ÿè¡Œå¯¾è±¡ã® Session ID ã‚’ç‰¹å®š\n",
    "target_sid = None\n",
    "if 'ai_session_id' in globals() and ai_session_id:\n",
    "    target_sid = ai_session_id\n",
    "elif 'session_id' in globals() and session_id:\n",
    "    target_sid = session_id\n",
    "else:\n",
    "    target_sid = paths.RUN_ID \n",
    "\n",
    "print(f\"ğŸš€ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ†æ(Part 1)ã‚’é–‹å§‹ã—ã¾ã™... (Target Session ID: {target_sid})\")\n",
    "\n",
    "if not target_sid:\n",
    "    print(\"âŒ Session ID ãŒç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 2. é–¢æ•°ã‚’å®Ÿè¡Œ\n",
    "# agent_minimal å†…éƒ¨ã§ load_configuration ãŒå‘¼ã°ã‚Œã¾ã™ã€‚\n",
    "# config.json ãŒç„¡ã„å ´åˆã¯ãã“ã§ FileNotFoundError ãŒç™ºç”Ÿã—ã€åœæ­¢ã—ã¾ã™ã€‚\n",
    "try:\n",
    "    # cfgå¼•æ•°ã‚’Noneã«ã™ã‚‹ã“ã¨ã§ã€load_configurationã®è‡ªå‹•æ¢ç´¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’åˆ©ç”¨\n",
    "    status, result_paths = agent_minimal(session_id=target_sid, cfg=None)\n",
    "\n",
    "    print(f\"\\n[Execution Result] Status: {status}\")\n",
    "    if status == \"OK\":\n",
    "        print(\"âœ… å‡¦ç†å®Œäº†ã€‚ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "        for key, path in result_paths.items():\n",
    "            print(f\" - {key}: {path}\")\n",
    "    else:\n",
    "        print(\"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\\n\", result_paths)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nâ›” è¨­å®šã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\nâ›” è¨­å®šå€¤ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"\\nâŒ äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b0aa89f-e7df-4f93-8120-a4dfc729d0e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:18:49.742266Z",
     "iopub.status.busy": "2026-02-02T13:18:49.742129Z",
     "iopub.status.idle": "2026-02-02T13:18:49.744977Z",
     "shell.execute_reply": "2026-02-02T13:18:49.744410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False False\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "fonts = [f.name for f in fm.fontManager.ttflist]\n",
    "print(\"Noto Sans CJK JP\" in fonts, \"IPAexGothic\" in fonts, \"IPAGothic\" in fonts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81ac32-1e7e-48ec-b0f1-a923a279eb09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f72129-9a9e-4b7c-b0cc-9d6b561522df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
