{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] BASE_DIR = /home/asomura/nextstep\n",
      "[INFO] RUN_ID = 2025-11-18_053624\n",
      "[INFO] ARTIFACTS_DIR = /home/asomura/nextstep/artifacts/2025-11-18_053624\n",
      "[INFO] Timestamp: 2025-11-20_185414\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 1: 環境設定と初期化\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# パスの設定\n",
    "BASE_DIR = Path(\"/home/asomura/nextstep\")\n",
    "sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "# _compatを使用（オプション）\n",
    "try:\n",
    "    from _compat import paths\n",
    "    cfg = paths.load_config(os.getenv(\"CONFIG_JSON\"), strict=False)\n",
    "    RUN_ID = cfg.get(\"run_id\") or os.getenv(\"RUN_ID\") or \"20251030_004055\"\n",
    "except ImportError:\n",
    "    RUN_ID = \"20251030_004055\"\n",
    "    print(\"[INFO] _compat not found, using default RUN_ID\")\n",
    "\n",
    "ARTIFACTS_DIR = BASE_DIR / \"artifacts\" / RUN_ID\n",
    "HANDOFF_DIR = ARTIFACTS_DIR / \"handoff\"\n",
    "LOGS_DIR = ARTIFACTS_DIR / \"logs\"\n",
    "\n",
    "print(f\"[INFO] BASE_DIR = {BASE_DIR}\")\n",
    "print(f\"[INFO] RUN_ID = {RUN_ID}\")\n",
    "print(f\"[INFO] ARTIFACTS_DIR = {ARTIFACTS_DIR}\")\n",
    "\n",
    "# 出力用のタイムスタンプ\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "print(f\"[INFO] Timestamp: {timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Handoff file not found: /home/asomura/nextstep/artifacts/2025-11-18_053624/handoff/04-3_llm_tools_setup_with_tools.pkl\n",
      "[INFO] Trying alternative path...\n",
      "[INFO] external_data loaded from: /home/asomura/nextstep/artifacts/20251030_004055/handoff/04-3_llm_tools_setup_with_tools.pkl\n",
      "[INFO] external_data keys: ['cfg', 'brand_keywords', 'cert_full_info_map', 'tools', 'llm', 'async_client', 'tools_code', 'present_top'] ...\n",
      "[INFO] Current brand_keywords count: 100\n",
      "[INFO] Added 39 new brands\n",
      "[INFO] Total brand_keywords: 139\n",
      "\n",
      "[Brand Check]\n",
      "  mufg            ✅\n",
      "  smbc            ✅\n",
      "  amazon          ✅\n",
      "  mercari         ✅\n",
      "  rakuten         ✅\n",
      "  metamask        ✅\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 2: External Dataの読み込みとBrand Keywords補強\n",
    "# ============================================\n",
    "\n",
    "# handoffからexternal_dataを読み込み\n",
    "handoff_path = HANDOFF_DIR / \"04-3_llm_tools_setup_with_tools.pkl\"\n",
    "\n",
    "if not handoff_path.exists():\n",
    "    print(f\"[ERROR] Handoff file not found: {handoff_path}\")\n",
    "    print(\"[INFO] Trying alternative path...\")\n",
    "    handoff_path = Path(\"/home/asomura/nextstep/artifacts/20251030_004055/handoff/04-3_llm_tools_setup_with_tools.pkl\")\n",
    "\n",
    "with open(handoff_path, 'rb') as f:\n",
    "    external_data = pickle.load(f)\n",
    "\n",
    "print(f\"[INFO] external_data loaded from: {handoff_path}\")\n",
    "print(f\"[INFO] external_data keys: {list(external_data.keys())[:8]} ...\")\n",
    "\n",
    "# brand_keywordsの確認と補強\n",
    "if 'brand_keywords' not in external_data:\n",
    "    external_data['brand_keywords'] = []\n",
    "    print(\"[WARNING] brand_keywords not found, creating new list\")\n",
    "\n",
    "# 現在のブランドを小文字で取得\n",
    "brands_lower = [b.lower() for b in external_data['brand_keywords'] if isinstance(b, str)]\n",
    "print(f\"[INFO] Current brand_keywords count: {len(external_data['brand_keywords'])}\")\n",
    "\n",
    "# 重要ブランドのリスト（日本の金融機関を重点的に）\n",
    "essential_brands = {\n",
    "    # 日本の金融機関\n",
    "    \"mufg\", \"smbc\", \"mizuho\", \"ufj\", \"mitsubishi\", \"sumitomo\", \"mitsui\",\n",
    "    \"resona\", \"jcb\", \"visa\", \"mastercard\", \"amex\",\n",
    "    \"japanpost\", \"jppost\", \"yucho\", \"jabank\", \"shinkin\",\n",
    "    # 日本のサービス\n",
    "    \"rakuten\", \"mercari\", \"yahoo\", \"line\", \"docomo\", \"au\", \"softbank\",\n",
    "    \"ntt\", \"kddi\", \"uniqlo\", \"muji\", \"nitori\",\n",
    "    # グローバルブランド\n",
    "    \"amazon\", \"google\", \"apple\", \"microsoft\", \"paypal\", \"netflix\",\n",
    "    \"facebook\", \"meta\", \"instagram\", \"twitter\", \"x\",\n",
    "    \"metamask\", \"binance\", \"coinbase\", \"ethereum\", \"bitcoin\",\n",
    "    # 短縮URL・CDN\n",
    "    \"bit\", \"tinyurl\", \"shorturl\", \"cdn\", \"cloudflare\", \"akamai\"\n",
    "}\n",
    "\n",
    "# 不足しているブランドを追加\n",
    "added_brands = []\n",
    "for brand in essential_brands:\n",
    "    if brand not in brands_lower:\n",
    "        external_data['brand_keywords'].append(brand)\n",
    "        added_brands.append(brand)\n",
    "\n",
    "print(f\"[INFO] Added {len(added_brands)} new brands\")\n",
    "print(f\"[INFO] Total brand_keywords: {len(external_data['brand_keywords'])}\")\n",
    "\n",
    "# 主要ブランドの確認表示\n",
    "check_brands = [\"mufg\", \"smbc\", \"amazon\", \"mercari\", \"rakuten\", \"metamask\"]\n",
    "print(\"\\n[Brand Check]\")\n",
    "for brand in check_brands:\n",
    "    exists = brand in [b.lower() for b in external_data['brand_keywords']]\n",
    "    status = \"✅\" if exists else \"❌\"\n",
    "    print(f\"  {brand:15} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading existing CSV from: /home/asomura/nextstep/artifacts/2025-11-16_184237/logs/random_eval_domains_latest.csv\n",
      "[INFO] Random100 loaded: 100 domains\n",
      "            domain  ml_probability\n",
      "0          jmbf.cn        0.453648\n",
      "1  baidu-xiamen.cn        0.244429\n",
      "2      tsjianye.cn        0.236587\n",
      "3    internetku.id        0.129556\n",
      "4  teslafarmer.com        0.094156\n",
      "\n",
      "[ML Probability Distribution]\n",
      "  Min:    0.004\n",
      "  Max:    0.497\n",
      "  Mean:   0.201\n",
      "  Median: 0.179\n",
      "  High risk (>0.4): 10 domains\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 3: Random100データの準備（元のノートブック準拠）\n",
    "# ============================================\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# データ探索用ヘルパー関数\n",
    "def _search_eval_df(obj):\n",
    "    '''dict/list 再帰で DataFrame を探す'''\n",
    "    if obj is None:\n",
    "        return None\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        return obj\n",
    "    if isinstance(obj, dict):\n",
    "        # よくあるキー直参照\n",
    "        for k in (\"false_negatives_df\", \"fn_df\", \"eval_df\", \"random_eval_df\"):\n",
    "            v = obj.get(k)\n",
    "            if isinstance(v, pd.DataFrame):\n",
    "                return v\n",
    "        for v in obj.values():\n",
    "            hit = _search_eval_df(v)\n",
    "            if hit is not None:\n",
    "                return hit\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        for v in obj:\n",
    "            hit = _search_eval_df(v)\n",
    "            if hit is not None:\n",
    "                return hit\n",
    "    return None\n",
    "\n",
    "def _normalize_eval_df(df):\n",
    "    '''列名の揺れに対応して domain, ml_probability の2列に正規化'''\n",
    "    print(\"[DEBUG] columns:\", list(df.columns))\n",
    "    \n",
    "    # lower → 元名のマップ\n",
    "    lower2orig = {c.lower(): c for c in df.columns}\n",
    "    \n",
    "    # 1) domain候補\n",
    "    domain_candidates = [\"domain\", \"fqdn\", \"domain_name\", \"hostname\", \"host\", \"requested_host\"]\n",
    "    domain_key = None\n",
    "    for key in domain_candidates:\n",
    "        if key in lower2orig:\n",
    "            domain_key = lower2orig[key]\n",
    "            break\n",
    "    \n",
    "    if domain_key is None:\n",
    "        # 部分一致\n",
    "        for c in df.columns:\n",
    "            if any(kw in c.lower() for kw in [\"domain\", \"fqdn\", \"host\", \"url\"]):\n",
    "                domain_key = c\n",
    "                print(f\"[DEBUG] domain fallback: {c}\")\n",
    "                break\n",
    "    \n",
    "    # 2) ml_probability候補\n",
    "    ml_candidates = [\"ml_probability\", \"ml_prob\", \"probability\", \"prediction_proba\", \n",
    "                     \"score\", \"pred_proba\", \"proba\", \"confidence\"]\n",
    "    mlp_key = None\n",
    "    for key in ml_candidates:\n",
    "        if key in lower2orig:\n",
    "            mlp_key = lower2orig[key]\n",
    "            break\n",
    "    \n",
    "    if mlp_key is None:\n",
    "        # floatカラムから推測\n",
    "        float_cols = [c for c in df.columns if pd.api.types.is_float_dtype(df[c])]\n",
    "        for c in float_cols:\n",
    "            if any(kw in c.lower() for kw in [\"prob\", \"score\", \"pred\"]):\n",
    "                mlp_key = c\n",
    "                print(f\"[DEBUG] ml_probability fallback: {c}\")\n",
    "                break\n",
    "    \n",
    "    if domain_key is None or mlp_key is None:\n",
    "        print(\"[ERROR] could not infer domain/ml_probability columns\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"[DEBUG] chosen columns: domain={domain_key}, ml_prob={mlp_key}\")\n",
    "    \n",
    "    tmp = df[[domain_key, mlp_key]].copy()\n",
    "    tmp.columns = [\"domain\", \"ml_probability\"]\n",
    "    \n",
    "    # url→domainの正規化\n",
    "    if \"url\" in domain_key.lower():\n",
    "        def _to_domain(x):\n",
    "            if not isinstance(x, str):\n",
    "                return \"\"\n",
    "            if \"://\" in x:\n",
    "                netloc = urlparse(x).netloc\n",
    "            else:\n",
    "                netloc = x\n",
    "            netloc = netloc.split(\"@\")[-1].split(\":\")[0]\n",
    "            return netloc.lower()\n",
    "        tmp[\"domain\"] = tmp[\"domain\"].map(_to_domain)\n",
    "    \n",
    "    # クリーンアップ\n",
    "    tmp[\"domain\"] = tmp[\"domain\"].astype(str).str.strip().str.lower()\n",
    "    tmp = tmp[tmp[\"domain\"].str.len() > 0]\n",
    "    tmp[\"ml_probability\"] = pd.to_numeric(tmp[\"ml_probability\"], errors=\"coerce\")\n",
    "    tmp = tmp[(tmp[\"ml_probability\"] >= 0.0) & (tmp[\"ml_probability\"] <= 1.0)]\n",
    "    tmp = tmp.dropna(subset=[\"ml_probability\"]).reset_index(drop=True)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# Random100データを取得\n",
    "def get_random100_domains():\n",
    "    '''Random100のドメインリストを取得（元のノートブックのロジック準拠）'''\n",
    "    \n",
    "    # 方法1: 既存のCSVから\n",
    "    csv_paths = [\n",
    "        LOGS_DIR / \"random_eval_domains_latest.csv\",\n",
    "        ARTIFACTS_DIR / \"logs\" / \"random_eval_domains_latest.csv\",\n",
    "        Path(\"/home/asomura/nextstep/artifacts/2025-11-16_184237/logs/random_eval_domains_latest.csv\")\n",
    "    ]\n",
    "    \n",
    "    for csv_path in csv_paths:\n",
    "        if csv_path.exists():\n",
    "            print(f\"[INFO] Loading existing CSV from: {csv_path}\")\n",
    "            df = pd.read_csv(csv_path)\n",
    "            return df\n",
    "    \n",
    "    # 方法2: pickleから生成（元のノートブックのロジック）\n",
    "    print(\"[INFO] CSV not found, generating from pickle...\")\n",
    "    \n",
    "    pickle_paths = [\n",
    "        HANDOFF_DIR / \"04-2_statistical_analysis.pkl\",\n",
    "        HANDOFF_DIR / \"04-3_llm_tools_setup_with_tools.pkl\",\n",
    "        Path(\"/home/asomura/nextstep/artifacts/20251030_004055/handoff/04-2_statistical_analysis.pkl\"),\n",
    "        Path(\"/home/asomura/nextstep/artifacts/20251030_004055/handoff/04-3_llm_tools_setup_with_tools.pkl\")\n",
    "    ]\n",
    "    \n",
    "    eval_source_df = None\n",
    "    for pickle_path in pickle_paths:\n",
    "        if not pickle_path.exists():\n",
    "            continue\n",
    "            \n",
    "        print(f\"[INFO] Trying: {pickle_path}\")\n",
    "        try:\n",
    "            with open(pickle_path, 'rb') as f:\n",
    "                obj = pickle.load(f)\n",
    "            \n",
    "            # DataFrameを探す\n",
    "            raw_df = _search_eval_df(obj)\n",
    "            if raw_df is not None and len(raw_df) > 0:\n",
    "                print(f\"[INFO] Found DataFrame with {len(raw_df)} rows\")\n",
    "                eval_source_df = raw_df\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to load {pickle_path}: {e}\")\n",
    "    \n",
    "    if eval_source_df is None:\n",
    "        raise RuntimeError(\n",
    "            \"評価用DataFrameが見つかりません。\\n\"\n",
    "            \"04-2 または 04-3 のpickleファイルを確認してください。\"\n",
    "        )\n",
    "    \n",
    "    # 正規化\n",
    "    norm_df = _normalize_eval_df(eval_source_df)\n",
    "    if norm_df is None:\n",
    "        raise RuntimeError(\"DataFrameの正規化に失敗しました\")\n",
    "    \n",
    "    print(f\"[INFO] Source rows (normalized): {len(norm_df)}\")\n",
    "    \n",
    "    # ランダムサンプリング（元のノートブックは固定シードなし）\n",
    "    sample_n = min(100, len(norm_df))\n",
    "    random100 = norm_df.sample(n=sample_n).reset_index(drop=True)\n",
    "    \n",
    "    # CSV保存\n",
    "    out_csv = LOGS_DIR / \"random_eval_domains_latest.csv\"\n",
    "    LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    random100.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[INFO] Random100 saved to: {out_csv}\")\n",
    "    \n",
    "    return random100\n",
    "\n",
    "# Random100データを取得\n",
    "random100_df = get_random100_domains()\n",
    "print(f\"[INFO] Random100 loaded: {len(random100_df)} domains\")\n",
    "print(random100_df.head())\n",
    "\n",
    "# ML確率の分布を確認\n",
    "print(f\"\\n[ML Probability Distribution]\")\n",
    "print(f\"  Min:    {random100_df['ml_probability'].min():.3f}\")\n",
    "print(f\"  Max:    {random100_df['ml_probability'].max():.3f}\")\n",
    "print(f\"  Mean:   {random100_df['ml_probability'].mean():.3f}\")\n",
    "print(f\"  Median: {random100_df['ml_probability'].median():.3f}\")\n",
    "\n",
    "# 高リスクドメイン（ML確率 > 0.4）の数\n",
    "high_risk = random100_df[random100_df['ml_probability'] > 0.4]\n",
    "print(f\"  High risk (>0.4): {len(high_risk)} domains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AGENT INITIALIZATION WITH LLM\n",
      "================================================================================\n",
      "\n",
      "[1] Fixing LLM Configuration\n",
      "----------------------------------------\n",
      "✅ LLM configuration enabled\n",
      "\n",
      "[2] Creating LLM Client\n",
      "----------------------------------------\n",
      "✅ LLM client created and set\n",
      "\n",
      "[3] Wiring Phase6\n",
      "----------------------------------------\n",
      "✅ Phase6 wired with real LLM\n",
      "\n",
      "[4] Importing LangGraphPhishingAgent\n",
      "----------------------------------------\n",
      "✅ LangGraphPhishingAgent imported\n",
      "\n",
      "[6] Initializing Agent\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'phishpkg.agent_foundations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[6] Initializing Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mLangGraphPhishingAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexternal_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_data\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Agent initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# LLM設定の確認と強制有効化\u001b[39;00m\n",
      "File \u001b[0;32m~/nextstep/phishing_agent/langgraph_module.py:181\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, strict_mode, use_llm_selection, use_llm_decision, config_path, external_data)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m     external_data: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    179\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrict_mode \u001b[38;5;241m=\u001b[39m strict_mode\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_llm_selection \u001b[38;5;241m=\u001b[39m use_llm_selection\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_llm_decision \u001b[38;5;241m=\u001b[39m use_llm_decision\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config \u001b[38;5;241m=\u001b[39m load_llm_config(config_path)\n",
      "File \u001b[0;32m~/nextstep/phishing_agent/langgraph_module.py:185\u001b[0m, in \u001b[0;36m_build_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config \u001b[38;5;241m=\u001b[39m load_llm_config(config_path)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mso \u001b[38;5;241m=\u001b[39m _SOClient(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_config)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexternal_data \u001b[38;5;241m=\u001b[39m external_data \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_version \u001b[38;5;241m=\u001b[39m __version__\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_graph() \u001b[38;5;28;01mif\u001b[39;00m _HAS_LANGGRAPH \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.04/lib/python3.9/site-packages/langgraph/graph/state.py:294\u001b[0m, in \u001b[0;36mStateGraph.__init__\u001b[0;34m(self, state_schema, config_schema, input_schema, output_schema, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_schema \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mtype\u001b[39m[OutputT], output_schema \u001b[38;5;129;01mor\u001b[39;00m state_schema)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_schema \u001b[38;5;241m=\u001b[39m config_schema\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_schema, allow_managed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_schema, allow_managed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.04/lib/python3.9/site-packages/langgraph/graph/state.py:307\u001b[0m, in \u001b[0;36mStateGraph._add_schema\u001b[0;34m(self, schema, allow_managed)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschemas:\n\u001b[1;32m    306\u001b[0m     _warn_invalid_state_schema(schema)\n\u001b[0;32m--> 307\u001b[0m     channels, managed, type_hints \u001b[38;5;241m=\u001b[39m \u001b[43m_get_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m managed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_managed:\n\u001b[1;32m    309\u001b[0m         names \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(managed)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.04/lib/python3.9/site-packages/langgraph/graph/state.py:1301\u001b[0m, in \u001b[0;36m_get_channels\u001b[0;34m(schema)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__annotations__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m   1296\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m: _get_channel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m, schema, allow_managed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)},\n\u001b[1;32m   1297\u001b[0m         {},\n\u001b[1;32m   1298\u001b[0m         {},\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[0;32m-> 1301\u001b[0m type_hints \u001b[38;5;241m=\u001b[39m \u001b[43mget_type_hints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_extras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1302\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1303\u001b[0m     name: _get_channel(name, typ)\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, typ \u001b[38;5;129;01min\u001b[39;00m type_hints\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__slots__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1306\u001b[0m }\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m   1308\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m all_keys\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, BaseChannel)},\n\u001b[1;32m   1309\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m all_keys\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m is_managed_value(v)},\n\u001b[1;32m   1310\u001b[0m     type_hints,\n\u001b[1;32m   1311\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-24.04/lib/python3.9/typing.py:1450\u001b[0m, in \u001b[0;36mget_type_hints\u001b[0;34m(obj, globalns, localns, include_extras)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__mro__\u001b[39m):\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m globalns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1450\u001b[0m         base_globals \u001b[38;5;241m=\u001b[39m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1452\u001b[0m         base_globals \u001b[38;5;241m=\u001b[39m globalns\n",
      "\u001b[0;31mKeyError\u001b[0m: 'phishpkg.agent_foundations'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Cell 4: LangGraphエージェントの初期化（Random100用完全版）\n",
    "# ============================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import types\n",
    "\n",
    "# 環境設定\n",
    "os.chdir(BASE_DIR)\n",
    "phishing_agent_path = BASE_DIR / \"phishing_agent\"\n",
    "if str(phishing_agent_path) not in sys.path:\n",
    "    sys.path.insert(0, str(phishing_agent_path))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AGENT INITIALIZATION WITH LLM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. VLLMのダミーAPIキーを設定（必須）\n",
    "os.environ['OPENAI_API_KEY'] = 'dummy-key-for-vllm'\n",
    "base_url = \"http://192.168.100.71:30000\"\n",
    "\n",
    "# 2. LLM設定を強制有効化\n",
    "print(\"\\n[1] Fixing LLM Configuration\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if 'external_data' not in globals():\n",
    "    print(\"❌ external_data not found! Run Cell 2 first.\")\n",
    "else:\n",
    "    if 'cfg' not in external_data:\n",
    "        external_data['cfg'] = {}\n",
    "    \n",
    "    external_data['cfg']['llm'] = {\n",
    "        'enabled': True,\n",
    "        'provider': 'vllm',\n",
    "        'base_url': base_url,\n",
    "        'vllm_base_url': base_url,\n",
    "        'model': 'Qwen/Qwen3-14B-FP8',\n",
    "        'vllm_model': 'Qwen/Qwen3-14B-FP8'\n",
    "    }\n",
    "    print(\"✅ LLM configuration enabled\")\n",
    "\n",
    "# 3. phishpkgをクリア（古いものがあれば）\n",
    "for key in list(sys.modules.keys()):\n",
    "    if key.startswith('phishpkg'):\n",
    "        del sys.modules[key]\n",
    "\n",
    "# 4. LLMクライアントを作成\n",
    "print(\"\\n[2] Creating LLM Client\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    from langchain_community.llms import VLLMOpenAI\n",
    "    \n",
    "    llm_client = VLLMOpenAI(\n",
    "        openai_api_base=base_url,\n",
    "        openai_api_key=\"dummy\",\n",
    "        model_name=\"Qwen/Qwen3-14B-FP8\",\n",
    "        temperature=0.1,\n",
    "        max_tokens=1024,\n",
    "    )\n",
    "    \n",
    "    external_data['llm'] = llm_client\n",
    "    print(\"✅ LLM client created and set\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ LLM client creation failed: {e}\")\n",
    "\n",
    "# 5. Phase6配線（LLM有効）\n",
    "print(\"\\n[3] Wiring Phase6\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    from phishing_agent.phase6_wiring import wire_phase6\n",
    "    wire_phase6(prefer_compat=True, fake_llm=False)  # fake_llm=Falseが重要！\n",
    "    print(\"✅ Phase6 wired with real LLM\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Phase6 wiring: {e}\")\n",
    "\n",
    "# 6. LangGraphエージェントをインポート\n",
    "print(\"\\n[4] Importing LangGraphPhishingAgent\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from phishing_agent.langgraph_module import LangGraphPhishingAgent\n",
    "print(\"✅ LangGraphPhishingAgent imported\")\n",
    "\n",
    "# ========================================\n",
    "# [COMMENTED OUT] Brand Detection Patching\n",
    "# ========================================\n",
    "# 改良版brand_impersonation_checkへの差し替えをコメントアウト\n",
    "# 理由: Cell 3（改良版の定義）を削除したため、未定義エラーを回避\n",
    "#\n",
    "# print(\"\\n[5] Patching Brand Detection\")\n",
    "# print(\"-\" * 40)\n",
    "#\n",
    "# if 'phishpkg.tools_module' in sys.modules:\n",
    "#     tools_module = sys.modules['phishpkg.tools_module']\n",
    "#     \n",
    "#     # 元の関数を保存\n",
    "#     if hasattr(tools_module, 'brand_impersonation_check'):\n",
    "#         tools_module._original_brand_check = tools_module.brand_impersonation_check\n",
    "#     \n",
    "#     # 改良版で置き換え\n",
    "#     tools_module.brand_impersonation_check = brand_impersonation_check_enhanced\n",
    "#     print(\"✅ Brand detection patched with enhanced version\")\n",
    "# else:\n",
    "#     print(\"⚠️ phishpkg.tools_module not found, brand detection might not be enhanced\")\n",
    "\n",
    "# 8. エージェント初期化\n",
    "print(\"\\n[6] Initializing Agent\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "agent = LangGraphPhishingAgent(\n",
    "    strict_mode=False,\n",
    "    external_data=external_data\n",
    ")\n",
    "print(\"✅ Agent initialized\")\n",
    "\n",
    "# LLM設定の確認と強制有効化\n",
    "if hasattr(agent, 'llm_config'):\n",
    "    if not agent.llm_config.enabled:\n",
    "        agent.llm_config.enabled = True\n",
    "        print(\"✅ Forced LLM enabled=True\")\n",
    "    print(f\"   LLM Status: enabled={agent.llm_config.enabled}\")\n",
    "\n",
    "# 9. 動作確認テスト\n",
    "print(\"\\n[7] Quick Verification Test\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "import time\n",
    "test_domain = \"test-amazon.com\"\n",
    "start = time.time()\n",
    "\n",
    "try:\n",
    "    result = agent.evaluate(test_domain, 0.35)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Test domain: {test_domain}\")\n",
    "    print(f\"  Time: {elapsed:.2f}s\")\n",
    "    print(f\"  is_phishing: {result.get('ai_is_phishing')}\")\n",
    "    \n",
    "    if elapsed > 2.0:\n",
    "        print(\"  ✅ LLM is working (slow response)\")\n",
    "    elif elapsed > 0.5:\n",
    "        print(\"  ⚠️ LLM might be working\")\n",
    "    else:\n",
    "        print(\"  ❌ LLM not working (too fast)\")\n",
    "        print(\"  ⚠️ Check VLLM server status\")\n",
    "    \n",
    "    # Brand検出の確認\n",
    "    brand_result = result.get('tool_results', {}).get('brand', {})\n",
    "    if brand_result.get('data', {}).get('detected_issues'):\n",
    "        brands = brand_result['data']['details'].get('detected_brands', [])\n",
    "        print(f\"  Brands detected: {brands}\")\n",
    "        print(\"  ✅ Brand detection is working\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Test failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AGENT READY FOR EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"✅ LLM: Enabled\")\n",
    "print(\"✅ Brand Detection: Using phishing_agent module version\")\n",
    "print(\"✅ Agent: Initialized\")\n",
    "print(\"\\nProceed to Cell 5 for Random100 evaluation\")\n",
    "print(\"Expected time: ~8-10 minutes for 100 domains\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Cell 5: Random100評価実行（修正版）\n",
    "# ============================================\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"[INFO] Starting evaluation of 100 domains...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# LLM設定の確認\n",
    "if 'agent' in globals() and hasattr(agent, 'llm_config'):\n",
    "    llm_config = agent.llm_config\n",
    "    if llm_config.enabled:\n",
    "        print(f\"[INFO] LLM initialized successfully: {llm_config.model} at {llm_config.base_url}\")\n",
    "    else:\n",
    "        print(\"[WARNING] LLM is disabled - results may be limited\")\n",
    "else:\n",
    "    print(\"[WARNING] Agent not properly initialized\")\n",
    "\n",
    "# 評価結果を格納\n",
    "results = []\n",
    "start_time = time.time()\n",
    "\n",
    "# 各ドメインを評価\n",
    "for idx, row in random100_df.iterrows():\n",
    "    domain = row['domain']\n",
    "    ml_prob = row['ml_probability']\n",
    "    \n",
    "    try:\n",
    "        # エージェントで評価\n",
    "        eval_start = time.time()\n",
    "        result = agent.evaluate(domain, ml_prob)\n",
    "        elapsed = time.time() - eval_start\n",
    "        \n",
    "        # detected_brandsを取得（修正版）\n",
    "        # まずトップレベルで探す\n",
    "        detected_brands = result.get('detected_brands', [])\n",
    "        \n",
    "        # もしトップレベルになければ、ai_assessmentから探す\n",
    "        if not detected_brands and 'ai_assessment' in result:\n",
    "            ai_assessment = result['ai_assessment']\n",
    "            if isinstance(ai_assessment, dict):\n",
    "                detected_brands = ai_assessment.get('detected_brands', [])\n",
    "        \n",
    "        # それでもなければ、risk_factorsから探す\n",
    "        if not detected_brands and 'risk_factors' in result:\n",
    "            risk_factors = result.get('risk_factors', {})\n",
    "            if isinstance(risk_factors, list):\n",
    "                # risk_factorsがリストの場合\n",
    "                for factor in risk_factors:\n",
    "                    if 'brand' in str(factor).lower():\n",
    "                        # ブランド関連のリスクファクターがある\n",
    "                        detected_brands = ['unknown_brand']\n",
    "                        break\n",
    "        \n",
    "        # 最後の手段：tool_resultsから直接取得を試みる（もし存在すれば）\n",
    "        if not detected_brands:\n",
    "            tool_results = result.get('tool_results', {})\n",
    "            if 'brand' in tool_results:\n",
    "                brand_result = tool_results['brand']\n",
    "                if isinstance(brand_result, dict):\n",
    "                    brand_data = brand_result.get('data', {})\n",
    "                    if isinstance(brand_data, dict):\n",
    "                        details = brand_data.get('details', {})\n",
    "                        if isinstance(details, dict):\n",
    "                            detected_brands = details.get('detected_brands', [])\n",
    "        \n",
    "        # brand_detectedの判定\n",
    "        brand_detected = len(detected_brands) > 0\n",
    "        \n",
    "        # 結果を記録\n",
    "        results.append({\n",
    "            'domain': domain,\n",
    "            'ml_probability': ml_prob,\n",
    "            'ai_is_phishing': result.get('ai_is_phishing', False),\n",
    "            'ai_confidence': result.get('ai_confidence', 0.0),\n",
    "            'ai_risk_level': result.get('ai_risk_level', 'unknown'),\n",
    "            'tools_used': result.get('tools_used', []),\n",
    "            'processing_time': elapsed,\n",
    "            'brand_detected': brand_detected,\n",
    "            'detected_brands': detected_brands if detected_brands else '',  # リストか空文字列\n",
    "            'error': None\n",
    "        })\n",
    "        \n",
    "        # プログレス表示（改良版）\n",
    "        phish_symbol = \"✓\" if result.get('ai_is_phishing') else \"✗\"\n",
    "        brand_str = f\"{detected_brands[0]}\" if detected_brands else \"-\"\n",
    "        \n",
    "        print(f\"[{idx+1:3}/{len(random100_df)}] {domain:40} \"\n",
    "              f\"ml={ml_prob:.3f} {phish_symbol} \"\n",
    "              f\"conf={result.get('ai_confidence', 0):.3f} \"\n",
    "              f\"risk={result.get('ai_risk_level', 'unknown'):12} \"\n",
    "              f\"brand={brand_str:15} \"\n",
    "              f\"({elapsed:.1f}s)\")\n",
    "        \n",
    "        # 20件ごとに統計を表示\n",
    "        if (idx + 1) % 20 == 0:\n",
    "            phishing_count = sum(1 for r in results if r['ai_is_phishing'])\n",
    "            brand_count = sum(1 for r in results if r['brand_detected'])\n",
    "            avg_time = sum(r['processing_time'] for r in results) / len(results)\n",
    "            print(f\"  --- Progress: {idx+1}/{len(random100_df)} | \"\n",
    "                  f\"Phishing: {phishing_count} | \"\n",
    "                  f\"Brands: {brand_count} | \"\n",
    "                  f\"Avg time: {avg_time:.2f}s ---\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # エラーが発生した場合\n",
    "        elapsed = time.time() - eval_start\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        results.append({\n",
    "            'domain': domain,\n",
    "            'ml_probability': ml_prob,\n",
    "            'ai_is_phishing': False,\n",
    "            'ai_confidence': 0.0,\n",
    "            'ai_risk_level': 'error',\n",
    "            'tools_used': [],\n",
    "            'processing_time': elapsed,\n",
    "            'brand_detected': False,\n",
    "            'detected_brands': '',\n",
    "            'error': error_msg\n",
    "        })\n",
    "        \n",
    "        print(f\"[{idx+1:3}/{len(random100_df)}] {domain:40} \"\n",
    "              f\"ml={ml_prob:.3f} ✗ ERROR: {error_msg[:30]}... ({elapsed:.1f}s)\")\n",
    "\n",
    "# 最終統計\n",
    "total_time = time.time() - start_time\n",
    "print(\"=\" * 80)\n",
    "print(f\"[INFO] Evaluation complete! Total time: {total_time:.2f}s\")\n",
    "\n",
    "# 結果をDataFrameに変換\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# タイムスタンプ付きでファイル保存\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "result_file = BASE_DIR / f\"random100_eval_{timestamp}.csv\"\n",
    "results_df.to_csv(result_file, index=False)\n",
    "print(f\"[INFO] Results saved to: {result_file}\")\n",
    "\n",
    "# 統計サマリーを計算\n",
    "summary = {\n",
    "    'total_domains': len(results_df),\n",
    "    'phishing_detected': results_df['ai_is_phishing'].sum(),\n",
    "    'phishing_rate': results_df['ai_is_phishing'].mean(),\n",
    "    'brand_detected': results_df['brand_detected'].sum(),\n",
    "    'brand_detection_rate': results_df['brand_detected'].mean(),\n",
    "    'avg_confidence': results_df['ai_confidence'].mean(),\n",
    "    'avg_processing_time': results_df['processing_time'].mean(),\n",
    "    'errors': results_df['error'].notna().sum(),\n",
    "}\n",
    "\n",
    "# リスクレベルの分布\n",
    "risk_dist = results_df['ai_risk_level'].value_counts().to_dict()\n",
    "summary['risk_distribution'] = risk_dist\n",
    "\n",
    "# ブランド検出の詳細\n",
    "brand_domains = results_df[results_df['brand_detected'] == True]\n",
    "if not brand_domains.empty:\n",
    "    # 検出されたブランドのリスト\n",
    "    all_brands = []\n",
    "    for brands_str in brand_domains['detected_brands']:\n",
    "        if brands_str and brands_str != '':\n",
    "            if isinstance(brands_str, list):\n",
    "                all_brands.extend(brands_str)\n",
    "            else:\n",
    "                all_brands.append(str(brands_str))\n",
    "    \n",
    "    from collections import Counter\n",
    "    brand_counts = Counter(all_brands)\n",
    "    summary['detected_brand_counts'] = dict(brand_counts)\n",
    "else:\n",
    "    summary['detected_brand_counts'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Summary saved to: /home/asomura/nextstep/random100_summary_2025-11-18_054804.json\n",
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "[Overall Statistics]\n",
      "  Total domains evaluated: 100\n",
      "  Phishing detected:       49 (49.0%)\n",
      "  Brand impersonation:     17 (17.0%)\n",
      "  Average confidence:      0.371\n",
      "  Average processing time: 6.93s\n",
      "  Errors:                  0\n",
      "\n",
      "[Risk Level Distribution]\n",
      "  critical        0 (  0.0%)\n",
      "  high           27 ( 27.0%)\n",
      "  medium-high    22 ( 22.0%)\n",
      "  medium          8 (  8.0%)\n",
      "  low            43 ( 43.0%)\n",
      "  unknown         0 (  0.0%)\n",
      "  error           0 (  0.0%)\n",
      "\n",
      "[Brand Detection Analysis]\n",
      "  x                      5 detections\n",
      "  x(compound)            4 detections\n",
      "  line(compound)         2 detections\n",
      "  line                   1 detections\n",
      "  ntt(fuzzy)             1 detections\n",
      "  visa                   1 detections\n",
      "  au(substring)          1 detections\n",
      "  mufg                   1 detections\n",
      "  aeoncard               1 detections\n",
      "\n",
      "[High Confidence Phishing (conf >= 0.6)]\n",
      "  Count: 26\n",
      "  - baidu-xiamen.cn                          conf=0.700 brands=['x']\n",
      "  - onlinetolling-nzta.com                   conf=0.700 brands=['line(compound)']\n",
      "  - beckli.online                            conf=0.700 brands=['line']\n",
      "  - dompatr.icu                              conf=0.669 brands=-\n",
      "  - discountdealsmarket.icu                  conf=0.601 brands=-\n",
      "  - usptranuex.com                           conf=0.700 brands=['x(compound)']\n",
      "  - login-service-index.com                  conf=0.700 brands=['x']\n",
      "  - yiwzp.com                                conf=0.615 brands=-\n",
      "  - vveb3-exodus-vvallet.top                 conf=0.700 brands=['x(compound)']\n",
      "  - zsedsu.cn                                conf=0.612 brands=-\n",
      "\n",
      "[Target Brand Detection]\n",
      "  mufg        1/ 1 (100.0%)\n",
      "  visa        1/ 1 (100.0%)\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "Phishing Detection Rate: 49.0%\n",
      "Brand Detection Rate:    17.0%\n",
      "Files saved:\n",
      "  - /home/asomura/nextstep/random100_eval_2025-11-18_054804.csv\n",
      "  - /home/asomura/nextstep/random100_summary_2025-11-18_054804.json\n"
     ]
    }
   ],
   "source": [
    "# NumPy/Pandas型をPython標準型に変換\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"NumPy/Pandas型をJSON化可能な型に変換\"\"\"\n",
    "    if isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, pd.Series):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# summaryを変換してから保存\n",
    "summary_converted = convert_to_json_serializable(summary)\n",
    "\n",
    "# サマリーを保存\n",
    "import json\n",
    "summary_file = BASE_DIR / f\"random100_summary_{timestamp}.json\"\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_converted, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[INFO] Summary saved to: {summary_file}\")\n",
    "\n",
    "# 結果表示\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n[Overall Statistics]\")\n",
    "print(f\"  Total domains evaluated: {summary['total_domains']}\")\n",
    "print(f\"  Phishing detected:       {summary['phishing_detected']} ({summary['phishing_rate']*100:.1f}%)\")\n",
    "print(f\"  Brand impersonation:     {summary['brand_detected']} ({summary['brand_detection_rate']*100:.1f}%)\")\n",
    "print(f\"  Average confidence:      {summary['avg_confidence']:.3f}\")\n",
    "print(f\"  Average processing time: {summary['avg_processing_time']:.2f}s\")\n",
    "print(f\"  Errors:                  {summary['errors']}\")\n",
    "\n",
    "print(f\"\\n[Risk Level Distribution]\")\n",
    "for level in ['critical', 'high', 'medium-high', 'medium', 'low', 'unknown', 'error']:\n",
    "    count = risk_dist.get(level, 0)\n",
    "    pct = (count / summary['total_domains']) * 100\n",
    "    print(f\"  {level:12}  {count:3} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n[Brand Detection Analysis]\")\n",
    "if summary['detected_brand_counts']:\n",
    "    for brand, count in sorted(summary['detected_brand_counts'].items(), \n",
    "                              key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  {brand:20} {count:3} detections\")\n",
    "else:\n",
    "    print(\"  No brands detected\")\n",
    "\n",
    "# 高信頼度のフィッシングサイトを表示\n",
    "print(f\"\\n[High Confidence Phishing (conf >= 0.6)]\")\n",
    "high_conf = results_df[(results_df['ai_is_phishing'] == True) & \n",
    "                       (results_df['ai_confidence'] >= 0.6)]\n",
    "print(f\"  Count: {len(high_conf)}\")\n",
    "if not high_conf.empty:\n",
    "    for _, row in high_conf.head(10).iterrows():\n",
    "        brands_str = row['detected_brands'] if row['detected_brands'] else '-'\n",
    "        print(f\"  - {row['domain']:40} conf={row['ai_confidence']:.3f} brands={brands_str}\")\n",
    "\n",
    "# 重要なブランドの検出状況\n",
    "print(f\"\\n[Target Brand Detection]\")\n",
    "target_brands = ['mufg', 'smbc', 'amazon', 'metamask', 'paypal', 'visa', 'mastercard']\n",
    "for brand in target_brands:\n",
    "    # ドメイン名にブランドが含まれるものを探す\n",
    "    brand_in_domain = results_df[results_df['domain'].str.contains(brand, case=False, na=False)]\n",
    "    # 実際に検出されたもの\n",
    "    brand_detected = 0\n",
    "    for _, row in brand_in_domain.iterrows():\n",
    "        if row['brand_detected'] and row['detected_brands']:\n",
    "            brands_list = row['detected_brands'] if isinstance(row['detected_brands'], list) else [row['detected_brands']]\n",
    "            if any(brand in str(b).lower() for b in brands_list):\n",
    "                brand_detected += 1\n",
    "    \n",
    "    if len(brand_in_domain) > 0:\n",
    "        print(f\"  {brand:10} {brand_detected:2}/{len(brand_in_domain):2} ({brand_detected/len(brand_in_domain)*100:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Phishing Detection Rate: {summary['phishing_rate']*100:.1f}%\")\n",
    "print(f\"Brand Detection Rate:    {summary['brand_detection_rate']*100:.1f}%\")\n",
    "print(f\"Files saved:\")\n",
    "print(f\"  - {result_file}\")\n",
    "print(f\"  - {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
