{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05_pipeline_analysis.ipynb\n",
    "\n",
    "## 概要\n",
    "02_main.py の出力を分析するノートブック。\n",
    "Stage1 (XGBoost) → Stage2 (LR Gate) → Stage3 (AI Agent) の各段階の性能を評価する。\n",
    "\n",
    "## 分析内容\n",
    "1. **Stage1 分析**: XGBoostモデルの性能、閾値分析\n",
    "2. **Stage2 分析**: LR Gateの性能、Handoff選択分析\n",
    "3. **Stage3 分析**: AI Agentのゲート発火分析（POST_LLM_FLIP_GATE, P1-P3, B1-B4）\n",
    "4. **FP/FN パターン分析**: 各段階での誤分類パターン\n",
    "5. **特徴量分析**: 識別力のある特徴量の特定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 0: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    precision_recall_curve, roc_curve, auc,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Japanese font support\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'sans-serif']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN_ID configuration\n",
    "# Change this to analyze different pipeline runs\n",
    "RUN_ID = os.environ.get('RUN_ID', '2026-01-17_132657')\n",
    "\n",
    "ARTIFACTS_DIR = Path(f'artifacts/{RUN_ID}')\n",
    "RESULTS_DIR = ARTIFACTS_DIR / 'results'\n",
    "HANDOFF_DIR = ARTIFACTS_DIR / 'handoff'\n",
    "MODELS_DIR = ARTIFACTS_DIR / 'models'\n",
    "\n",
    "print(f\"RUN_ID: {RUN_ID}\")\n",
    "print(f\"ARTIFACTS_DIR: {ARTIFACTS_DIR}\")\n",
    "print(f\"Directory exists: {ARTIFACTS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stage1 decisions\n",
    "stage1_df = pd.read_csv(RESULTS_DIR / 'stage1_decisions_latest.csv')\n",
    "print(f\"Stage1 decisions: {len(stage1_df)} rows\")\n",
    "print(f\"Columns: {list(stage1_df.columns)}\")\n",
    "\n",
    "# Load Stage2 decisions\n",
    "stage2_df = pd.read_csv(RESULTS_DIR / 'stage2_decisions_latest.csv')\n",
    "print(f\"\\nStage2 decisions: {len(stage2_df)} rows\")\n",
    "print(f\"Columns: {list(stage2_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation data (if available)\n",
    "eval_files = list((RESULTS_DIR / 'stage2_validation').glob('eval_df__*.csv'))\n",
    "if eval_files:\n",
    "    eval_df = pd.read_csv(eval_files[0])\n",
    "    print(f\"Evaluation data: {len(eval_df)} rows\")\n",
    "    print(f\"Columns: {list(eval_df.columns)}\")\n",
    "else:\n",
    "    eval_df = None\n",
    "    print(\"No evaluation data found\")\n",
    "\n",
    "# Load all_test_merged (full test set with agent predictions)\n",
    "merged_files = list((RESULTS_DIR / 'stage2_validation').glob('all_test_merged__*.csv'))\n",
    "if merged_files:\n",
    "    all_test_df = pd.read_csv(merged_files[0])\n",
    "    print(f\"\\nAll test merged: {len(all_test_df)} rows\")\n",
    "    print(f\"Columns: {list(all_test_df.columns)}\")\n",
    "else:\n",
    "    all_test_df = None\n",
    "    print(\"No all_test_merged data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load handoff data with features\n",
    "handoff_pkl = HANDOFF_DIR / '04-1_config_and_data_preparation.pkl'\n",
    "if handoff_pkl.exists():\n",
    "    with open(handoff_pkl, 'rb') as f:\n",
    "        handoff_data = pickle.load(f)\n",
    "    \n",
    "    fn_features_df = handoff_data.get('fn_features_df')\n",
    "    cert_full_info_map = handoff_data.get('cert_full_info_map', {})\n",
    "    brand_keywords = handoff_data.get('brand_keywords', [])\n",
    "    DANGEROUS_TLDS = set(handoff_data.get('DANGEROUS_TLDS', []))\n",
    "    LEGITIMATE_TLDS = set(handoff_data.get('LEGITIMATE_TLDS', []))\n",
    "    \n",
    "    print(f\"Handoff features: {len(fn_features_df)} rows\")\n",
    "    print(f\"Cert info map: {len(cert_full_info_map)} entries\")\n",
    "    print(f\"Brand keywords: {len(brand_keywords)} keywords\")\n",
    "    print(f\"Dangerous TLDs: {len(DANGEROUS_TLDS)}\")\n",
    "else:\n",
    "    print(\"Handoff data not found\")\n",
    "    fn_features_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load route1 thresholds\n",
    "with open(RESULTS_DIR / 'route1_thresholds.json', 'r') as f:\n",
    "    route1_thresholds = json.load(f)\n",
    "\n",
    "print(\"Route1 Thresholds:\")\n",
    "print(f\"  t_low: {route1_thresholds['t_low']}\")\n",
    "print(f\"  t_high: {route1_thresholds['t_high']}\")\n",
    "print(f\"  coverage: {route1_thresholds['coverage']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Stage1 Analysis (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage1 decision distribution\n",
    "print(\"=\" * 60)\n",
    "print(\"Stage1 Decision Distribution\")\n",
    "print(\"=\" * 60)\n",
    "print(stage1_df['stage1_decision'].value_counts())\n",
    "\n",
    "# Create stage1_pred from decision\n",
    "stage1_df['stage1_pred'] = stage1_df['stage1_decision'].map({\n",
    "    'phishing': 1,\n",
    "    'benign': 0,\n",
    "    'handoff_to_agent': -1  # defer\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage1 performance (excluding handoff)\n",
    "stage1_decided = stage1_df[stage1_df['stage1_pred'] >= 0].copy()\n",
    "\n",
    "if len(stage1_decided) > 0:\n",
    "    y_true_s1 = stage1_decided['y_true']\n",
    "    y_pred_s1 = stage1_decided['stage1_pred']\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Stage1 Performance (Auto-decided samples only)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total auto-decided: {len(stage1_decided)}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true_s1, y_pred_s1, target_names=['Benign', 'Phishing']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true_s1, y_pred_s1)\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Benign', 'Phishing'],\n",
    "                yticklabels=['Benign', 'Phishing'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title('Stage1 Confusion Matrix (Auto-decided)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML probability distribution by class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution\n",
    "ax = axes[0]\n",
    "for label, name in [(0, 'Benign'), (1, 'Phishing')]:\n",
    "    subset = stage1_df[stage1_df['y_true'] == label]['ml_probability']\n",
    "    ax.hist(subset, bins=50, alpha=0.6, label=f'{name} (n={len(subset)})')\n",
    "ax.axvline(route1_thresholds['t_low'], color='green', linestyle='--', label=f\"t_low={route1_thresholds['t_low']:.4f}\")\n",
    "ax.axvline(route1_thresholds['t_high'], color='red', linestyle='--', label=f\"t_high={route1_thresholds['t_high']:.4f}\")\n",
    "ax.set_xlabel('ML Probability')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('ML Probability Distribution by Class')\n",
    "ax.legend()\n",
    "\n",
    "# Log scale\n",
    "ax = axes[1]\n",
    "for label, name in [(0, 'Benign'), (1, 'Phishing')]:\n",
    "    subset = stage1_df[stage1_df['y_true'] == label]['ml_probability']\n",
    "    ax.hist(subset, bins=50, alpha=0.6, label=f'{name}')\n",
    "ax.axvline(route1_thresholds['t_low'], color='green', linestyle='--', label=f\"t_low\")\n",
    "ax.axvline(route1_thresholds['t_high'], color='red', linestyle='--', label=f\"t_high\")\n",
    "ax.set_xlabel('ML Probability')\n",
    "ax.set_ylabel('Count (log scale)')\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('ML Probability Distribution (Log Scale)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sweep analysis\n",
    "thresholds = np.arange(0.0, 1.01, 0.05)\n",
    "results = []\n",
    "\n",
    "for th in thresholds:\n",
    "    y_pred = (stage1_df['ml_probability'] >= th).astype(int)\n",
    "    y_true = stage1_df['y_true']\n",
    "    \n",
    "    tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "    fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': th,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn\n",
    "    })\n",
    "\n",
    "sweep_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(sweep_df['threshold'], sweep_df['precision'], label='Precision', marker='o')\n",
    "ax.plot(sweep_df['threshold'], sweep_df['recall'], label='Recall', marker='s')\n",
    "ax.plot(sweep_df['threshold'], sweep_df['f1'], label='F1', marker='^')\n",
    "ax.axvline(0.5, color='gray', linestyle='--', alpha=0.5, label='Default (0.5)')\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Threshold Sweep Analysis')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best F1\n",
    "best_idx = sweep_df['f1'].idxmax()\n",
    "print(f\"\\nBest F1: {sweep_df.loc[best_idx, 'f1']:.4f} at threshold {sweep_df.loc[best_idx, 'threshold']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Stage2 Analysis (LR Gate / Handoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage2 handoff analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"Stage2 Handoff Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Handoff region = samples that Stage1 didn't auto-decide\n",
    "handoff_region = stage1_df[stage1_df['stage1_decision'] == 'handoff_to_agent']\n",
    "print(f\"\\nHandoff region size: {len(handoff_region)}\")\n",
    "print(f\"  Phishing (y_true=1): {(handoff_region['y_true']==1).sum()}\")\n",
    "print(f\"  Benign (y_true=0): {(handoff_region['y_true']==0).sum()}\")\n",
    "\n",
    "# ML probability in handoff region\n",
    "print(f\"\\nML probability in handoff region:\")\n",
    "print(handoff_region['ml_probability'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handoff region ML distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for label, name, color in [(0, 'Benign (FP Risk)', 'blue'), (1, 'Phishing (FN)', 'red')]:\n",
    "    subset = handoff_region[handoff_region['y_true'] == label]['ml_probability']\n",
    "    ax.hist(subset, bins=50, alpha=0.6, label=f'{name} (n={len(subset)})', color=color)\n",
    "\n",
    "ax.set_xlabel('ML Probability')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('ML Probability Distribution in Handoff Region')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Certificate features in handoff region (if available)\n",
    "if fn_features_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Certificate Features in Handoff Region\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Compare FN vs FP\n",
    "    fn_group = fn_features_df[fn_features_df['y_true'] == 1]\n",
    "    fp_group = fn_features_df[fn_features_df['y_true'] == 0]\n",
    "    \n",
    "    print(f\"\\nFN (Phishing in handoff): {len(fn_group)}\")\n",
    "    print(f\"FP Risk (Benign in handoff): {len(fp_group)}\")\n",
    "    \n",
    "    # Feature comparison\n",
    "    features_to_compare = ['ml_probability', 'cert_validity_days', 'cert_age_days', \n",
    "                           'san_count', 'is_free_ca', 'has_organization']\n",
    "    \n",
    "    comparison = []\n",
    "    for feat in features_to_compare:\n",
    "        if feat in fn_features_df.columns:\n",
    "            fn_mean = fn_group[feat].mean()\n",
    "            fp_mean = fp_group[feat].mean()\n",
    "            comparison.append({\n",
    "                'Feature': feat,\n",
    "                'FN Mean': fn_mean,\n",
    "                'FP Mean': fp_mean,\n",
    "                'Diff': fn_mean - fp_mean\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    print(\"\\nFeature Comparison (FN vs FP in Handoff):\")\n",
    "    print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Stage3 Analysis (AI Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage3 AI Agent performance\n",
    "if eval_df is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Stage3 AI Agent Performance\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nEvaluation samples: {len(eval_df)}\")\n",
    "    print(f\"  Phishing: {(eval_df['y_true']==1).sum()}\")\n",
    "    print(f\"  Benign: {(eval_df['y_true']==0).sum()}\")\n",
    "    \n",
    "    # Agent predictions\n",
    "    y_true = eval_df['y_true']\n",
    "    y_agent = eval_df['agent_pred'].astype(int)\n",
    "    \n",
    "    print(\"\\nAI Agent Classification Report:\")\n",
    "    print(classification_report(y_true, y_agent, target_names=['Benign', 'Phishing']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_agent)\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', ax=ax,\n",
    "                xticklabels=['Benign', 'Phishing'],\n",
    "                yticklabels=['Benign', 'Phishing'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_title('AI Agent Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage1 vs AI Agent comparison\n",
    "if eval_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Stage1 vs AI Agent Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    y_true = eval_df['y_true']\n",
    "    y_stage1 = eval_df['stage1_pred'].astype(int)\n",
    "    y_agent = eval_df['agent_pred'].astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = []\n",
    "    for name, y_pred in [('Stage1', y_stage1), ('AI Agent', y_agent)]:\n",
    "        tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "        fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "        tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "        fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics.append({\n",
    "            'Model': name,\n",
    "            'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn,\n",
    "            'Precision': f'{precision:.3f}',\n",
    "            'Recall': f'{recall:.3f}',\n",
    "            'F1': f'{f1:.3f}'\n",
    "        })\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: FP/FN Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TLD classification\n",
    "HIGH_DANGER_TLDS = frozenset([\n",
    "    'tk', 'ml', 'ga', 'cf', 'gq',  # Free TLDs\n",
    "    'icu', 'cfd', 'sbs', 'rest', 'cyou',  # Phishing-specific\n",
    "    'pw', 'buzz', 'lat',  # High phishing rate\n",
    "])\n",
    "\n",
    "MEDIUM_DANGER_TLDS = frozenset([\n",
    "    'top', 'shop', 'xyz', 'cc', 'online', 'site', 'website',\n",
    "    'club', 'vip', 'asia', 'one', 'link', 'click', 'live',\n",
    "    'cn', 'tokyo', 'dev', 'me', 'pe', 'ar', 'cl', 'mw', 'ci',\n",
    "])\n",
    "\n",
    "def extract_tld(domain):\n",
    "    \"\"\"Extract TLD from domain\"\"\"\n",
    "    import re\n",
    "    match = re.search(r'\\.([^.]+)$', str(domain))\n",
    "    return match.group(1) if match else ''\n",
    "\n",
    "def classify_tld(tld):\n",
    "    \"\"\"Classify TLD danger level\"\"\"\n",
    "    tld = tld.lower()\n",
    "    if tld in HIGH_DANGER_TLDS:\n",
    "        return 'high_danger'\n",
    "    elif tld in MEDIUM_DANGER_TLDS:\n",
    "        return 'medium_danger'\n",
    "    else:\n",
    "        return 'non_danger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP Analysis\n",
    "if eval_df is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"False Positive (FP) Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fp_cases = eval_df[(eval_df['agent_pred'] == True) & (eval_df['y_true'] == 0)].copy()\n",
    "    print(f\"\\nTotal FP: {len(fp_cases)}\")\n",
    "    \n",
    "    # TLD analysis\n",
    "    fp_cases['tld'] = fp_cases['domain'].apply(extract_tld)\n",
    "    fp_cases['tld_class'] = fp_cases['tld'].apply(classify_tld)\n",
    "    \n",
    "    print(\"\\nFP by TLD class:\")\n",
    "    print(fp_cases['tld_class'].value_counts())\n",
    "    \n",
    "    # ML distribution\n",
    "    print(\"\\nFP ML distribution:\")\n",
    "    ml_bins = [0, 0.15, 0.20, 0.30, 0.50, 1.0]\n",
    "    fp_cases['ml_bin'] = pd.cut(fp_cases['ml_probability'], bins=ml_bins)\n",
    "    print(fp_cases['ml_bin'].value_counts().sort_index())\n",
    "    \n",
    "    # Certificate analysis\n",
    "    print(\"\\nFP Certificate analysis:\")\n",
    "    short_cert = fp_cases['cert_validity_days'] <= 90\n",
    "    print(f\"  Short cert (<=90 days): {short_cert.sum()} ({short_cert.mean()*100:.1f}%)\")\n",
    "    print(f\"  Long cert (>90 days): {(~short_cert).sum()} ({(~short_cert).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP samples that should have been blocked by POST_LLM_FLIP_GATE\n",
    "if eval_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"POST_LLM_FLIP_GATE Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Non-danger TLD + ML < 0.30 should be blocked\n",
    "    should_be_blocked = (\n",
    "        (fp_cases['tld_class'] == 'non_danger') & \n",
    "        (fp_cases['ml_probability'] < 0.30)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFP that should have been blocked (non-danger TLD + ML<0.30): {should_be_blocked.sum()}\")\n",
    "    print(f\"  → These were likely overridden by P1 gate (low_signal_phishing_gate)\")\n",
    "    \n",
    "    # With short certificate\n",
    "    blocked_with_short_cert = should_be_blocked & short_cert\n",
    "    print(f\"\\n  + Short cert (<=90 days): {blocked_with_short_cert.sum()}\")\n",
    "    print(f\"  → P1 gate condition: brand_detected + short_cert + ML<0.30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FN Analysis\n",
    "if eval_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"False Negative (FN) Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fn_cases = eval_df[(eval_df['agent_pred'] == False) & (eval_df['y_true'] == 1)].copy()\n",
    "    print(f\"\\nTotal FN: {len(fn_cases)}\")\n",
    "    \n",
    "    # TLD analysis\n",
    "    fn_cases['tld'] = fn_cases['domain'].apply(extract_tld)\n",
    "    fn_cases['tld_class'] = fn_cases['tld'].apply(classify_tld)\n",
    "    \n",
    "    print(\"\\nFN by TLD class:\")\n",
    "    print(fn_cases['tld_class'].value_counts())\n",
    "    \n",
    "    # ML distribution\n",
    "    print(\"\\nFN ML distribution:\")\n",
    "    fn_cases['ml_bin'] = pd.cut(fn_cases['ml_probability'], bins=ml_bins)\n",
    "    print(fn_cases['ml_bin'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP/FN sample display\n",
    "if eval_df is not None:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Sample FP Cases (non-danger TLD, ML < 0.30)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fp_low_ml = fp_cases[\n",
    "        (fp_cases['tld_class'] == 'non_danger') & \n",
    "        (fp_cases['ml_probability'] < 0.30)\n",
    "    ][['domain', 'ml_probability', 'cert_validity_days', 'cert_san_count']].head(15)\n",
    "    \n",
    "    print(fp_low_ml.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Gate Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate different gate configurations\nif eval_df is not None:\n    print(\"=\" * 60)\n    print(\"Gate Configuration Simulation\")\n    print(\"=\" * 60)\n    \n    # Add TLD info\n    sim_df = eval_df.copy()\n    sim_df['tld'] = sim_df['domain'].apply(extract_tld)\n    sim_df['tld_class'] = sim_df['tld'].apply(classify_tld)\n    sim_df['short_cert'] = sim_df['cert_validity_days'] <= 90\n    \n    y_true = sim_df['y_true']\n    \n    def calc_metrics(y_pred):\n        tp = ((y_pred == 1) & (y_true == 1)).sum()\n        fp = ((y_pred == 1) & (y_true == 0)).sum()\n        tn = ((y_pred == 0) & (y_true == 0)).sum()\n        fn = ((y_pred == 0) & (y_true == 1)).sum()\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n        return {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn, 'Precision': precision, 'Recall': recall, 'F1': f1}\n    \n    # Current (baseline)\n    current = calc_metrics(sim_df['agent_pred'].astype(int))\n    current['Config'] = 'Current (Agent)'\n    \n    # Stage1 only\n    stage1_only = calc_metrics(sim_df['stage1_pred'].astype(int))\n    stage1_only['Config'] = 'Stage1 Only'\n    \n    # Scenario: Disable P1 gate for non-danger TLD\n    # = Block LLM PHISH when: non-danger TLD + ML < 0.30\n    y_sim1 = sim_df['agent_pred'].astype(int).copy()\n    block_mask = (\n        (sim_df['agent_pred'] == True) & \n        (sim_df['tld_class'] == 'non_danger') & \n        (sim_df['ml_probability'] < 0.30)\n    )\n    y_sim1[block_mask] = 0\n    scenario1 = calc_metrics(y_sim1)\n    scenario1['Config'] = 'P1 disabled (non-danger TLD)'\n    \n    # Scenario: More aggressive gate (ML < 0.35)\n    y_sim2 = sim_df['agent_pred'].astype(int).copy()\n    block_mask2 = (\n        (sim_df['agent_pred'] == True) & \n        (sim_df['tld_class'] == 'non_danger') & \n        (sim_df['ml_probability'] < 0.35)\n    )\n    y_sim2[block_mask2] = 0\n    scenario2 = calc_metrics(y_sim2)\n    scenario2['Config'] = 'Gate ML < 0.35 (non-danger)'\n    \n    # Summary\n    results = pd.DataFrame([stage1_only, current, scenario1, scenario2])\n    results = results[['Config', 'TP', 'FP', 'TN', 'FN', 'Precision', 'Recall', 'F1']]\n    results['Precision'] = results['Precision'].apply(lambda x: f'{x:.3f}')\n    results['Recall'] = results['Recall'].apply(lambda x: f'{x:.3f}')\n    results['F1'] = results['F1'].apply(lambda x: f'{x:.3f}')\n    \n    print(\"\\nGate Configuration Comparison:\")\n    print(results.to_string(index=False))"
  },
  {
   "cell_type": "code",
   "source": "# Sample Extra TP domains that would be LOST by Gate B (non_danger + ML < 0.35)\nif eval_df is not None and len(extra_tp) > 0:\n    print(\"=\" * 80)\n    print(\"Extra TP at Risk: Phishing that would be MISSED by Gate B (non_danger + ML < 0.35)\")\n    print(\"=\" * 80)\n    \n    gate_b_mask = (extra_tp['tld_class'] == 'non_danger') & (extra_tp['ml_probability'] < 0.35)\n    at_risk = extra_tp[gate_b_mask]\n    \n    print(f\"\\nTotal Extra TP: {len(extra_tp)}\")\n    print(f\"At risk with Gate B: {len(at_risk)}\")\n    \n    if len(at_risk) > 0:\n        print(f\"\\nDomains that would be MISSED (shown as BENIGN when actually PHISHING):\")\n        cols = ['domain', 'ml_probability', 'tld_class', 'cert_validity_days', 'cert_san_count']\n        for _, row in at_risk[cols].iterrows():\n            print(f\"  {row['domain']:<45} ML={row['ml_probability']:.3f}  cert={row['cert_validity_days']:.0f}d  SAN={row['cert_san_count']:.0f}\")\n        \n        # Check if these are brand impersonation\n        print(f\"\\n  → Review these domains: are they clear brand impersonation?\")\n        print(f\"  → If yes, consider improving brand detection instead of loosening gate\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Sample Extra FP domains (to understand why Agent is wrong)\nif eval_df is not None and len(extra_fp) > 0:\n    print(\"=\" * 80)\n    print(\"Sample Extra FP Domains (Agent wrong, Stage1 correct)\")\n    print(\"=\" * 80)\n    \n    cols_to_show = ['domain', 'ml_probability', 'tld_class', 'cert_validity_days', 'cert_san_count']\n    \n    # By TLD class\n    for tld_class in ['non_danger', 'medium_danger', 'high_danger']:\n        subset = extra_fp[extra_fp['tld_class'] == tld_class]\n        if len(subset) > 0:\n            print(f\"\\n--- {tld_class.upper()} TLD ({len(subset)} cases) ---\")\n            sample = subset.nsmallest(10, 'ml_probability')[cols_to_show]\n            for _, row in sample.iterrows():\n                print(f\"  {row['domain']:<40} ML={row['ml_probability']:.3f}  cert={row['cert_validity_days']:.0f}d  SAN={row['cert_san_count']:.0f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Gate Candidate Impact Analysis: FP saved vs TP lost\nif eval_df is not None and len(extra_fp) > 0:\n    print(\"=\" * 80)\n    print(\"Gate Candidate Impact Analysis: FP saved vs TP lost\")\n    print(\"=\" * 80)\n    \n    # Extra TP: Stage1 said BENIGN, Agent said PHISH (correctly)\n    extra_tp = analysis_df[\n        (analysis_df['stage1_pred'] == 0) &  # Stage1 = BENIGN\n        (analysis_df['agent_pred'] == True) &  # Agent = PHISH\n        (analysis_df['y_true'] == 1)  # Actually PHISHING\n    ].copy()\n    extra_tp['tld'] = extra_tp['domain'].apply(extract_tld)\n    extra_tp['tld_class'] = extra_tp['tld'].apply(classify_tld)\n    \n    print(f\"\\nExtra TP (Agent correctly identifies phishing that Stage1 missed): {len(extra_tp)}\")\n    \n    # Analyze each gate candidate\n    gate_candidates = [\n        ('Gate A: non_danger + ML < 0.30', \n         lambda df: (df['tld_class'] == 'non_danger') & (df['ml_probability'] < 0.30)),\n        ('Gate B: non_danger + ML < 0.35', \n         lambda df: (df['tld_class'] == 'non_danger') & (df['ml_probability'] < 0.35)),\n        ('Gate C: non_danger + ML < 0.40', \n         lambda df: (df['tld_class'] == 'non_danger') & (df['ml_probability'] < 0.40)),\n        ('Gate D: (non_danger + ML < 0.35) OR (SAN > 10)', \n         lambda df: ((df['tld_class'] == 'non_danger') & (df['ml_probability'] < 0.35)) | (df['cert_san_count'] > 10)),\n        ('Gate E: non_danger + ML < 0.35 + short_cert', \n         lambda df: (df['tld_class'] == 'non_danger') & (df['ml_probability'] < 0.35) & (df['cert_validity_days'] <= 90)),\n    ]\n    \n    print(\"\\n\" + \"-\" * 80)\n    print(f\"{'Gate':<45} {'FP Saved':>10} {'TP Lost':>10} {'Net':>10} {'F1 Impact':>12}\")\n    print(\"-\" * 80)\n    \n    # Current metrics\n    y_true = analysis_df['y_true']\n    y_agent = analysis_df['agent_pred'].astype(int)\n    \n    base_tp = ((y_agent == 1) & (y_true == 1)).sum()\n    base_fp = ((y_agent == 1) & (y_true == 0)).sum()\n    base_fn = ((y_agent == 0) & (y_true == 1)).sum()\n    base_prec = base_tp / (base_tp + base_fp)\n    base_rec = base_tp / (base_tp + base_fn)\n    base_f1 = 2 * base_prec * base_rec / (base_prec + base_rec)\n    \n    for name, condition_fn in gate_candidates:\n        fp_blocked = condition_fn(extra_fp).sum()\n        tp_blocked = condition_fn(extra_tp).sum()\n        \n        # Calculate new F1\n        new_tp = base_tp - tp_blocked\n        new_fp = base_fp - fp_blocked\n        new_fn = base_fn + tp_blocked\n        new_prec = new_tp / (new_tp + new_fp) if (new_tp + new_fp) > 0 else 0\n        new_rec = new_tp / (new_tp + new_fn) if (new_tp + new_fn) > 0 else 0\n        new_f1 = 2 * new_prec * new_rec / (new_prec + new_rec) if (new_prec + new_rec) > 0 else 0\n        \n        f1_change = new_f1 - base_f1\n        net = fp_blocked - tp_blocked\n        \n        print(f\"{name:<45} {fp_blocked:>10} {tp_blocked:>10} {net:>+10} {f1_change:>+11.4f}\")\n    \n    print(\"-\" * 80)\n    print(f\"Current: TP={base_tp}, FP={base_fp}, FN={base_fn}, F1={base_f1:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extra FP: Pattern Discovery\nif eval_df is not None and len(extra_fp) > 0:\n    print(\"=\" * 80)\n    print(\"Extra FP: Pattern Discovery\")\n    print(\"=\" * 80)\n    \n    # Define patterns to check\n    patterns = []\n    \n    # Pattern 1: Non-danger TLD + Low ML + Short Cert\n    p1 = (extra_fp['tld_class'] == 'non_danger') & \\\n         (extra_fp['ml_probability'] < 0.30) & \\\n         (extra_fp['cert_validity_days'] <= 90)\n    patterns.append(('P1: non_danger + ML<0.30 + short_cert', p1))\n    \n    # Pattern 2: Non-danger TLD + Very Low ML\n    p2 = (extra_fp['tld_class'] == 'non_danger') & \\\n         (extra_fp['ml_probability'] < 0.20)\n    patterns.append(('P2: non_danger + ML<0.20', p2))\n    \n    # Pattern 3: High SAN count (legitimate indicator)\n    p3 = extra_fp['cert_san_count'] > 10\n    patterns.append(('P3: SAN count > 10', p3))\n    \n    # Pattern 4: Medium danger TLD + Low ML\n    p4 = (extra_fp['tld_class'] == 'medium_danger') & \\\n         (extra_fp['ml_probability'] < 0.35)\n    patterns.append(('P4: medium_danger + ML<0.35', p4))\n    \n    # Pattern 5: Non-danger + ML < 0.35 (broader)\n    p5 = (extra_fp['tld_class'] == 'non_danger') & \\\n         (extra_fp['ml_probability'] < 0.35)\n    patterns.append(('P5: non_danger + ML<0.35', p5))\n    \n    # Pattern 6: Long certificate (>365 days)\n    p6 = extra_fp['cert_validity_days'] > 365\n    patterns.append(('P6: long_cert (>365 days)', p6))\n    \n    print(\"\\nPattern Analysis:\")\n    print(\"-\" * 60)\n    print(f\"{'Pattern':<45} {'Count':>8} {'%':>8}\")\n    print(\"-\" * 60)\n    \n    for name, mask in patterns:\n        count = mask.sum()\n        pct = count / len(extra_fp) * 100 if len(extra_fp) > 0 else 0\n        print(f\"{name:<45} {count:>8} {pct:>7.1f}%\")\n    \n    print(\"-\" * 60)\n    print(f\"{'Total Extra FP':<45} {len(extra_fp):>8}\")\n    \n    # Combined pattern analysis\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Combined Pattern: Potential Gate Candidates\")\n    print(\"=\" * 60)\n    \n    # Candidate 1: non_danger + ML < 0.35\n    c1 = (extra_fp['tld_class'] == 'non_danger') & (extra_fp['ml_probability'] < 0.35)\n    print(f\"\\n[Candidate 1] non_danger TLD + ML < 0.35\")\n    print(f\"  Would block: {c1.sum()} Extra FP ({c1.sum()/len(extra_fp)*100:.1f}%)\")\n    \n    # Candidate 2: non_danger + ML < 0.40\n    c2 = (extra_fp['tld_class'] == 'non_danger') & (extra_fp['ml_probability'] < 0.40)\n    print(f\"\\n[Candidate 2] non_danger TLD + ML < 0.40\")\n    print(f\"  Would block: {c2.sum()} Extra FP ({c2.sum()/len(extra_fp)*100:.1f}%)\")\n    \n    # Candidate 3: high SAN (>10) + non_danger\n    c3 = (extra_fp['cert_san_count'] > 10) & (extra_fp['tld_class'] == 'non_danger')\n    print(f\"\\n[Candidate 3] SAN > 10 + non_danger TLD\")\n    print(f\"  Would block: {c3.sum()} Extra FP ({c3.sum()/len(extra_fp)*100:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extra FP: ML probability and certificate analysis\nif eval_df is not None and len(extra_fp) > 0:\n    print(\"=\" * 60)\n    print(\"Extra FP: ML Probability Distribution\")\n    print(\"=\" * 60)\n    \n    # ML distribution\n    ml_bins = [0, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.50, 1.0]\n    extra_fp['ml_bin'] = pd.cut(extra_fp['ml_probability'], bins=ml_bins)\n    \n    print(\"\\nML probability distribution:\")\n    ml_dist = extra_fp['ml_bin'].value_counts().sort_index()\n    for bin_range, count in ml_dist.items():\n        pct = count / len(extra_fp) * 100\n        bar = '█' * int(pct / 2)\n        print(f\"  {bin_range}: {count:3d} ({pct:5.1f}%) {bar}\")\n    \n    print(f\"\\nML statistics:\")\n    print(f\"  Mean: {extra_fp['ml_probability'].mean():.4f}\")\n    print(f\"  Median: {extra_fp['ml_probability'].median():.4f}\")\n    print(f\"  Std: {extra_fp['ml_probability'].std():.4f}\")\n    print(f\"  Min: {extra_fp['ml_probability'].min():.4f}\")\n    print(f\"  Max: {extra_fp['ml_probability'].max():.4f}\")\n    \n    # Certificate analysis\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Extra FP: Certificate Analysis\")\n    print(\"=\" * 60)\n    \n    short_cert = extra_fp['cert_validity_days'] <= 90\n    has_cert = extra_fp['cert_validity_days'] > 0\n    \n    print(f\"\\nCertificate validity:\")\n    print(f\"  Has certificate: {has_cert.sum()} ({has_cert.mean()*100:.1f}%)\")\n    print(f\"  Short cert (<=90 days): {short_cert.sum()} ({short_cert.mean()*100:.1f}%)\")\n    print(f\"  Long cert (>90 days): {(~short_cert & has_cert).sum()}\")\n    \n    # SAN count\n    print(f\"\\nSAN count statistics:\")\n    print(f\"  Mean: {extra_fp['cert_san_count'].mean():.1f}\")\n    print(f\"  Median: {extra_fp['cert_san_count'].median():.1f}\")\n    print(f\"  Low SAN (<=3): {(extra_fp['cert_san_count'] <= 3).sum()}\")\n    print(f\"  High SAN (>10): {(extra_fp['cert_san_count'] > 10).sum()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extra FP Analysis: Stage1=BENIGN, Agent=PHISH, y_true=0\nif eval_df is not None:\n    print(\"=\" * 80)\n    print(\"EXTRA FP ANALYSIS: Cases where AI Agent incorrectly overrides Stage1\")\n    print(\"=\" * 80)\n    \n    # Create analysis dataframe\n    analysis_df = eval_df.copy()\n    analysis_df['tld'] = analysis_df['domain'].apply(extract_tld)\n    analysis_df['tld_class'] = analysis_df['tld'].apply(classify_tld)\n    \n    # Extra FP: Stage1 said BENIGN (correct), Agent said PHISH (wrong)\n    extra_fp = analysis_df[\n        (analysis_df['stage1_pred'] == 0) &  # Stage1 = BENIGN\n        (analysis_df['agent_pred'] == True) &  # Agent = PHISH\n        (analysis_df['y_true'] == 0)  # Actually BENIGN\n    ].copy()\n    \n    # Stage1 FP (for comparison)\n    stage1_fp = analysis_df[\n        (analysis_df['stage1_pred'] == 1) &\n        (analysis_df['y_true'] == 0)\n    ]\n    \n    print(f\"\\nStage1 FP count: {len(stage1_fp)}\")\n    print(f\"AI Agent total FP: {len(fp_cases)}\")\n    print(f\"Extra FP (Agent adds): {len(extra_fp)}\")\n    print(f\"  → AI Agent is adding {len(extra_fp)} incorrect PHISH predictions\")\n    \n    # TLD distribution of Extra FP\n    print(\"\\n\" + \"-\" * 60)\n    print(\"Extra FP by TLD Class:\")\n    print(\"-\" * 60)\n    print(extra_fp['tld_class'].value_counts())\n    print(f\"\\nPercentage breakdown:\")\n    for tld_class in ['non_danger', 'medium_danger', 'high_danger']:\n        count = (extra_fp['tld_class'] == tld_class).sum()\n        pct = count / len(extra_fp) * 100 if len(extra_fp) > 0 else 0\n        print(f\"  {tld_class}: {count} ({pct:.1f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Cell 6b: AI Agent Extra FP Analysis (Deep Dive)\n\n**目的**: Stage1がBENIGNと判定したのに、AI AgentがPHISHと誤判定したケース（Extra FP）を深掘り分析\n\n**問題**: AI Agent は Stage1 より 95件も多くFPを出している。この原因を特定し、改善策を検討する。",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost analysis (FN = 3.0, FP = 1.0)\n",
    "if eval_df is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Cost Analysis (FN=3.0, FP=1.0)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    FN_COST = 3.0\n",
    "    FP_COST = 1.0\n",
    "    \n",
    "    def calc_cost(tp, fp, tn, fn):\n",
    "        return fn * FN_COST + fp * FP_COST\n",
    "    \n",
    "    # Current\n",
    "    y_true = eval_df['y_true']\n",
    "    y_agent = eval_df['agent_pred'].astype(int)\n",
    "    y_stage1 = eval_df['stage1_pred'].astype(int)\n",
    "    \n",
    "    agent_tp = ((y_agent == 1) & (y_true == 1)).sum()\n",
    "    agent_fp = ((y_agent == 1) & (y_true == 0)).sum()\n",
    "    agent_fn = ((y_agent == 0) & (y_true == 1)).sum()\n",
    "    agent_cost = calc_cost(agent_tp, agent_fp, 0, agent_fn)\n",
    "    \n",
    "    stage1_tp = ((y_stage1 == 1) & (y_true == 1)).sum()\n",
    "    stage1_fp = ((y_stage1 == 1) & (y_true == 0)).sum()\n",
    "    stage1_fn = ((y_stage1 == 0) & (y_true == 1)).sum()\n",
    "    stage1_cost = calc_cost(stage1_tp, stage1_fp, 0, stage1_fn)\n",
    "    \n",
    "    print(f\"\\nStage1 Only:\")\n",
    "    print(f\"  FP: {stage1_fp}, FN: {stage1_fn}\")\n",
    "    print(f\"  Cost: {stage1_cost:.0f}\")\n",
    "    \n",
    "    print(f\"\\nAI Agent:\")\n",
    "    print(f\"  FP: {agent_fp}, FN: {agent_fn}\")\n",
    "    print(f\"  Cost: {agent_cost:.0f}\")\n",
    "    \n",
    "    print(f\"\\nDifference: {agent_cost - stage1_cost:+.0f}\")\n",
    "    if agent_cost < stage1_cost:\n",
    "        print(\"  → AI Agent reduces total cost\")\n",
    "    else:\n",
    "        print(\"  → AI Agent increases total cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\" * 80)\n",
    "print(\"PIPELINE ANALYSIS SUMMARY REPORT\")\n",
    "print(f\"RUN_ID: {RUN_ID}\")\n",
    "print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n### Stage1 (XGBoost)\")\n",
    "print(f\"- Total samples: {len(stage1_df)}\")\n",
    "print(f\"- Auto-decided: {len(stage1_decided)}\")\n",
    "print(f\"- Handoff to agent: {len(handoff_region)}\")\n",
    "print(f\"- Thresholds: t_low={route1_thresholds['t_low']:.4f}, t_high={route1_thresholds['t_high']:.4f}\")\n",
    "\n",
    "if eval_df is not None:\n",
    "    print(\"\\n### Stage3 (AI Agent)\")\n",
    "    print(f\"- Evaluated samples: {len(eval_df)}\")\n",
    "    print(f\"- Agent FP: {agent_fp}\")\n",
    "    print(f\"- Agent FN: {agent_fn}\")\n",
    "    print(f\"- F1 Score: {2*agent_tp/(2*agent_tp+agent_fp+agent_fn):.3f}\")\n",
    "\n",
    "print(\"\\n### Key Findings\")\n",
    "if eval_df is not None:\n",
    "    fp_non_danger_low_ml = len(fp_cases[(fp_cases['tld_class'] == 'non_danger') & (fp_cases['ml_probability'] < 0.30)])\n",
    "    print(f\"- FP with non-danger TLD + ML<0.30: {fp_non_danger_low_ml}\")\n",
    "    print(f\"  → These should be blocked by POST_LLM_FLIP_GATE but are overridden by P1 gate\")\n",
    "    print(f\"- FP with short certificate (<=90 days): {short_cert.sum()} ({short_cert.mean()*100:.1f}%)\")\n",
    "    print(f\"  → Let's Encrypt (90-day certs) is common on legitimate sites\")\n",
    "\n",
    "print(\"\\n### Recommendations\")\n",
    "print(\"1. Consider disabling P1 gate for non-danger TLDs with ML < 0.30\")\n",
    "print(\"2. Review brand detection logic to reduce false brand matches\")\n",
    "print(\"3. Consider adding certificate age (cert_age_days) as additional signal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}